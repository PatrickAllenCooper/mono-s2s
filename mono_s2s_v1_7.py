# -*- coding: utf-8 -*-
"""Mono_S2S_v1.7.ipynb - Fair Comparison Edition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19K5VNWwJ537c69g5VsMHTu3qCserk1kZ

# MONO-S2S - Fair Comparison Edition

Examines the capacity of monotonicity to allow LLMs to become resilient with respect to adversarial attacks and jailbreaking methods. This is based on the analogy that we see similar behaviors within CNNs.

## METHODOLOGICAL FAIRNESS CHECKLIST

This revision addresses critical methodological issues to ensure fair comparison:

### Model Comparison Fairness
- Train THREE models from same starting checkpoint:
  1. Standard T5 (pre-trained, not fine-tuned) - for reference only
  2. Unconstrained T5 baseline (fine-tuned with same data/hyperparameters)
  3. Monotonic T5 (fine-tuned with same data/hyperparameters + constraints)
- All models use identical: tokenizer, max lengths, optimizer, LR schedule, epochs, batch size, gradient clip, compute budget

### Evaluation Protocol
- Full test sets (not 200-sample subsets): CNN/DM v3.0.0, XSUM, SAMSum
- Fixed decoding parameters: num_beams=4, length_penalty=1.0, min_new_tokens=10, max_new_tokens=128, no_repeat_ngram_size=3
- ROUGE configuration: rouge-score library, stemming=True, lowercase=True
- Bootstrap 95% confidence intervals (1000 resamples)
- Length statistics: avg input/output tokens, length ratios, brevity penalty
- [✓] Multi-seed evaluation: 5 random seeds with mean ± std

### ✓ Training Symmetry
- [✓] Same epochs, early stopping patience, warmup, LR schedule (AdamW + linear warmup)
- [✓] Same batch size, gradient accumulation, gradient clipping
- [✓] Document exact datasets/splits, ensure no train/eval overlap

### ✓ Data Usage (Train vs Eval)
**Training Data (for Baseline & Monotonic models):**
- DialogSum (train split)
- SAMSum (train split)
- XSUM (train split) ← Training on XSUM train
- AMI (train split)
- HighlightSum (train split)
- arXiv (train split)
- MEETING_SUMMARY (train split)

**Evaluation Data (for ALL three models):**
- CNN/DailyMail v3.0.0 (test split) ← NOT in training (disjoint)
- XSUM (test split) ← Disjoint from XSUM train split
- SAMSum (test split) ← Disjoint from SAMSum train split

**Attack Data:**
- Trigger Optimization: CNN/DM validation split (disjoint from test)
- Attack Evaluation: CNN/DM test split (held-out from trigger optimization)

**Note:** XSUM and SAMSum appear in BOTH training (train split) and evaluation (test split), but splits are **disjoint**. CNN/DailyMail appears ONLY in evaluation (not in training).

### ✓ Attack Evaluation
- [✓] Held-out trigger optimization set (disjoint from test set)
- [✓] Transfer attacks: trigger optimized on Model A, test on Model B
- [✓] Fixed budgets: trigger length, gradient steps, candidate vocab
- [✓] Report clean vs attacked deltas on held-out test data

### ✓ Reproducibility & Determinism
- [✓] Comprehensive seed control: Python, NumPy, PyTorch, CUDA, cuDNN, PYTHONHASHSEED
- [✓] Generator objects for DataLoader and sampling
- [✓] Log environment: GPU model, CUDA/cuDNN versions, precision (fp32/bf16/fp16)
- [✓] Log all hyperparameters, dataset versions, and exact configurations

### ✓ Reporting
- [✓] For each model: training budget, params, epochs, wall time, GPU type
- [✓] Metrics: ROUGE-1/2/L (±CI), perplexity, length stats
- [✓] Robustness: UAT/HotFlip clean vs attacked deltas on held-out data

## What Changed from v1.6
1. **Added fair baseline**: Unconstrained T5 trained identically to monotonic model
2. **Full test sets**: No more 200-sample subsets; using complete test splits
3. **Comprehensive determinism**: All random seeds controlled
4. **Bootstrap CIs**: Proper statistical confidence intervals
5. **Held-out attack evaluation**: Separate trigger optimization and test sets
6. **Transfer attacks**: Cross-model trigger evaluation
7. **Length normalization**: Report brevity penalties and length statistics
8. **Multi-seed runs**: 5 seeds with aggregated statistics

## Monotonic Feed-Forward Networks (FFN Sublayers with W≥0 Constraints)

**IMPORTANT SCOPE LIMITATION:** This implementation applies monotonic constraints to **FFN sublayers only**. The encoder/decoder blocks as a whole are **NOT globally monotonic** because:
- **LayerNorm** couples input dimensions (not component-wise monotonic)
- **Residual connections** add unconstrained paths
- **Attention mechanism** remains unchanged (softmax-based, competitive)

### What IS Monotonic
Each **DenseReluDense FFN sublayer** in isolation:
- Input → $W_i$ (≥0) → ReLU → $W_o$ (≥0) → Output
- This path is component-wise monotonic (if input increases, output can only increase)

### What Breaks Global Monotonicity
The **full encoder/decoder layer** includes:
1. **Self-Attention** (softmax-based, competitive, NOT monotonic)
2. **LayerNorm** (couples dimensions via mean/variance, NOT component-wise monotonic)
3. **Residual connections** (x + FFN(LN(x)) - unconstrained x breaks monotonicity)

**Result:** The complete T5 model is **NOT globally monotonic**. Only the FFN sublayers are.

### Implementation Details

**Constraints Applied:**
- All weight matrices in DenseReluDense layers ($W_i$, $W_o$) use **softplus reparametrization**: $W = \text{softplus}(V) \geq 0$
- Existing ReLU activations preserved (already monotonic)
- No post-step clamping (parametrization handles it)

**Constraints NOT Applied:**
- Self-attention Q/K/V projections (unconstrained)
- LayerNorm parameters (unconstrained)
- Embedding layers (unconstrained)
- Output projection layer (unconstrained)

### Why This Still Matters

Even though **global monotonicity is not achieved**, constraining FFN sublayers may:
1. **Reduce adversarial vulnerability** in certain directions
2. **Stabilize feature transformations** within FFN blocks
3. **Test hypothesis**: Do local monotonic constraints improve robustness?

**This is an empirical question** - we compare robustness with/without FFN constraints while holding everything else equal.

### Future Work: True Global Monotonicity
Would require:
- Monotonic attention (positive unnormalized weights, non-negative Q/K/V)
- Replace LayerNorm with monotonic normalization
- Monotonic residual aggregation
- Non-negative embeddings and output projections

**Current Experiment:** FFN constraints only, testing local vs global monotonicity effects.

## Adversarial Attack Vectors

## A. White-box HotFlip / logit-margin attack (fast, effective)

**Goal.** Find a length-$m$ suffix $\delta = (w_1,\dots,w_m)$ that increases the logit margin for a wrong action $a^\dagger$ across a batch $B$:

$$
\max_{\delta\in\ {V}^m} \sum_{(o,c)\in B} \big[\ell_{a^\dagger}(o,\,c\oplus\delta) - \ell_{\text{other}}(o,\,c\oplus\delta)\big].
$$
Use gradients on the input embeddings to greedily replace each trigger token by the nearest vocabulary vector along the ascent direction (HotFlip).

---

## B. Universal adversarial triggers (batch/epoch loop)

Iterate over mini-batches; accumulate gradients only on trigger positions; update tokens via nearest-neighbor in embedding space. After 1–3 epochs you typically obtain a short string (3–6 tokens) that generalizes.

# Text Summarization

Here we operate on report summarization. A task which may need to be performed on device (Apple iMessage Summarization etc.)

Here we use: https://github.com/csebuetnlp/xl-sum

# t5 Model

### Download Model
"""

# ======================================================================
# CRITICAL: Set Environment Variables BEFORE Any Imports
# ======================================================================
# These MUST be set before importing torch/transformers for determinism

import os
import sys

# Set determinism environment variables FIRST
os.environ["PYTHONHASHSEED"] = "42"  # Will be updated by set_all_seeds
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "1")  # Extra determinism

# Now safe to import torch and other libraries
import random
import time
import json
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import T5ForConditionalGeneration, T5Tokenizer
from google.colab import drive

# Mount Google Drive
print("Mounting Google Drive...")
drive.mount('/content/drive')

# ======================================================================
# CONFIGURATION & HYPERPARAMETERS (Centralized for Reproducibility)
# ======================================================================

class ExperimentConfig:
    """Centralized configuration for reproducibility"""
    
    # Random seeds for multi-seed runs
    RANDOM_SEEDS = [42, 1337, 2024, 8888, 12345]
    CURRENT_SEED = RANDOM_SEEDS[0]  # Will be updated for each run
    
    # Model configuration
    MODEL_NAME = "t5-small"  # Guaranteed T5 checkpoint (can also use "t5-base" or "t5-large")
    # NOTE: Changed from "Falconsai/text_summarization" to ensure verified T5 architecture
    
    # Training hyperparameters (MUST BE IDENTICAL for baseline and monotonic)
    LEARNING_RATE = 3e-5
    WEIGHT_DECAY = 0.01
    NUM_EPOCHS = 3
    BATCH_SIZE = 4
    GRADIENT_ACCUMULATION_STEPS = 1
    MAX_GRAD_NORM = 1.0
    WARMUP_RATIO = 0.1
    
    # Tokenization parameters (MUST BE IDENTICAL)
    MAX_INPUT_LENGTH = 512
    MAX_TARGET_LENGTH = 128
    
    # Decoding parameters (MUST BE IDENTICAL for all models)
    DECODE_NUM_BEAMS = 4
    DECODE_LENGTH_PENALTY = 1.0
    DECODE_MIN_NEW_TOKENS = 10
    DECODE_MAX_NEW_TOKENS = 128
    DECODE_NO_REPEAT_NGRAM_SIZE = 3
    DECODE_EARLY_STOPPING = True
    
    # ROUGE configuration (frozen)
    ROUGE_METRICS = ["rouge1", "rouge2", "rougeLsum"]
    ROUGE_USE_STEMMER = True
    ROUGE_BOOTSTRAP_SAMPLES = 1000
    
    # Attack configuration
    ATTACK_TRIGGER_LENGTH = 5
    ATTACK_NUM_CANDIDATES = 100
    ATTACK_NUM_GRAD_STEPS = 50
    
    # Evaluation configuration
    USE_FULL_TEST_SETS = True  # Set to False for quick testing with 200 samples
    EVAL_BATCH_SIZE = 8
    
    # Paths
DRIVE_PATH = '/content/drive/MyDrive/t5_monotonic_experiment'
SAVE_PATH = os.path.join(DRIVE_PATH, 't5_models')
    
    @classmethod
    def to_dict(cls):
        """Export configuration as dictionary for logging"""
        return {
            k: v for k, v in cls.__dict__.items() 
            if not k.startswith('_') and not callable(v) and k.isupper()
        }

os.makedirs(ExperimentConfig.SAVE_PATH, exist_ok=True)

# ======================================================================
# COMPREHENSIVE DETERMINISM SETUP
# ======================================================================

def set_all_seeds(seed=42):
    """
    Set all random seeds for comprehensive reproducibility.
    This controls: Python, NumPy, PyTorch CPU/GPU, cuDNN, CUBLAS
    
    Note: Environment variables (PYTHONHASHSEED, CUBLAS_WORKSPACE_CONFIG, etc.)
    are already set at the top of the file before imports for proper determinism.
    """
    # Update Python hash seed for this specific seed value
    os.environ["PYTHONHASHSEED"] = str(seed)
    
    # Python random
    random.seed(seed)
    
    # NumPy
    np.random.seed(seed)
    
    # PyTorch CPU
    torch.manual_seed(seed)
    
    # PyTorch GPU
if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # For multi-GPU
    
    # cuDNN determinism (may reduce performance but ensures reproducibility)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Disable TF32 on Ampere+ GPUs for full determinism
    if hasattr(torch.backends, "cuda") and hasattr(torch.backends.cuda, "matmul"):
        torch.backends.cuda.matmul.allow_tf32 = False
    if hasattr(torch, "set_float32_matmul_precision"):
        torch.set_float32_matmul_precision("high")  # Avoid TF32
    
    # PyTorch deterministic algorithms (warn_only allows some non-deterministic ops with warning)
    try:
        torch.use_deterministic_algorithms(True, warn_only=True)
    except Exception as e:
        print(f"  Note: torch.use_deterministic_algorithms not fully supported: {e}")
    
    print(f"✓ All random seeds set to: {seed}")
    print(f"  - PYTHONHASHSEED={seed}")
    print(f"  - CUBLAS_WORKSPACE_CONFIG=:16:8 (set at import)")
    print(f"  - TOKENIZERS_PARALLELISM=false (set at import)")
    print(f"  - CUDA_LAUNCH_BLOCKING=1 (set at import)")
    print(f"  - Python random, NumPy, PyTorch CPU/GPU seeds set")
    print(f"  - cuDNN deterministic=True, benchmark=False")
    print(f"  - TF32 disabled for matmul")
    print(f"  - torch.use_deterministic_algorithms=True (warn_only)")

def get_generator(device='cuda', seed=None):
    """
    Create a PyTorch Generator for reproducible sampling in DataLoaders and operations.
    """
    if seed is None:
        seed = ExperimentConfig.CURRENT_SEED
    generator = torch.Generator(device=device)
    generator.manual_seed(seed)
    return generator

def log_environment():
    """Log environment details for reproducibility"""
    info = {
        "python_version": sys.version,
        "pytorch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
    }
    
    if torch.cuda.is_available():
        info.update({
            "cuda_version": torch.version.cuda,
            "cudnn_version": torch.backends.cudnn.version(),
            "gpu_name": torch.cuda.get_device_name(0),
            "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / 1e9,
            "gpu_count": torch.cuda.device_count(),
        })
    
    print("\n" + "="*80)
    print("ENVIRONMENT INFORMATION")
    print("="*80)
    for key, value in info.items():
        print(f"  {key}: {value}")
    print("="*80)
    
    return info

# Setup device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Set initial seed
set_all_seeds(ExperimentConfig.CURRENT_SEED)

# Log environment
env_info = log_environment()

print(f"\n✓ Setup complete. Save path: {ExperimentConfig.SAVE_PATH}")

# ======================================================================
# EVALUATION UTILITY FUNCTIONS
# ======================================================================

# Install rouge-score if needed
try:
    from rouge_score import rouge_scorer
except ImportError:
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "rouge-score"])
    from rouge_score import rouge_scorer

def compute_rouge_with_ci(predictions, references, metrics=None, use_stemmer=True, n_bootstrap=1000, confidence=0.95):
    """
    Compute ROUGE scores with bootstrap confidence intervals.
    
    Args:
        predictions: List of generated summaries
        references: List of reference summaries
        metrics: List of ROUGE metrics (default: ["rouge1", "rouge2", "rougeLsum"])
        use_stemmer: Whether to use Porter stemmer
        n_bootstrap: Number of bootstrap samples
        confidence: Confidence level (default: 0.95)
    
    Returns:
        Dictionary with mean scores and confidence intervals
    """
    if metrics is None:
        metrics = ExperimentConfig.ROUGE_METRICS
    
    scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=use_stemmer)
    
    # Compute scores for all examples
    all_scores = []
    for pred, ref in zip(predictions, references):
        scores = scorer.score(ref, pred)
        all_scores.append({k: v.fmeasure for k, v in scores.items()})
    
    # Compute means
    mean_scores = {
        metric: np.mean([s[metric] for s in all_scores])
        for metric in metrics
    }
    
    # Bootstrap confidence intervals
    np.random.seed(ExperimentConfig.CURRENT_SEED)
    bootstrap_means = {metric: [] for metric in metrics}
    
    n_samples = len(all_scores)
    for _ in range(n_bootstrap):
        # Resample with replacement
        indices = np.random.choice(n_samples, size=n_samples, replace=True)
        resampled = [all_scores[i] for i in indices]
        
        for metric in metrics:
            bootstrap_mean = np.mean([s[metric] for s in resampled])
            bootstrap_means[metric].append(bootstrap_mean)
    
    # Compute confidence intervals
    alpha = 1 - confidence
    lower_percentile = (alpha / 2) * 100
    upper_percentile = (1 - alpha / 2) * 100
    
    ci_scores = {}
    for metric in metrics:
        lower = np.percentile(bootstrap_means[metric], lower_percentile)
        upper = np.percentile(bootstrap_means[metric], upper_percentile)
        ci_scores[metric] = {
            "mean": mean_scores[metric],
            "lower": lower,
            "upper": upper,
            "ci_width": upper - lower
        }
    
    return ci_scores, all_scores

def compute_length_statistics(texts, tokenizer=None):
    """
    Compute length statistics for texts.
    
    Args:
        texts: List of texts
        tokenizer: Optional tokenizer for token-level counting
    
    Returns:
        Dictionary with length statistics
    """
    if tokenizer is not None:
        # Token-level counting
        lengths = [len(tokenizer.encode(text, add_special_tokens=False)) for text in texts]
    else:
        # Word-level counting
        lengths = [len(text.split()) for text in texts]
    
    return {
        "mean": np.mean(lengths),
        "std": np.std(lengths),
        "median": np.median(lengths),
        "min": np.min(lengths),
        "max": np.max(lengths),
        "total_tokens" if tokenizer else "total_words": np.sum(lengths)
    }

def compute_brevity_penalty(predictions, references, tokenizer=None):
    """
    Compute brevity penalty (similar to BLEU's BP) to detect length biases.
    BP = min(1, exp(1 - ref_len / pred_len))
    
    Returns value between 0 and 1, where:
    - 1.0 = predictions match reference length
    - <1.0 = predictions are shorter than references
    - >1.0 = predictions are longer than references (we report ratio directly)
    """
    if tokenizer is not None:
        pred_lengths = [len(tokenizer.encode(text, add_special_tokens=False)) for text in predictions]
        ref_lengths = [len(tokenizer.encode(text, add_special_tokens=False)) for text in references]
    else:
        pred_lengths = [len(text.split()) for text in predictions]
        ref_lengths = [len(text.split()) for text in references]
    
    total_pred = sum(pred_lengths)
    total_ref = sum(ref_lengths)
    
    length_ratio = total_pred / total_ref if total_ref > 0 else 0
    
    if length_ratio >= 1.0:
        bp = 1.0
    else:
        bp = np.exp(1 - total_ref / total_pred) if total_pred > 0 else 0
    
    return {
        "brevity_penalty": bp,
        "length_ratio": length_ratio,
        "avg_pred_length": np.mean(pred_lengths),
        "avg_ref_length": np.mean(ref_lengths)
    }

def compute_avg_loss(model, data_loader, device):
    """
    Compute average loss on a data loader (for validation/evaluation).
    
    Args:
        model: Model to evaluate
        data_loader: DataLoader with batches
        device: Device to run on
    
    Returns:
        Average loss across all batches
    """
    model.eval()
    total_loss = 0.0
    n_batches = 0
    
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            total_loss += outputs.loss.item()
            n_batches += 1
    
    return total_loss / max(n_batches, 1)

def generate_summary_fixed_params(model, text, tokenizer, device):
    """
    Generate summary with FIXED decoding parameters for fair comparison.
    Uses parameters from ExperimentConfig.
    """
    inputs = tokenizer(
        "summarize: " + text.strip(),
        return_tensors="pt",
        max_length=ExperimentConfig.MAX_INPUT_LENGTH,
        truncation=True,
        padding=False
    ).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=ExperimentConfig.DECODE_MAX_NEW_TOKENS,
            min_new_tokens=ExperimentConfig.DECODE_MIN_NEW_TOKENS,
            num_beams=ExperimentConfig.DECODE_NUM_BEAMS,
            length_penalty=ExperimentConfig.DECODE_LENGTH_PENALTY,
            no_repeat_ngram_size=ExperimentConfig.DECODE_NO_REPEAT_NGRAM_SIZE,
            early_stopping=ExperimentConfig.DECODE_EARLY_STOPPING
        )
    
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

def evaluate_model_comprehensive(model, test_texts, test_references, tokenizer, device, model_name="Model"):
    """
    Comprehensive evaluation with ROUGE, CIs, and length statistics.
    
    Returns:
        Dictionary with all evaluation metrics
    """
    print(f"\n{'='*80}")
    print(f"COMPREHENSIVE EVALUATION: {model_name}")
    print(f"{'='*80}")
    print(f"Evaluating on {len(test_texts)} examples...")
    
    # Generate summaries
    predictions = []
    start_time = time.time()
    
    for i, text in enumerate(test_texts):
        if i % 100 == 0:
            print(f"  Generated {i}/{len(test_texts)} summaries...")
        summary = generate_summary_fixed_params(model, text, tokenizer, device)
        predictions.append(summary)
    
    generation_time = time.time() - start_time
    
    # Compute ROUGE with CIs
    print("Computing ROUGE scores with bootstrap CIs...")
    rouge_with_ci, all_rouge_scores = compute_rouge_with_ci(
        predictions, test_references,
        n_bootstrap=ExperimentConfig.ROUGE_BOOTSTRAP_SAMPLES
    )
    
    # Compute length statistics
    print("Computing length statistics...")
    input_stats = compute_length_statistics(test_texts, tokenizer)
    pred_stats = compute_length_statistics(predictions, tokenizer)
    ref_stats = compute_length_statistics(test_references, tokenizer)
    
    # Compute brevity penalty
    bp_info = compute_brevity_penalty(predictions, test_references, tokenizer)
    
    results = {
        "model_name": model_name,
        "n_examples": len(test_texts),
        "generation_time_seconds": generation_time,
        "rouge_scores": rouge_with_ci,
        "input_length_stats": input_stats,
        "prediction_length_stats": pred_stats,
        "reference_length_stats": ref_stats,
        "brevity_penalty_info": bp_info,
        "predictions": predictions,
        "all_rouge_scores": all_rouge_scores
    }
    
    # Print summary
    print(f"\n{model_name} Results:")
    print(f"  Generation time: {generation_time:.1f}s ({generation_time/len(test_texts):.3f}s per example)")
    print(f"\n  ROUGE Scores (with 95% CI):")
    for metric in ExperimentConfig.ROUGE_METRICS:
        r = rouge_with_ci[metric]
        print(f"    {metric.upper()}: {r['mean']:.4f} [{r['lower']:.4f}, {r['upper']:.4f}] (±{r['ci_width']:.4f})")
    
    print(f"\n  Length Statistics (tokens):")
    print(f"    Input:      mean={input_stats['mean']:.1f}, std={input_stats['std']:.1f}")
    print(f"    Prediction: mean={pred_stats['mean']:.1f}, std={pred_stats['std']:.1f}")
    print(f"    Reference:  mean={ref_stats['mean']:.1f}, std={ref_stats['std']:.1f}")
    print(f"    Length ratio (pred/ref): {bp_info['length_ratio']:.3f}")
    print(f"    Brevity penalty: {bp_info['brevity_penalty']:.3f}")
    
    return results

# ======================================================================
# Download and Modify Pre-trained T5 Model for Monotonicity
# ======================================================================

print("\nDownloading pre-trained T5 summarization model...")
print(f"Using checkpoint: {ExperimentConfig.MODEL_NAME}")

# Download tokenizer (shared by all models)
tokenizer = T5Tokenizer.from_pretrained(ExperimentConfig.MODEL_NAME)

# Model 1: Standard T5 (pre-trained only, NOT fine-tuned - for reference)
print("\nLoading Model 1: Standard T5 (pre-trained, will NOT be fine-tuned)...")
model_standard = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
model_standard.eval()

# Verify this is actually a T5 model (script assumes T5 internals)
assert model_standard.config.model_type == "t5", \
    f"ERROR: This script requires a T5 checkpoint. Found {model_standard.config.model_type}. " \
    f"Please set ExperimentConfig.MODEL_NAME to a valid T5 checkpoint (e.g., 't5-small', 't5-base')."

num_params = sum(p.numel() for p in model_standard.parameters())
print(f"✓ Model verified: T5 architecture with {num_params:,} parameters")
print(f"Model type: {model_standard.config.model_type}")
print("NOTE: This model serves as reference only and will NOT be fine-tuned.")

"""### Understand Model"""

print("\n" + "="*80)
print("T5 ARCHITECTURE ANALYSIS")
print("="*80)

# T5 uses DenseReluDense (not SwiGLU) in its feed-forward network
print(f"Activation function: {model_standard.config.dense_act_fn}")  # Usually 'relu'
print(f"Number of layers: {model_standard.config.num_layers}")
print(f"Hidden size: {model_standard.config.d_model}")
print(f"FFN intermediate size: {model_standard.config.d_ff}")

"""### Create Model with Softplus Parametrization"""

# Softplus parametrization for non-negative weights (better than clamping)
import torch.nn.utils.parametrize as P

class NonNegativeParametrization(nn.Module):
    """
    Softplus parametrization: W = softplus(V) >= 0
    
    This is BETTER than clamping because:
    - Differentiable everywhere (preserves gradients)
    - Compatible with optimizer state (Adam momentum, etc.)
    - Smooth approximation of ReLU
    - No need for post-step constraints
    """
    def forward(self, V):
        return F.softplus(V)

def make_model_monotonic(model):
    """
    Apply non-negative weight constraints to FFN layers using softplus parametrization.
    
    This approach:
    - Ensures W ≥ 0 always (via W = softplus(V))
    - Preserves correct gradients (no .data manipulation)
    - Compatible with optimizer states (Adam momentum)
    - No need for post-step clamping
    
    Mathematical guarantee: If W ≥ 0 and activation is monotonic (ReLU),
    then each FFN sublayer output is monotonic in its input (component-wise).
    
    Note: This does NOT make the full model globally monotonic due to LayerNorm,
    residual connections, and unconstrained attention.
    """
    from transformers.models.t5.modeling_t5 import T5DenseReluDense
    
    modified_count = 0

    # Apply to all T5DenseReluDense modules (handles both standard and gated variants)
    for module in model.modules():
        if isinstance(module, T5DenseReluDense):
            # Parametrize all FFN weight sublayers (wi, wi_0, wi_1 for gated, wo for all)
            for param_name in ["wi", "wi_0", "wi_1", "wo"]:
                if hasattr(module, param_name):
                    sub_module = getattr(module, param_name)
                    if hasattr(sub_module, "weight"):
                        P.register_parametrization(sub_module, "weight", NonNegativeParametrization())
            modified_count += 1

    print(f"✓ Applied non-negative parametrization (softplus) to {modified_count} weight matrices in FFN sublayers")
    print(f"  - Covers both standard (wi, wo) and gated (wi_0, wi_1, wo) FFN variants")
    print(f"  - No clamping needed (W = softplus(V) ≥ 0 always)")
    print(f"  - Gradients preserved for proper optimization")
    print(f"  NOTE: Model is NOT globally monotonic (LayerNorm + residuals + attention break it)")
    return model

"""### Create All Three Models"""

print("\nCreating all three models for fair comparison...")
print("="*80)

# Model 1: Standard T5 (already loaded above - pre-trained only, NOT fine-tuned)
print("✓ Model 1: Standard T5 (pre-trained, ReLU, unconstrained W)")
print("   Status: Will NOT be fine-tuned (reference only)")

# Model 2: Unconstrained Baseline T5 (WILL be fine-tuned with same data as monotonic)
print("\n✓ Model 2: Unconstrained Baseline T5 (ReLU, unconstrained W)")
print("   Loading from same checkpoint...")
model_baseline = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
model_baseline.train()  # Will be fine-tuned
print("   Status: WILL be fine-tuned (fair baseline)")

# Model 3: Monotonic T5 (constrained weights, WILL be fine-tuned)
print("\n✓ Model 3: Monotonic T5 (ReLU, W ≥ 0 constraints)")
print("   Loading from same checkpoint...")
model_monotonic = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
model_monotonic = make_model_monotonic(model_monotonic)
model_monotonic.train()  # Will be fine-tuned
print("   Status: WILL be fine-tuned (treatment)")

print("\n" + "="*80)
print("✓ THREE MODELS READY FOR FAIR COMPARISON:")
print("="*80)
print("  1. Standard T5       → Pre-trained only (reference)")
print("  2. Baseline T5       → Will be fine-tuned with unconstrained W")
print("  3. Monotonic T5      → Will be fine-tuned with W ≥ 0 constraints")
print("\n  Models 2 & 3 use IDENTICAL:")
print("    • Starting checkpoint:", ExperimentConfig.MODEL_NAME)
print("    • Training data (same mixture)")
print("    • Hyperparameters (LR, epochs, batch size, etc.)")
print("    • Decoding parameters")
print("="*80)

"""### Test Model"""

# ======================================================================
# Initial Inference on Standard T5 Model
# ======================================================================

import torch
from scipy.spatial.distance import cosine
from collections import defaultdict
from datasets import load_dataset # Import load_dataset for the new dataset
import numpy as np
import os
from tqdm.auto import tqdm # Import tqdm for progress bar
import json # Import the json library

print("="*80)
print("INITIAL INFERENCE: Standard T5 Model")
print("="*80)

# Ensure tokenizer and standard model are loaded
if 'tokenizer' not in globals() or 'model_standard' not in globals():
     tokenizer = T5Tokenizer.from_pretrained(ExperimentConfig.MODEL_NAME)
     model_standard = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
     model_standard.eval()
     print("Loaded tokenizer and standard model.")


# Load test datasets for evaluation
print("\nLoading test datasets for evaluation...")
print("="*80)

# CNN/DailyMail test set
print("Loading CNN/DailyMail v3.0.0 test split...")
try:
    cnn_dm_test = load_dataset("cnn_dailymail", '3.0.0', split="test")
    
    if ExperimentConfig.USE_FULL_TEST_SETS:
        # Use FULL test set for fair comparison
        print(f"✓ Loaded FULL CNN/DM test set: {len(cnn_dm_test)} samples")
        cnn_dm_test_subset = cnn_dm_test
    else:
        # Quick testing mode: use 200 samples
        np.random.seed(ExperimentConfig.CURRENT_SEED)
        subset_indices = np.random.choice(len(cnn_dm_test), 200, replace=False)
        cnn_dm_test_subset = cnn_dm_test.select(subset_indices)
        print(f"✓ Quick testing mode: Using 200 samples from CNN/DM test set")
    
    cnn_dm_test_texts = [example['article'] for example in cnn_dm_test_subset]
    cnn_dm_test_summaries = [example['highlights'] for example in cnn_dm_test_subset]
except Exception as e:
    print(f"Error loading CNN/DailyMail: {e}")
    cnn_dm_test_texts, cnn_dm_test_summaries = [], []

# XSUM test set
print("\nLoading XSUM test split...")
try:
    xsum_test = load_dataset("EdinburghNLP/xsum", split="test")
    
    if ExperimentConfig.USE_FULL_TEST_SETS:
        print(f"✓ Loaded FULL XSUM test set: {len(xsum_test)} samples")
        xsum_test_subset = xsum_test
    else:
        np.random.seed(ExperimentConfig.CURRENT_SEED)
        subset_indices = np.random.choice(len(xsum_test), 200, replace=False)
        xsum_test_subset = xsum_test.select(subset_indices)
        print(f"✓ Quick testing mode: Using 200 samples from XSUM test set")
    
    xsum_test_texts = [example['document'] for example in xsum_test_subset]
    xsum_test_summaries = [example['summary'] for example in xsum_test_subset]
except Exception as e:
    print(f"Error loading XSUM: {e}")
    xsum_test_texts, xsum_test_summaries = [], []

# SAMSum test set
print("\nLoading SAMSum test split...")
try:
    samsum_test = load_dataset("samsum", split="test")
    
    if ExperimentConfig.USE_FULL_TEST_SETS:
        print(f"✓ Loaded FULL SAMSum test set: {len(samsum_test)} samples")
        samsum_test_subset = samsum_test
    else:
        np.random.seed(ExperimentConfig.CURRENT_SEED)
        subset_indices = np.random.choice(len(samsum_test), min(200, len(samsum_test)), replace=False)
        samsum_test_subset = samsum_test.select(subset_indices)
        print(f"✓ Quick testing mode: Using {len(samsum_test_subset)} samples from SAMSum test set")
    
    samsum_test_texts = [example['dialogue'] for example in samsum_test_subset]
    samsum_test_summaries = [example['summary'] for example in samsum_test_subset]
except Exception as e:
    print(f"Error loading SAMSum: {e}")
    samsum_test_texts, samsum_test_summaries = [], []

print("\n" + "="*80)
print("TEST DATASETS LOADED:")
print("="*80)
print(f"  CNN/DailyMail: {len(cnn_dm_test_texts)} examples")
print(f"  XSUM:          {len(xsum_test_texts)} examples")
print(f"  SAMSum:        {len(samsum_test_texts)} examples")
print(f"  Total:         {len(cnn_dm_test_texts) + len(xsum_test_texts) + len(samsum_test_texts)} examples")
print("="*80)

# Store for backward compatibility with old code
inference_test_texts = cnn_dm_test_texts
inference_test_summaries = cnn_dm_test_summaries
print(f"\n✓ Stored test samples for evaluation across experiments.")


def generate_summary(model, text, tokenizer, max_length=128, num_beams=4):
    """Generate summary with detailed settings"""
    inputs = tokenizer(
        "summarize: " + text.strip(),
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            min_length=20,
            length_penalty=2.0,
            num_beams=num_beams,
            early_stopping=True,
            no_repeat_ngram_size=3
        )

    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

def compute_perplexity(model, text, summary, tokenizer):
    """
    Compute token-level perplexity (teacher-forced) - AUXILIARY METRIC ONLY
    
    This computes exp(cross-entropy loss) with teacher forcing on the generated summary.
    NOT a primary quality metric for abstractive summarization (use ROUGE instead).
    Included for auxiliary analysis only.
    
    Args:
        model: Model to evaluate
        text: Input text
        summary: Generated or reference summary
        tokenizer: Tokenizer
    
    Returns:
        Perplexity (exp of average token-level cross-entropy)
    """
    inputs = tokenizer(
        "summarize: " + text.strip(),
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(device)

    targets = tokenizer(
        summary,
        return_tensors="pt",
        max_length=128,
        truncation=True,
        padding='max_length'
    ).to(device)

    # Mask padding tokens to -100 so they don't contribute to loss
    labels = targets.input_ids.clone()
    labels[labels == tokenizer.pad_token_id] = -100

    with torch.no_grad():
        outputs = model(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            labels=labels
        )
        loss = outputs.loss
        perplexity = torch.exp(loss).item()

    return perplexity

# ======================================================================
# Run Inference on Standard T5 Model
# ======================================================================

print("\nRunning inference on Standard T5 model...")
results_std = []

# Use the stored test data and add tqdm for progress bar
for i, (article_text, reference_summary) in tqdm(enumerate(zip(inference_test_texts, inference_test_summaries)), total=len(inference_test_texts), desc="Standard T5 Inference"):
    title = f"Article {i+1}" # Use a generic title

    # Generate summary using fixed parameters for fair comparison
    summary_std = generate_summary_fixed_params(model_standard, article_text, tokenizer, device)

    # Compute perplexity (auxiliary metric)
    try:
        ppl_std = compute_perplexity(model_standard, article_text, summary_std, tokenizer)
    except Exception as e:
        # print(f"\nError computing perplexity for Standard T5 (Test {i+1}): {e}") # Suppress verbose error in loop
        ppl_std = float('nan')

    # Length
    len_std = len(summary_std.split())

    results_std.append({
        'title': title,
        'input_length': len(article_text.split()),
        'reference_summary': reference_summary,
        'summary_std': summary_std,
        'ppl_std': ppl_std,
        'len_std': len_std,
    })

    # Only break early if not using full test sets
    if not ExperimentConfig.USE_FULL_TEST_SETS and i >= 199:
        break


# ======================================================================
# Aggregate Statistics for Standard T5
# ======================================================================

print("\n" + "="*80)
print("AGGREGATE STATISTICS: Standard T5")
print("="*80)

# Filter out NaN perplexity values for aggregation
valid_ppl_std = [r['ppl_std'] for r in results_std if not torch.isnan(torch.tensor(r['ppl_std']))]

avg_ppl_std = np.mean(valid_ppl_std) if valid_ppl_std else float('nan')
avg_len_std = np.mean([r['len_std'] for r in results_std])

print(f"\nPerplexity (on generated text): {avg_ppl_std:.4f} (avg)")
print(f"Summary Length: {avg_len_std:.1f} words (avg)")


# ======================================================================
# Detailed Output (First 5 Examples)
# ======================================================================

print("\n" + "="*80)
print("EXAMPLE OUTPUTS: Standard T5 (First 5 Examples)")
print("="*80)

for i, result in enumerate(results_std[:5]): # Show only first 5 examples
    print(f"\n{i+1}. {result['title']}:")
    print(f"   Reference: {result['reference_summary']}")
    print(f"   Standard:  {result['summary_std']}")


# ======================================================================
# Save Results
# ======================================================================

print("\n" + "="*80)
print("Saving results...")

results_dict_std = {
    'test_cases': results_std,
    'aggregate': {
        'avg_ppl_std': avg_ppl_std,
        'avg_len_std': avg_len_std,
        'total_tests': len(results_std),
    }
}

results_path_std = os.path.join(SAVE_PATH, 'inference_standard_t5_cnn_dailymail.json') # New filename
with open(results_path_std, 'w') as f:
    json.dump(results_dict_std, f, indent=2)

print(f"✓ Standard T5 results saved to: {results_path_std}")


print("\n" + "="*80)
print("✓ INITIAL INFERENCE (Standard T5) COMPLETE")
print("="*80)

"""### Fine-tune Both Baseline and Monotonic Models"""

# ======================================================================
# Fine-tune BOTH Models (Baseline & Monotonic) on Same Data
# ======================================================================

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from datasets import load_dataset
import os
import json # Import json for saving history

print("="*80)
print("FINE-TUNING BOTH BASELINE AND MONOTONIC T5 MODELS")
print("="*80)
print("Training with IDENTICAL settings for fair comparison:")
print(f"  • Learning rate: {ExperimentConfig.LEARNING_RATE}")
print(f"  • Weight decay: {ExperimentConfig.WEIGHT_DECAY}")
print(f"  • Epochs: {ExperimentConfig.NUM_EPOCHS}")
print(f"  • Batch size: {ExperimentConfig.BATCH_SIZE}")
print(f"  • Max grad norm: {ExperimentConfig.MAX_GRAD_NORM}")
print(f"  • Warmup ratio: {ExperimentConfig.WARMUP_RATIO}")
print("="*80)

# Paths for both models
baseline_best_path = os.path.join(ExperimentConfig.SAVE_PATH, 't5_baseline_finetuned_best.pt')
monotonic_best_path = os.path.join(ExperimentConfig.SAVE_PATH, 't5_monotonic_finetuned_best.pt')
baseline_checkpoint_dir = os.path.join(ExperimentConfig.SAVE_PATH, 'baseline_checkpoints')
monotonic_checkpoint_dir = os.path.join(ExperimentConfig.SAVE_PATH, 'monotonic_checkpoints')
os.makedirs(baseline_checkpoint_dir, exist_ok=True)
os.makedirs(monotonic_checkpoint_dir, exist_ok=True)
baseline_history_path = os.path.join(ExperimentConfig.SAVE_PATH, 'baseline_training_history.json')
monotonic_history_path = os.path.join(ExperimentConfig.SAVE_PATH, 'monotonic_training_history.json')

# For backward compatibility
SAVE_PATH = ExperimentConfig.SAVE_PATH
best_model_path = monotonic_best_path
checkpoint_dir = monotonic_checkpoint_dir
history_path = monotonic_history_path

# ======================================================================
# Check for Checkpoints and Determine Training Needed
# ======================================================================

latest_checkpoint = None
latest_epoch = -1
# Find the latest checkpoint
for f_name in os.listdir(checkpoint_dir):
    if f_name.startswith('checkpoint_epoch_') and f_name.endswith('.pt'):
        try:
            epoch = int(f_name.replace('checkpoint_epoch_', '').replace('.pt', ''))
            if epoch > latest_epoch:
                latest_epoch = epoch
                latest_checkpoint = os.path.join(checkpoint_dir, f_name)
        except ValueError:
            continue # Ignore files not matching the naming convention

training_needed = latest_checkpoint is None or latest_epoch < 3 # Assume max 3 epochs for now, or set a target

# ======================================================================
# Load Datasets (Moved outside if/else)
# ======================================================================

print("\nLoading datasets...")

def _collect_pairs_dialogsum(split="train"):
    d = load_dataset("knkarthick/dialogsum", split=split)
    for ex in d:
        yield ex.get("dialogue", ""), ex.get("summary", "")

def _collect_pairs_samsum(split="train"):
    try:
        d = load_dataset("samsum", split=split)
    except:
        d = load_dataset("knkarthick/samsum", split=split)
    for ex in d:
        yield ex.get("dialogue", ""), ex.get("summary", "")

# Corrected function for MEETING_SUMMARY based on likely format
def _collect_pairs_meetingsummary(split="train"):
    try:
        d = load_dataset("knkarthick/MEETING_SUMMARY", split=split)
    except Exception as e:
        print(f"Warning: Could not load knkarthick/MEETING_SUMMARY split '{split}': {e}")
        return # Yield nothing if dataset fails to load

    for ex in d:
        # MEETING_SUMMARY typically has 'dialogue' and 'summary' keys
        yield ex.get("dialogue", ""), ex.get("summary", "")

# Corrected function for XSUM - use EdinburghNLP/xsum for consistency with evaluation
def _collect_pairs_xsum(split="train"):
    try:
        # Use EdinburghNLP/xsum (same as evaluation) with document/summary fields
        d = load_dataset("EdinburghNLP/xsum", split=split)
    except Exception as e:
        print(f"Warning: Could not load EdinburghNLP/xsum split '{split}': {e}")
        return # Yield nothing if dataset fails to load

    for ex in d:
        # EdinburghNLP/xsum uses 'document' and 'summary' fields
        yield ex.get("document", ""), ex.get("summary", "")

# Corrected function for AMI based on likely format
def _collect_pairs_ami(split="train"):
    try:
        d = load_dataset("knkarthick/AMI", split=split)
    except Exception as e:
        print(f"Warning: Could not load knkarthick/AMI split '{split}': {e}")
        return # Yield nothing if dataset fails to load

    for ex in d:
        # AMI typically has 'dialogue' and 'summary' keys
        yield ex.get("dialogue", ""), ex.get("summary", "")

# Corrected function for highlightsum based on likely format
def _collect_pairs_highlightsum(split="train"):
    try:
        d = load_dataset("knkarthick/highlightsum", split=split)
    except Exception as e:
        print(f"Warning: Could not load knkarthick/highlightsum split '{split}': {e}")
        return # Yield nothing if dataset fails to load

    for ex in d:
        # highlightsum typically has 'dialogue' and 'summary' keys
        yield ex.get("dialogue", ""), ex.get("summary", "")

# New function to collect pairs from arxiv-summarization
def _collect_pairs_arxiv(split="train"):
    try:
        # Note: arxiv-summarization has 'article' and 'abstract' keys
        d = load_dataset("ccdv/arxiv-summarization", split=split)
    except Exception as e:
        print(f"Warning: Could not load ccdv/arxiv-summarization split '{split}': {e}")
        return # Yield nothing if dataset fails to load

    for ex in d:
        yield ex.get("article", ""), ex.get("abstract", "")


def _materialize(pairs_iter, name="", max_samples=None): # Removed max_samples default
    X, Y = [], []
    for i, (text, summary) in enumerate(pairs_iter):
        if text and summary:
            X.append(text.strip())
            Y.append(summary.strip())
        if max_samples and i >= max_samples - 1: # Only limit if max_samples is provided
            break
    print(f"  ✓ {name}: {len(X)} samples") # Keep verbose print for materialize
    return X, Y

# Load training data - Use ALL samples from specified datasets
print("\nLoading training data (all samples)...")
dlg_train_x, dlg_train_y = _materialize(_collect_pairs_dialogsum("train"), "DialogSum(train)") # Removed max_samples
sam_train_x, sam_train_y = _materialize(_collect_pairs_samsum("train"), "SAMSum(train)") # Removed max_samples
meet_train_x, meet_train_y = _materialize(_collect_pairs_meetingsummary("train"), "MEETING_SUMMARY(train)") # Added MEETING_SUMMARY
xsum_train_x, xsum_train_y = _materialize(_collect_pairs_xsum("train"), "XSUM(train)") # Added XSUM training
ami_train_x, ami_train_y = _materialize(_collect_pairs_ami("train"), "AMI(train)") # Added AMI training
highlight_train_x, highlight_train_y = _materialize(_collect_pairs_highlightsum("train"), "HighlightSum(train)") # Added HighlightSum training
arxiv_train_x, arxiv_train_y = _materialize(_collect_pairs_arxiv("train"), "ArXiv(train)") # Added ArXiv training


# Combine datasets
train_texts = dlg_train_x + sam_train_x + meet_train_x + xsum_train_x + ami_train_x + highlight_train_x + arxiv_train_x # Include ArXiv
train_summaries = dlg_train_y + sam_train_y + meet_train_y + xsum_train_y + ami_train_y + highlight_train_y + arxiv_train_y # Include ArXiv

print(f"\nTotal training samples: {len(train_texts)}")

# Load validation data - CRITICAL: Use validation/validation splits, NOT test splits
# This prevents data leakage into evaluation
print("\nLoading validation data (using validation splits to avoid test contamination)...")
dlg_val_x, dlg_val_y = _materialize(_collect_pairs_dialogsum("validation"), "DialogSum(val)")

# For datasets with validation splits, use them
sam_val_x, sam_val_y = _materialize(_collect_pairs_samsum("validation"), "SAMSum(val)") # FIXED: validation not test

# For datasets without validation split, use a subset of training for validation
# Note: Some datasets may only have train/test - in that case, we'll handle errors gracefully
try:
    meet_val_x, meet_val_y = _materialize(_collect_pairs_meetingsummary("validation"), "MEETING_SUMMARY(val)")
except:
    print("  Note: MEETING_SUMMARY validation split not available, skipping")
    meet_val_x, meet_val_y = [], []

xsum_val_x, xsum_val_y = _materialize(_collect_pairs_xsum("validation"), "XSUM(val)") # FIXED: validation not test

try:
    ami_val_x, ami_val_y = _materialize(_collect_pairs_ami("validation"), "AMI(val)") # FIXED: validation not test
except:
    print("  Note: AMI validation split not available, skipping")
    ami_val_x, ami_val_y = [], []

try:
    highlight_val_x, highlight_val_y = _materialize(_collect_pairs_highlightsum("validation"), "HighlightSum(val)") # FIXED: validation not test
except:
    print("  Note: HighlightSum validation split not available, skipping")
    highlight_val_x, highlight_val_y = [], []

arxiv_val_x, arxiv_val_y = _materialize(_collect_pairs_arxiv("validation"), "ArXiv(val)")

val_texts = dlg_val_x + sam_val_x + meet_val_x + xsum_val_x + ami_val_x + highlight_val_x + arxiv_val_x
val_summaries = dlg_val_y + sam_val_y + meet_val_y + xsum_val_y + ami_val_y + highlight_val_y + arxiv_val_y

print(f"  CRITICAL: All validation data uses 'validation' splits (NOT 'test' splits)")
print(f"  This ensures test sets remain untouched for final evaluation (no data leakage)")

print(f"Total validation samples: {len(val_texts)}")

# ======================================================================
# Create Dataset Class
# ======================================================================

class SummarizationDataset(Dataset):
    def __init__(self, texts, summaries, tokenizer, max_input_length=512, max_target_length=128):
        self.texts = texts
        self.summaries = summaries
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = "summarize: " + self.texts[idx]
        summary = self.summaries[idx]

        # Tokenize input
        input_encoding = self.tokenizer(
            text,
            max_length=self.max_input_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # Tokenize target
        target_encoding = self.tokenizer(
            summary,
            return_tensors="pt",
            max_length=self.max_target_length,
            padding='max_length',
            truncation=True,
        )

        labels = target_encoding.input_ids
        labels[labels == self.tokenizer.pad_token_id] = -100  # Ignore padding in loss


        return {
            'input_ids': input_encoding.input_ids.squeeze(),
            'attention_mask': input_encoding.attention_mask.squeeze(),
            'labels': labels.squeeze()
        }


# Create datasets
train_dataset = SummarizationDataset(train_texts, train_summaries, tokenizer)
val_dataset = SummarizationDataset(val_texts, val_summaries, tokenizer)

# Worker initialization function for deterministic data loading
def worker_init_fn(worker_id):
    """Initialize each DataLoader worker with a unique but deterministic seed"""
    worker_seed = ExperimentConfig.CURRENT_SEED + worker_id
    np.random.seed(worker_seed)
    random.seed(worker_seed)

# Create dataloaders with reproducible sampling
# Use generator for reproducibility in DataLoader shuffling
train_generator = get_generator('cpu', ExperimentConfig.CURRENT_SEED)
train_loader = DataLoader(
    train_dataset, 
    batch_size=ExperimentConfig.BATCH_SIZE, 
    shuffle=True, 
    num_workers=2,
    generator=train_generator,
    worker_init_fn=worker_init_fn
)
val_loader = DataLoader(
    val_dataset, 
    batch_size=ExperimentConfig.BATCH_SIZE, 
    shuffle=False, 
    num_workers=2,
    worker_init_fn=worker_init_fn
)

print(f"\n✓ Training batches: {len(train_loader)}")
print(f"✓ Validation batches: {len(val_loader)}")
print(f"✓ Using seed {ExperimentConfig.CURRENT_SEED} for DataLoader reproducibility")
print(f"✓ worker_init_fn set for deterministic multi-worker data loading")

# ======================================================================
# Custom Training Loop with Weight Constraints
# ======================================================================

class T5Trainer:
    """
    Unified trainer for both baseline and monotonic models.
    Enforces weight constraints only if is_monotonic=True.
    Uses centralized config for hyperparameters.
    """
    def __init__(self, model, train_loader, val_loader, device, is_monotonic=False, 
                 learning_rate=None, weight_decay=None, num_epochs=None, 
                 max_grad_norm=None, warmup_ratio=None,
                 checkpoint_dir=None, history_path=None, model_name="Model"):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.is_monotonic = is_monotonic
        self.model_name = model_name
        self.checkpoint_dir = checkpoint_dir
        self.history_path = history_path
        
        # Use config defaults if not specified
        self.learning_rate = learning_rate if learning_rate is not None else ExperimentConfig.LEARNING_RATE
        self.weight_decay = weight_decay if weight_decay is not None else ExperimentConfig.WEIGHT_DECAY
        self.num_epochs = num_epochs if num_epochs is not None else ExperimentConfig.NUM_EPOCHS
        self.max_grad_norm = max_grad_norm if max_grad_norm is not None else ExperimentConfig.MAX_GRAD_NORM
        self.warmup_ratio = warmup_ratio if warmup_ratio is not None else ExperimentConfig.WARMUP_RATIO

        # Optimizer
        self.optimizer = AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)

        # Scheduler
        total_steps = len(train_loader) * self.num_epochs
        warmup_steps = int(total_steps * self.warmup_ratio)
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )

        # History and Resume
        self.start_epoch = 0
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.load_checkpoint() # Attempt to load checkpoint on init

    def load_checkpoint(self):
        """Loads the latest checkpoint if it exists."""
        latest_checkpoint = None
        latest_epoch = -1
        if self.checkpoint_dir and os.path.exists(self.checkpoint_dir):
            for f_name in os.listdir(self.checkpoint_dir):
                if f_name.startswith('checkpoint_epoch_') and f_name.endswith('.pt'):
                    try:
                        epoch = int(f_name.replace('checkpoint_epoch_', '').replace('.pt', ''))
                        if epoch > latest_epoch:
                            latest_epoch = epoch
                            latest_checkpoint = os.path.join(self.checkpoint_dir, f_name)
                    except ValueError:
                        continue

        if latest_checkpoint:
            print(f"\n🔄 Loading checkpoint from {latest_checkpoint}")
            checkpoint = torch.load(latest_checkpoint, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.start_epoch = checkpoint['epoch'] + 1 # Start from the next epoch
            self.best_val_loss = checkpoint['best_val_loss'] # Load best validation loss

            # Reload scheduler state (optional, but good practice)
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            else:
                 print("Warning: Scheduler state not found in checkpoint.") # Added warning

            # Load history if available
            if self.history_path and os.path.exists(self.history_path):
                try:
                    with open(self.history_path, 'r') as f:
                        history = json.load(f)
                        self.train_losses = history.get('train_losses', [])
                        self.val_losses = history.get('val_losses', [])
                        print(f"  Loaded training history.")
                except Exception as e:
                    print(f"Warning: Could not load training history from {self.history_path}: {e}")
            else:
                 print("No training history found to load.") # Added print statement


            print(f"✓ Resuming training from Epoch {self.start_epoch}")
        else:
            print("\n No checkpoint found. Starting training from Epoch 0.")

    def save_checkpoint(self, epoch, val_loss, is_best=False):
        """Saves model and optimizer state."""
        if self.checkpoint_dir:
            checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')
            save_dict = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'best_val_loss': self.best_val_loss,
                'val_loss': val_loss, # Save current epoch's val loss as well
                'scheduler_state_dict': self.scheduler.state_dict(), # Save scheduler state
            }
            torch.save(save_dict, checkpoint_path)
            print(f"  ✓ Checkpoint saved: {checkpoint_path}")

            # Save training history
            if self.history_path:
                 try:
                    history_data = {
                        'train_losses': self.train_losses,
                        'val_losses': self.val_losses,
                        'best_val_loss': self.best_val_loss
                    }
                    with open(self.history_path, 'w') as f:
                        json.dump(history_data, f, indent=2)
                    # print("  ✓ Training history saved.") # Suppress verbose print
                 except Exception as e:
                     print(f"Warning: Could not save training history to {self.history_path}: {e}")


            if is_best:
                best_path = os.path.join(self.checkpoint_dir, 'best_model.pt')
                torch.save(self.model.state_dict(), best_path) # Save only model state for 'best_model.pt'
                # print(f"  ✓ Best model state saved: {best_path}") # Suppress verbose print


    def train_epoch(self):
        self.model.train()
        total_loss = 0
        progress_bar = tqdm(self.train_loader, desc=f"Training {self.model_name}")

        for batch_idx, batch in enumerate(progress_bar):
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # Forward pass
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss

            # Backward pass
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_grad_norm)

            # Optimizer step
            self.optimizer.step()
            self.scheduler.step()

            # Note: No manual weight constraints needed with softplus parametrization
            # W = softplus(V) is always ≥ 0 automatically

            self.optimizer.zero_grad()

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': loss.item(), 'lr': self.scheduler.get_last_lr()[0]})

        avg_loss = total_loss / len(self.train_loader)
        return avg_loss

    def validate(self):
        self.model.eval()
        total_loss = 0

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss
                total_loss += outputs.loss.item()

        avg_loss = total_loss / len(self.val_loader)
        return avg_loss

    def train(self):
        print(f"\nStarting training from Epoch {self.start_epoch + 1}/{self.num_epochs}...") # Adjusted start epoch print
        print("="*80)

        # Ensure model_monotonic_finetuned is initialized before the loop if resuming from a checkpoint
        if self.start_epoch > 0 and 'model_monotonic_finetuned' not in globals():
             global model_monotonic_finetuned
             model_monotonic_finetuned = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(self.device)
             model_monotonic_finetuned = make_model_monotonic(model_monotonic_finetuned)
             model_monotonic_finetuned.load_state_dict(self.model.state_dict()) # Load state from the loaded checkpoint
             model_monotonic_finetuned.eval()


        for epoch in range(self.start_epoch, self.num_epochs):
            print(f"\nEpoch {epoch + 1}/{self.num_epochs}")
            print("-"*80)

            # Train
            train_loss = self.train_epoch()
            self.train_losses.append(train_loss)

            # Validate
            val_loss = self.validate()
            self.val_losses.append(val_loss)

            print(f"\nEpoch {epoch + 1} Results:")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Val Loss:   {val_loss:.4f}")

            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss

            self.save_checkpoint(epoch + 1, val_loss, is_best) # Save checkpoint after each epoch

        print("\n" + "="*80)
        print(" Training complete!")
        print(f"  Best validation loss: {self.best_val_loss:.4f}")

        return self.train_losses, self.val_losses

# Ensure tokenizer and standard model are loaded (they are needed for evaluation regardless of training)
if 'tokenizer' not in globals() or 'model_standard' not in globals():
     tokenizer = T5Tokenizer.from_pretrained(ExperimentConfig.MODEL_NAME)
     model_standard = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
     model_standard.eval()
     print("Loaded tokenizer and standard model for evaluation.")

# Ensure monotonic unfinetuned model is loaded (needed for comparison plot)
if 'model_monotonic' not in globals():
    model_monotonic = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
    model_monotonic = make_model_monotonic(model_monotonic)
    model_monotonic.eval()
    print("Loaded unfinetuned monotonic model for comparison.")


    # ======================================================================
# Train BOTH Baseline and Monotonic Models with Identical Settings
    # ======================================================================

# Check if baseline training is needed
baseline_best_path_check = os.path.join(baseline_checkpoint_dir, 'best_model.pt')
baseline_training_needed = not os.path.exists(baseline_best_path_check)

# Check if monotonic training is needed
monotonic_best_path_check = os.path.join(monotonic_checkpoint_dir, 'best_model.pt')
monotonic_training_needed = not os.path.exists(monotonic_best_path_check)

    print("\n" + "="*80)
print("TRAINING STATUS CHECK")
print("="*80)
print(f"Baseline model training needed: {baseline_training_needed}")
print(f"Monotonic model training needed: {monotonic_training_needed}")
    print("="*80)

# Train Baseline Model (if needed)
if baseline_training_needed:
    print("\n" + "="*80)
    print("TRAINING BASELINE T5 MODEL (Unconstrained)")
    print("="*80)
    print("This model uses standard T5 architecture with NO monotonicity constraints.")
    print("Training with IDENTICAL hyperparameters as monotonic model for fair comparison.")
    
    # Reload fresh baseline model for training
    print("\nInitializing fresh baseline model...")
    model_baseline_train = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
    
    # Create trainer (no constraints)
    baseline_trainer = T5Trainer(
        model=model_baseline_train,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        is_monotonic=False,  # No weight constraints
        checkpoint_dir=baseline_checkpoint_dir,
        history_path=baseline_history_path,
        model_name="Baseline T5"
    )
    
    # Train
    print("\nStarting baseline training...")
    baseline_start_time = time.time()
    baseline_train_losses, baseline_val_losses = baseline_trainer.train()
    baseline_training_time = time.time() - baseline_start_time
    
    print(f"\n✓ Baseline training complete in {baseline_training_time:.1f}s ({baseline_training_time/60:.1f} minutes)")
    
    # Clean up
    del model_baseline_train, baseline_trainer
    torch.cuda.empty_cache()
else:
    print("\n✓ Baseline model already trained. Skipping baseline training.")

# Train Monotonic Model (if needed)
if monotonic_training_needed:
    print("\n" + "="*80)
    print("TRAINING MONOTONIC T5 MODEL (W ≥ 0 Constraints)")
    print("="*80)
    print("This model enforces non-negative weights in feed-forward layers.")
    print("Training with IDENTICAL hyperparameters as baseline model for fair comparison.")
    
    # Reload fresh monotonic model for training
    print("\nInitializing fresh monotonic model...")
    model_monotonic_train = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
    model_monotonic_train = make_model_monotonic(model_monotonic_train)

    # Create trainer (with constraints)
    monotonic_trainer = T5Trainer(
        model=model_monotonic_train,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        is_monotonic=True,  # Enforce weight constraints
        checkpoint_dir=monotonic_checkpoint_dir,
        history_path=monotonic_history_path,
        model_name="Monotonic T5"
    )

    # Train
    print("\nStarting monotonic training...")
    monotonic_start_time = time.time()
    monotonic_train_losses, monotonic_val_losses = monotonic_trainer.train()
    monotonic_training_time = time.time() - monotonic_start_time
    
    print(f"\n✓ Monotonic training complete in {monotonic_training_time:.1f}s ({monotonic_training_time/60:.1f} minutes)")
    
    # Clean up
    del model_monotonic_train, monotonic_trainer
    torch.cuda.empty_cache()
else:
    print("\n✓ Monotonic model already trained. Skipping monotonic training.")


# ======================================================================
# Load Best Fine-tuned Models After Training
# ======================================================================

print("\n" + "="*80)
print("LOADING FINE-TUNED MODELS")
print("="*80)

# Load Baseline Model
baseline_best_model_path = os.path.join(baseline_checkpoint_dir, 'best_model.pt')
if os.path.exists(baseline_best_model_path):
    print(f"\nLoading best fine-tuned baseline model from: {baseline_best_model_path}")
    model_baseline_finetuned = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
    model_baseline_finetuned.load_state_dict(torch.load(baseline_best_model_path, map_location=device))
    model_baseline_finetuned.eval()
    print("✓ Baseline model loaded.")
else:
    print("\n⚠ Warning: Fine-tuned baseline model not found!")
    model_baseline_finetuned = None

# Load Monotonic Model
monotonic_best_model_path = os.path.join(monotonic_checkpoint_dir, 'best_model.pt')
if os.path.exists(monotonic_best_model_path):
    print(f"\nLoading best fine-tuned monotonic model from: {monotonic_best_model_path}")
    model_monotonic_finetuned = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
    model_monotonic_finetuned = make_model_monotonic(model_monotonic_finetuned)
    model_monotonic_finetuned.load_state_dict(torch.load(monotonic_best_model_path, map_location=device))
    model_monotonic_finetuned.eval()
    print("✓ Monotonic model loaded.")
else:
    print("\n⚠ Warning: Fine-tuned monotonic model not found!")
    model_monotonic_finetuned = None

# For backward compatibility
best_model_state_path = monotonic_best_model_path


# Load history for plotting if it exists
history = {}
if os.path.exists(history_path):
    try:
        with open(history_path, 'r') as f:
            history = json.load(f)
            train_losses = history.get('train_losses', [])
            val_losses = history.get('val_losses', [])
            best_val_loss = history.get('best_val_loss', float('inf'))
            print(" Loaded training history for visualization.")
    except Exception as e:
         print(f"Warning: Could not load training history from {history_path} for plotting: {e}")
         train_losses = []
         val_losses = []
         best_val_loss = float('inf') # Reset if load fails
else:
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    print("No training history found for visualization.")


# Re-calculate validation losses for plotting if needed (e.g., if loaded without history)
if not train_losses or not val_losses:
     print("\n Calculating validation losses for comparison plot (no history found)...")
     # Ensure val_loader is defined and loaded
     if 'val_loader' not in globals():
        # Need to load validation data and create loader if history wasn't loaded
        # CRITICAL: Use validation splits only (NOT test splits) to avoid data leakage
        print("  Loading validation data and creating DataLoader...")
        dlg_val_x, dlg_val_y = _materialize(_collect_pairs_dialogsum("validation"), "DialogSum(val)")
        sam_val_x, sam_val_y = _materialize(_collect_pairs_samsum("validation"), "SAMSum(val)") # FIXED
        try:
            meet_val_x, meet_val_y = _materialize(_collect_pairs_meetingsummary("validation"), "MEETING_SUMMARY(val)") # FIXED
        except:
            meet_val_x, meet_val_y = [], []
        xsum_val_x, xsum_val_y = _materialize(_collect_pairs_xsum("validation"), "XSUM(val)") # FIXED
        try:
            ami_val_x, ami_val_y = _materialize(_collect_pairs_ami("validation"), "AMI(val)") # FIXED
        except:
            ami_val_x, ami_val_y = [], []
        try:
            highlight_val_x, highlight_val_y = _materialize(_collect_pairs_highlightsum("validation"), "HighlightSum(val)") # FIXED
        except:
            highlight_val_x, highlight_val_y = [], []
        arxiv_val_x, arxiv_val_y = _materialize(_collect_pairs_arxiv("validation"), "ArXiv(val)")

        val_texts = dlg_val_x + sam_val_x + meet_val_x + xsum_val_x + ami_val_x + highlight_val_x + arxiv_val_x
        val_summaries = dlg_val_y + sam_val_y + meet_val_y + xsum_val_y + ami_val_y + highlight_val_y + arxiv_val_y

        batch_size = 4
        val_dataset = SummarizationDataset(val_texts, val_summaries, tokenizer) # Need SummarizationDataset class defined above
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)


     if model_standard is not None:
        val_loss_std = compute_avg_loss(model_standard, val_loader, device)
     else:
        val_loss_std = float('nan')

     if 'model_monotonic' in globals() and model_monotonic is not None:
        val_loss_mono_before = compute_avg_loss(model_monotonic, val_loader, device)
     else:
        val_loss_mono_before = float('nan')


     if model_monotonic_finetuned is not None:
        val_loss_mono_after = compute_avg_loss(model_monotonic_finetuned, val_loader, device)
     else:
        val_loss_mono_after = float('nan')

     print("✓ Validation losses calculated for plotting.")


# ======================================================================
# Compare Before and After Fine-tuning (Move outside if/else) - Now happens after potential training/loading
# ======================================================================

print("\n" + "="*80)
print("COMPARING MODELS")
print("="*80)

# Test on sample (Ensure models are available)
if model_standard is not None and model_monotonic_finetuned is not None:
    test_text = """
    Person1: Hi, I need to book a flight to New York for next Tuesday.
    Person2: Sure! I can help with that. What time would you prefer?
    Person1: Morning flights work best for me, around 8 or 9 AM.
    Person2: Perfect. I found a 8:30 AM departure. Should I book it?
    Person1: Yes, please go ahead.
    """

    print("\nTest Input:")
    print(test_text)

    print("\n" + "-"*80)
    print("SUMMARIES:")
    print("-"*80)

    summary_std = generate_summary_fixed_params(model_standard, test_text, tokenizer, device)
    print(f"\n1. Standard T5 (baseline):")
    print(f"   {summary_std}")

    # Ensure model_monotonic is defined and loaded before use
    if 'model_monotonic' in globals() and model_monotonic is not None:
        summary_mono_before = generate_summary_fixed_params(model_monotonic, test_text, tokenizer, device)
        print(f"\n2. Monotonic T5 (before fine-tuning):")
        print(f"   {summary_mono_before}")
    else:
        print(f"\n2. Monotonic T5 (before fine-tuning): Model not available for testing.")


    summary_mono_after = generate_summary_fixed_params(model_monotonic_finetuned, test_text, tokenizer, device)
    print(f"\n3. Monotonic T5 (after fine-tuning):")
    print(f"   {summary_mono_after}")
else:
     print("\nSkipping sample comparison - Models not loaded.")


# ======================================================================
# Visualize Training Progress (Move outside if/else) - Now happens after potential training/loading
# ======================================================================

print("\n" + "="*80)
print("TRAINING VISUALIZATION")
print("="*80)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Training curves (Only if history is available)
if train_losses and val_losses:
    ax = axes[0]
    epochs = range(1, len(train_losses) + 1)
    ax.plot(epochs, train_losses, 'o-', label='Train Loss', linewidth=2, markersize=8)
    ax.plot(epochs, val_losses, 's-', label='Val Loss', linewidth=2, markersize=8)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Loss', fontsize=12)
    ax.set_title('Training Progress', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
else:
    # If no training history, plot a placeholder or leave blank
    axes[0].set_title('Training Progress (No History)', fontsize=14, fontweight='bold')
    axes[0].text(0.5, 0.5, "Training history not available", horizontalalignment='center', verticalalignment='center', transform=axes[0].transAxes, fontsize=12)
    axes[0].set_xticks([])
    axes[0].set_yticks([])


# Plot 2: Final comparison
ax = axes[1]

# Only plot if validation losses were calculated
if 'val_loss_std' in locals() and 'val_loss_mono_before' in locals() and 'val_loss_mono_after' in locals():
    models = ['Standard\nT5', 'Monotonic\n(before FT)', 'Monotonic\n(after FT)']

    # Use the calculated validation losses
    losses = [val_loss_std, val_loss_mono_before, val_loss_mono_after]
    colors = ['#3498db', '#e74c3c', '#27ae60']

    bars = ax.bar(models, losses, color=colors, alpha=0.7, width=0.6)
    ax.set_ylabel('Validation Loss', fontsize=12)
    ax.set_title('Model Comparison', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

    # Add value labels
    for bar, loss in zip(bars, losses):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{loss:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
else:
    ax.set_title('Model Comparison (No Validation Loss Data)', fontsize=14, fontweight='bold')
    ax.text(0.5, 0.5, "Validation loss data not available", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12)
    ax.set_xticks([])
    ax.set_yticks([])


plt.tight_layout()
plot_path = os.path.join(SAVE_PATH, 'finetuning_results.png')
plt.savefig(plot_path, dpi=150, bbox_inches='tight')
plt.show()

print(f"\n✓ Visualization saved to: {plot_path}")

# ======================================================================
# Summary (Move outside if/else) - Now happens after potential training/loading
# ======================================================================

print("\n" + "="*80)
print("FINE-TUNING SUMMARY")
print("="*80)

# Only print summary if validation losses were calculated
if 'val_loss_std' in locals() and 'val_loss_mono_before' in locals() and 'val_loss_mono_after' in locals():
    print(f"\nValidation Loss:")
    print(f"  Standard T5:                {val_loss_std:.4f}")
    # Ensure val_loss_std is not zero before division
    if val_loss_std > 1e-8:
        print(f"  Monotonic (before FT):      {val_loss_mono_before:.4f} ({((val_loss_mono_before - val_loss_std)/val_loss_std*100):+.1f}%)")
        print(f"  Monotonic (after FT):       {val_loss_mono_after:.4f} ({((val_loss_mono_after - val_loss_std)/val_loss_std*100):+.1f}%)")
    else:
         print(f"  Monotonic (before FT):      {val_loss_mono_before:.4f}")
         print(f"  Monotonic (after FT):       {val_loss_mono_after:.4f}")
         print("  Relative comparison skipped due to near-zero Standard T5 validation loss.")

"""## Inference Comparison: Standard T5 vs. Fine-tuned Monotonic T5 (PRELIMINARY)

**NOTE:** This is a PRELIMINARY comparison for quick inspection only.
The PRIMARY, methodologically rigorous three-way comparison (with bootstrap CIs, 
token-level lengths, and all three models) is in the "COMPREHENSIVE THREE-WAY EVALUATION" 
section below.

This section provides a quick look at Standard vs Monotonic (without baseline) using 
word-level lengths (not token-level). Refer to the comprehensive section for 
publication-quality results.
"""

# ======================================================================
# Inference Comparison: Standard T5 vs. Fine-tuned Monotonic T5
# ======================================================================

print("="*80)
print("INFERENCE COMPARISON: Standard T5 vs. Fine-tuned Monotonic T5")
print("="*80)

# Ensure models are loaded (should be from previous cells)
if 'model_standard' not in globals():
    print(" Standard T5 model not found. Please run the model loading cells.")
    model_standard = None # Set to None to avoid errors later
if 'model_monotonic_finetuned' not in globals():
    print(" Fine-tuned Monotonic T5 model not found. Please run the fine-tuning cell.")
    model_monotonic_finetuned = None # Set to None

if model_standard is None or model_monotonic_finetuned is None:
    print("\nSkipping comparison due to missing models.")
else:
    # Ensure test data is loaded (should be from the data loading cell)
    if 'inference_test_texts' not in globals() or 'inference_test_summaries' not in globals():
        print("\n Consistent test data not found. Please run the data loading cell.")
        print("Attempting to load data again (this might cause inconsistency if seed changed)...")
        try:
            # Load a smaller subset for faster testing in Colab
            # Use a consistent seed for reproducibility of the subset selection
            test_dataset_full = load_dataset("cnn_dailymail", '3.0.0', split="test")
            np.random.seed(42) # Ensure the same subset is picked each time
            subset_indices = np.random.choice(len(test_dataset_full), 200, replace=False) # Use 200 samples consistently
            test_dataset = test_dataset_full.select(subset_indices)

            inference_test_texts = [example['article'] for example in test_dataset]
            inference_test_summaries = [example['highlights'] for example in test_dataset]
            print(f" Loaded {len(inference_test_texts)} test samples for consistent use across experiments.")
        except Exception as e:
            print(f"Error loading cnn_dailymail dataset: {e}")
            inference_test_texts = []
            inference_test_summaries = []


    if not inference_test_texts:
        print("\nSkipping comparison as test data is not available.")
    else:
        # Ensure utility functions are defined (should be from initial inference cell)
        if 'generate_summary' not in globals() or 'compute_perplexity' not in globals():
             print("\n Utility functions (generate_summary, compute_perplexity) not found. Please run the initial inference cell.")
             # Define dummy functions to avoid errors, though results will be invalid
             def generate_summary(*args, **kwargs): return "Error: Function not defined"
             def compute_perplexity(*args, **kwargs): return float('nan')


        print("\nRunning inference on both models...")
        results_comparison = []

        # Use the stored test data and add tqdm for progress bar
        for i, (article_text, reference_summary) in tqdm(enumerate(zip(inference_test_texts, inference_test_summaries)), total=len(inference_test_texts), desc="Inference Comparison"):
            title = f"Article {i+1}" # Use a generic title

            # Generate summaries using fixed parameters for fair comparison
            summary_std = generate_summary_fixed_params(model_standard, article_text, tokenizer, device)
            summary_mono = generate_summary_fixed_params(model_monotonic_finetuned, article_text, tokenizer, device)

            # Compute perplexities (auxiliary metric - not primary comparison)
            try:
                ppl_std = compute_perplexity(model_standard, article_text, summary_std, tokenizer)
                ppl_mono = compute_perplexity(model_monotonic_finetuned, article_text, summary_mono, tokenizer) # Compute perplexity given input
            except Exception as e:
                # print(f"Error computing perplexity (Test {i+1}): {e}") # Suppress verbose error
                ppl_std, ppl_mono = float('nan'), float('nan')

            # Length
            len_std = len(summary_std.split())
            len_mono = len(summary_mono.split())

            # Compute ROUGE scores (more relevant for summarization quality)
            # Ensure rouge_scorer is imported (should be from UAT cell)
            if 'rouge_scorer' not in globals():
                try:
                    from rouge_score import rouge_scorer
                except ImportError:
                    import subprocess, sys
                    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "rouge-score"])
                 from rouge_score import rouge_scorer

            scorer = rouge_scorer.RougeScorer(["rouge1","rouge2","rougeLsum"], use_stemmer=True)
            scores_std = scorer.score(reference_summary, summary_std)
            scores_mono = scorer.score(reference_summary, summary_mono)

            results_comparison.append({
                'title': title,
                'input_length': len(article_text.split()),
                'reference_summary': reference_summary,
                'summary_std': summary_std,
                'summary_mono': summary_mono,
                'ppl_std': ppl_std,
                'ppl_mono': ppl_mono,
                'len_std': len_std,
                'len_mono': len_mono,
                'rouge_std': {k: v.fmeasure for k, v in scores_std.items()},
                'rouge_mono': {k: v.fmeasure for k, v in scores_mono.items()},
            })

            # Only break early if not using full test sets
            if not ExperimentConfig.USE_FULL_TEST_SETS and i >= 199:
                break


        # ======================================================================
        # Aggregate Statistics
        # ======================================================================

        print("\n" + "="*80)
        print("AGGREGATE STATISTICS")
        print("="*80)

        # Filter out NaN perplexity values for aggregation
        valid_ppl_std = [r['ppl_std'] for r in results_comparison if not torch.isnan(torch.tensor(r['ppl_std']))]
        valid_ppl_mono = [r['ppl_mono'] for r in results_comparison if not torch.isnan(torch.tensor(r['ppl_mono']))]


        avg_ppl_std = np.mean(valid_ppl_std) if valid_ppl_std else float('nan')
        avg_ppl_mono = np.mean(valid_ppl_mono) if valid_ppl_mono else float('nan')
        avg_len_std = np.mean([r['len_std'] for r in results_comparison])
        avg_len_mono = np.mean([r['len_mono'] for r in results_comparison])

        # Aggregate ROUGE scores
        avg_rouge_std = {
            k: np.mean([r['rouge_std'][k] for r in results_comparison])
            for k in ["rouge1","rouge2","rougeLsum"]
        }
        avg_rouge_mono = {
            k: np.mean([r['rouge_mono'][k] for r in results_comparison])
            for k in ["rouge1","rouge2","rougeLsum"]
        }
        rouge_delta = {
             k: avg_rouge_mono[k] - avg_rouge_std[k]
             for k in ["rouge1","rouge2","rougeLsum"]
        }


        print(f"\nPerplexity (on generated text):")
        print(f"  Standard:  {avg_ppl_std:.4f} (avg)")
        print(f"  Monotonic: {avg_ppl_mono:.4f} (avg)")

        print(f"\nSummary Length:")
        print(f"  Standard:  {avg_len_std:.1f} words (avg)")
        print(f"  Monotonic: {avg_len_mono:.1f} words (avg)")

        print(f"\nROUGE Scores (F1):")
        print(f"  Standard:  R1={avg_rouge_std['rouge1']:.3f}, R2={avg_rouge_std['rouge2']:.3f}, RL={avg_rouge_std['rougeLsum']:.3f}")
        print(f"  Monotonic: R1={avg_rouge_mono['rouge1']:.3f}, R2={avg_rouge_mono['rouge2']:.3f}, RL={avg_rouge_mono['rougeLsum']:.3f}")
        print(f"  ΔROUGE:    ΔR1={rouge_delta['rouge1']:.3f}, ΔR2={rouge_delta['rouge2']:.3f}, ΔRL={rouge_delta['rougeLsum']:.3f}")


        # ======================================================================
        # Quality Assessment Summary
        # ======================================================================

        print("\n" + "="*80)
        print("QUALITY ASSESSMENT SUMMARY")
        print("="*80)

        print("\n Key Findings:")

        # Assess based on ROUGE-Lsum
        rouge_l_diff_percent = (rouge_delta['rougeLsum'] / avg_rouge_std['rougeLsum']) * 100 if avg_rouge_std['rougeLsum'] > 0 else float('nan')

        if not np.isnan(rouge_l_diff_percent):
            if abs(rouge_l_diff_percent) < 5:
                 print(f"  • Fine-tuned monotonic model achieves COMPARABLE ROUGE-Lsum ({rouge_l_diff_percent:+.1f}%)")
            elif rouge_l_diff_percent > 0:
                 print(f"  • Fine-tuned monotonic model achieves SLIGHTLY BETTER ROUGE-Lsum ({rouge_l_diff_percent:+.1f}%)")
            else:
                 print(f"  • Fine-tuned monotonic model shows SLIGHTLY WORSE ROUGE-Lsum ({rouge_l_diff_percent:+.1f}%)")
        else:
             print("  • ROUGE-Lsum comparison not available.")


        # Using length as secondary indicator
        len_diff_percent = (avg_len_mono - avg_len_std) / avg_len_std * 100 if avg_len_std > 0 else float('nan')
        if not np.isnan(len_diff_percent) and abs(len_diff_percent) > 10:
            print(f"  • Note: Monotonic model summary length differs significantly ({len_diff_percent:+.1f}%)")


        print("\nInterpretation:")
        if not np.isnan(rouge_l_diff_percent) and abs(rouge_l_diff_percent) < 10:
             print("  The monotonic constraint (W ≥ 0) on the fine-tuned model, after training,")
             print("  appears to preserve summarization quality reasonably well compared to the")
             print("  standard model, based on ROUGE scores.")
             print("  Ready for adversarial robustness testing!")
        else:
             print("  The monotonic constraint seems to have impacted summarization quality more significantly.")
             print("  This might affect the interpretation of adversarial testing results.")
             print("  Consider if the observed quality difference is acceptable for your goals.")


        # ======================================================================
        # Detailed Side-by-Side Comparison (First few examples)
        # ======================================================================

        print("\n" + "="*80)
        print("SIDE-BY-SIDE COMPARISON (First 5 Examples)")
        print("="*80)

        for i, result in enumerate(results_comparison[:5]): # Show only first 5 examples
            print(f"\n{i+1}. {result['title']}:")
            print(f"   Reference: {result['reference_summary']}")
            print(f"   Standard:  {result['summary_std']}")
            print(f"   Monotonic: {result['summary_mono']}")
            print(f"   ROUGE (Std): R1={result['rouge_std']['rouge1']:.3f}, R2={result['rouge_std']['rouge2']:.3f}, RL={result['rouge_std']['rougeLsum']:.3f}")
            print(f"   ROUGE (Mon): R1={result['rouge_mono']['rouge1']:.3f}, R2={result['rouge_mono']['rouge2']:.3f}, RL={result['rouge_mono']['rougeLsum']:.3f}")


        # ======================================================================
        # Save Results
        # ======================================================================

        print("\n" + "="*80)
        print("Saving comparison results...")

        results_comparison_dict = {
            'test_cases': [{ # Save only relevant info per case
                'title': r['title'],
                'input_length': r['input_length'],
                'reference_summary': r['reference_summary'],
                'summary_std': r['summary_std'],
                'summary_mono': r['summary_mono'],
                'ppl_std': r['ppl_std'],
                'ppl_mono': r['ppl_mono'],
                'len_std': r['len_std'],
                'len_mono': r['len_mono'],
                'rouge_std': r['rouge_std'],
                'rouge_mono': r['rouge_mono'],
            } for r in results_comparison],
            'aggregate': {
                'avg_ppl_std': avg_ppl_std,
                'avg_ppl_mono': avg_ppl_mono,
                'avg_len_std': avg_len_std,
                'avg_len_mono': avg_len_mono,
                'avg_rouge_std': avg_rouge_std,
                'avg_rouge_mono': avg_rouge_mono,
                'rouge_delta': rouge_delta,
                'total_tests': len(results_comparison),
            }
        }

        results_path_comparison = os.path.join(SAVE_PATH, 'inference_comparison_standard_vs_monotonic_cnn_dailymail.json')
        with open(results_path_comparison, 'w') as f:
            json.dump(results_comparison_dict, f, indent=2)

        print(f"✓ Comparison results saved to: {results_path_comparison}")

        # ======================================================================
        # Visualization
        # ======================================================================

        import matplotlib.pyplot as plt

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))

        # Plot 1: ROUGE-Lsum Comparison
        ax = axes[0]
        models = ['Standard', 'Monotonic (FT)']
        rouge_l_scores = [avg_rouge_std['rougeLsum'], avg_rouge_mono['rougeLsum']]
        colors = ['#3498db', '#e74c3c']

        bars = ax.bar(models, rouge_l_scores, color=colors, alpha=0.7, width=0.6)
        ax.set_ylabel('ROUGE-Lsum (F1)', fontsize=12)
        ax.set_title('Average ROUGE-Lsum Comparison', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_ylim([0, 0.5]) # Adjust based on expected ROUGE scores

        for bar, score in zip(bars, rouge_l_scores):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{score:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')


        # Plot 2: All ROUGE Metrics
        ax = axes[1]
        x_pos = np.arange(len(["rouge1","rouge2","rougeLsum"]))
        width = 0.35

        std_scores = [avg_rouge_std[k] for k in ["rouge1","rouge2","rougeLsum"]]
        mono_scores = [avg_rouge_mono[k] for k in ["rouge1","rouge2","rougeLsum"]]

        ax.bar(x_pos - width/2, std_scores, width, label='Standard', alpha=0.8, color='#3498db')
        ax.bar(x_pos + width/2, mono_scores, width, label='Monotonic (FT)', alpha=0.8, color='#e74c3c')
        ax.set_ylabel('ROUGE Score (F1)', fontsize=12)
        ax.set_title('Average ROUGE Scores (All Metrics)', fontsize=14, fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(["ROUGE-1","ROUGE-2","ROUGE-Lsum"], fontsize=10)
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_ylim([0, 0.5]) # Adjust based on expected ROUGE scores


        # Plot 3: Length Comparison (Aggregate)
        ax = axes[2]
        models = ['Standard', 'Monotonic (FT)']
        lengths = [avg_len_std, avg_len_mono]
        colors = ['#3498db', '#e74c3c']

        bars = ax.bar(models, lengths, color=colors, alpha=0.7, width=0.6)
        ax.set_ylabel('Average Summary Length (words)', fontsize=12)
        ax.set_title('Average Summary Length Comparison', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')

        for bar, length in zip(bars, lengths):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{length:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')


        plt.tight_layout()
        plot_path = os.path.join(SAVE_PATH, 'inference_comparison_standard_vs_monotonic_cnn_dailymail.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.show()

        print(f"✓ Visualization saved to: {plot_path}")


        print("\n" + "="*80)
        print("✓ INFERENCE COMPARISON COMPLETE")
        print("="*80)

"""## COMPREHENSIVE THREE-WAY EVALUATION WITH BOOTSTRAP CIs

This section provides the PRIMARY comparison between all three models using:
- Fixed decoding parameters (from ExperimentConfig)
- Bootstrap 95% confidence intervals
- Full test sets (CNN/DM, XSUM, SAMSum)
- Token-level length statistics
"""

# ======================================================================
# THREE-WAY COMPREHENSIVE EVALUATION (Primary Results)
# ======================================================================

print("\n" + "="*80)
print("COMPREHENSIVE THREE-WAY EVALUATION - PRIMARY RESULTS")
print("="*80)
print("Evaluating all three models with bootstrap CIs on multiple test sets...")
print("="*80)

# Ensure all three models are available
if model_baseline_finetuned is None or model_monotonic_finetuned is None:
    print("\nWARNING: Cannot perform comprehensive three-way evaluation:")
    if model_baseline_finetuned is None:
        print("    - Baseline model not trained/loaded")
    if model_monotonic_finetuned is None:
        print("    - Monotonic model not trained/loaded")
    print("\n Please train both models first for fair comparison.")
    print(" Skipping comprehensive evaluation.")
else:
    # Evaluate on CNN/DailyMail
    print("\n" + "="*80)
    print("DATASET 1: CNN/DailyMail")
    print("="*80)
    
    eval_standard_cnn = evaluate_model_comprehensive(
        model_standard, cnn_dm_test_texts, cnn_dm_test_summaries, 
        tokenizer, device, "Standard T5 (pre-trained)"
    )
    
    eval_baseline_cnn = evaluate_model_comprehensive(
        model_baseline_finetuned, cnn_dm_test_texts, cnn_dm_test_summaries,
        tokenizer, device, "Baseline T5 (fine-tuned, unconstrained)"
    )
    
    eval_monotonic_cnn = evaluate_model_comprehensive(
        model_monotonic_finetuned, cnn_dm_test_texts, cnn_dm_test_summaries,
        tokenizer, device, "Monotonic T5 (fine-tuned, W≥0)"
    )
    
    # Evaluate on XSUM
    if len(xsum_test_texts) > 0:
        print("\n" + "="*80)
        print("DATASET 2: XSUM")
        print("="*80)
        
        eval_standard_xsum = evaluate_model_comprehensive(
            model_standard, xsum_test_texts, xsum_test_summaries,
            tokenizer, device, "Standard T5 (pre-trained)"
        )
        
        eval_baseline_xsum = evaluate_model_comprehensive(
            model_baseline_finetuned, xsum_test_texts, xsum_test_summaries,
            tokenizer, device, "Baseline T5 (fine-tuned, unconstrained)"
        )
        
        eval_monotonic_xsum = evaluate_model_comprehensive(
            model_monotonic_finetuned, xsum_test_texts, xsum_test_summaries,
            tokenizer, device, "Monotonic T5 (fine-tuned, W≥0)"
        )
    
    # Evaluate on SAMSum
    if len(samsum_test_texts) > 0:
        print("\n" + "="*80)
        print("DATASET 3: SAMSum")
        print("="*80)
        
        eval_standard_samsum = evaluate_model_comprehensive(
            model_standard, samsum_test_texts, samsum_test_summaries,
            tokenizer, device, "Standard T5 (pre-trained)"
        )
        
        eval_baseline_samsum = evaluate_model_comprehensive(
            model_baseline_finetuned, samsum_test_texts, samsum_test_summaries,
            tokenizer, device, "Baseline T5 (fine-tuned, unconstrained)"
        )
        
        eval_monotonic_samsum = evaluate_model_comprehensive(
            model_monotonic_finetuned, samsum_test_texts, samsum_test_summaries,
            tokenizer, device, "Monotonic T5 (fine-tuned, W≥0)"
        )
    
    # Print final comparison table
    print("\n" + "="*80)
    print("FINAL COMPARISON TABLE (with Bootstrap 95% CIs)")
    print("="*80)
    print("\nCNN/DailyMail Results:")
    print(f"{'Model':<40} {'ROUGE-1':>15} {'ROUGE-2':>15} {'ROUGE-L':>15} {'Length Ratio':>15}")
    print("-" * 100)
    
    for eval_result, name in [(eval_standard_cnn, "Standard T5"),
                               (eval_baseline_cnn, "Baseline T5 (FT)"), 
                               (eval_monotonic_cnn, "Monotonic T5 (FT)")]:
        r1 = eval_result['rouge_scores']['rouge1']
        r2 = eval_result['rouge_scores']['rouge2']
        rl = eval_result['rouge_scores']['rougeLsum']
        lr = eval_result['brevity_penalty_info']['length_ratio']
        print(f"{name:<40} "
              f"{r1['mean']:.4f}[±{r1['ci_width']:.4f}] "
              f"{r2['mean']:.4f}[±{r2['ci_width']:.4f}] "
              f"{rl['mean']:.4f}[±{rl['ci_width']:.4f}] "
              f"{lr:.3f}")
    
    print("\n" + "="*80)
    print("KEY FINDINGS:")
    print("="*80)
    
    # Compare Baseline vs Monotonic (the key comparison)
    baseline_rl = eval_baseline_cnn['rouge_scores']['rougeLsum']['mean']
    monotonic_rl = eval_monotonic_cnn['rouge_scores']['rougeLsum']['mean']
    baseline_ci = eval_baseline_cnn['rouge_scores']['rougeLsum']['ci_width']
    monotonic_ci = eval_monotonic_cnn['rouge_scores']['rougeLsum']['ci_width']
    
    print(f"\nBaseline vs Monotonic (ROUGE-L on CNN/DM):")
    print(f"  Baseline:  {baseline_rl:.4f} ± {baseline_ci:.4f}")
    print(f"  Monotonic: {monotonic_rl:.4f} ± {monotonic_ci:.4f}")
    print(f"  Difference: {monotonic_rl - baseline_rl:+.4f}")
    
    # Check if CIs overlap
    baseline_lower = eval_baseline_cnn['rouge_scores']['rougeLsum']['lower']
    baseline_upper = eval_baseline_cnn['rouge_scores']['rougeLsum']['upper']
    monotonic_lower = eval_monotonic_cnn['rouge_scores']['rougeLsum']['lower']
    monotonic_upper = eval_monotonic_cnn['rouge_scores']['rougeLsum']['upper']
    
    cis_overlap = not (baseline_upper < monotonic_lower or monotonic_upper < baseline_lower)
    
    if cis_overlap:
        print(f"  ✓ 95% CIs OVERLAP - No significant difference in quality")
    else:
        if monotonic_rl > baseline_rl:
            print(f"  Monotonic BETTER - CIs do not overlap")
        else:
            print(f"  WARNING: Baseline BETTER - CIs do not overlap")
    
    print("\n" + "="*80)

"""# Universal Adversarial Triggers (UAT)

## Intuitive explanation
A **universal adversarial trigger** is a short, fixed string of tokens (e.g., a few rare words or characters) that you can prepend or append to almost any input and reliably **steer a language model’s behavior**—causing wrong classifications, toxic generations, biased summaries, or a specific target output. Think of it like a “master key” phrase: instead of crafting a different perturbation per example, you find **one tiny phrase that works on most examples** because it exploits model quirks shared across inputs (brittle correlations in embeddings and attention that get activated by those tokens).

## Technical explanation
Let f_theta be a trained model and D be the data distribution. A UAT is a short token sequence t = (t1, ..., tk) (small k) that approximately maximizes expected loss over D:

- **Targeted (force a specific label/output y_tgt)**:  
  Maximize  E_{(x,y) ~ D}  L( f_theta( concat(t, x) ), y_tgt )

- **Untargeted (cause general failure)**:  
  Maximize  E_{(x,y) ~ D}  L( f_theta( concat(t, x) ), y )

Here, concat(t, x) denotes concatenation at the input level (prefix or suffix). Since tokens are discrete, optimization typically uses **gradient-guided search in embedding space** with a projection back to tokens. Common implementations:

- **HotFlip / coordinate ascent**: use gradients w.r.t. each trigger token’s embedding to propose substitutions that increase loss.
- **Continuous relaxation (e.g., Gumbel-Softmax)**: learn a soft distribution per position, then discretize.
- **Embedding-space PGD**: perturb continuous trigger embeddings with projected gradient steps, then map to nearest tokens.

**Why UATs generalize across inputs**

- The trigger **activates global, high-gain directions** in the network (e.g., attention key–query patterns or feature subspaces) that **systematically bias** downstream layers regardless of the specific x.
- Tokenization/embeddings create **shared nonlinear pathways**; a short sequence can consistently steer internal activations into regions with high loss or toward y_tgt.

**Practical notes**

- Placement: prefix often dominates for encoder–decoder and instruction-tuned models; suffix can work for classifiers and some generators.
- Constraints: small k, valid vocabulary, optional semantic/printability constraints.
- Transferability: UATs can transfer **across inputs, tasks, and sometimes models** trained on similar corpora.
- Mitigations: adversarial training with universal triggers, input sanitization/defensive prompting, confidence/rejection heads, and monitoring for anomalous token patterns.

### Imports and Setup
"""

import torch
import torch.nn.functional as F
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from collections import defaultdict

print("="*80)
print("UNIVERSAL ADVERSARIAL TRIGGER (UAT) ATTACK")
print("="*80)

"""### Load Dataset"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from transformers import T5ForConditionalGeneration, T5Tokenizer
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from datasets import load_dataset
import os

print("="*80)
print("LOADING MODELS FOR UAT ATTACK")
print("="*80)

# Ensure paths are set
if 'SAVE_PATH' not in globals():
    DRIVE_PATH = '/content/drive/MyDrive/t5_monotonic_experiment'
    SAVE_PATH = os.path.join(DRIVE_PATH, 't5_models')

if 'device' not in globals():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

"""### Load Standard T5"""

print("\n1. Loading Standard T5 (baseline)...")

tokenizer = T5Tokenizer.from_pretrained(ExperimentConfig.MODEL_NAME)
model_standard = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
model_standard.eval()

# Use sum(p.numel()...) for compatibility
num_params = sum(p.numel() for p in model_standard.parameters())
print(f"   Standard T5 loaded ({num_params:,} parameters)")

"""### Create Unfinetuned Monotonic Model (Fresh)"""

print("\n2. Creating unfinetuned monotonic model...")

# Reload fresh model and apply constraints
model_monotonic_unfinetuned = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
model_monotonic_unfinetuned = make_model_monotonic(model_monotonic_unfinetuned)
model_monotonic_unfinetuned.eval()

print(f"   Monotonic (unfinetuned) created with weight constraints")

"""### Load Fine-tuned Baseline Model (from checkpoint)"""

print("\n3. Loading fine-tuned BASELINE model from checkpoint...")

# Use correct checkpoint directory structure
baseline_checkpoint_dir = os.path.join(ExperimentConfig.SAVE_PATH, 'baseline_checkpoints')
baseline_best_path = os.path.join(baseline_checkpoint_dir, 'best_model.pt')

if os.path.exists(baseline_best_path):
    # Create architecture (standard T5, NO constraints)
    model_baseline_finetuned = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
    
    # Load trained weights
    state_dict = torch.load(baseline_best_path, map_location=device)
    model_baseline_finetuned.load_state_dict(state_dict)
    model_baseline_finetuned.eval()
    
    print(f"   Fine-tuned baseline loaded from: {baseline_best_path}")
else:
    print(f"   WARNING: Baseline checkpoint not found: {baseline_best_path}")
    print(f"   Baseline model will not be included in UAT evaluation.")
    model_baseline_finetuned = None

"""### Load Fine-tuned Monotonic Model (from checkpoint)"""

print("\n4. Loading fine-tuned MONOTONIC model from checkpoint...")

# Use correct checkpoint directory structure
monotonic_checkpoint_dir = os.path.join(ExperimentConfig.SAVE_PATH, 'monotonic_checkpoints')
monotonic_best_path = os.path.join(monotonic_checkpoint_dir, 'best_model.pt')

if os.path.exists(monotonic_best_path):
    # Create architecture first
    model_monotonic_finetuned = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
    model_monotonic_finetuned = make_model_monotonic(model_monotonic_finetuned)

    # Load trained weights
    state_dict = torch.load(monotonic_best_path, map_location=device)
    model_monotonic_finetuned.load_state_dict(state_dict)
    model_monotonic_finetuned.eval()

    print(f"   Fine-tuned monotonic loaded from: {monotonic_best_path}")
else:
    print(f"   WARNING: Monotonic checkpoint not found: {monotonic_best_path}")
    print(f"   Please run the fine-tuning code first!")
    model_monotonic_finetuned = None

"""### Model Verification"""

print("\n" + "="*80)
print("MODEL VERIFICATION")
print("="*80)

test_input = "Person1: Let's meet tomorrow. Person2: Sounds good!"

def quick_test(model, text):
    """Quick generation test"""
    inputs = tokenizer(
        "summarize: " + text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=50,
            num_beams=2,
            early_stopping=True
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"\nTest input: {test_input}")
print(f"\nStandard T5:")
print(f"  {quick_test(model_standard, test_input)}")

print(f"\nMonotonic (unfinetuned):")
print(f"  {quick_test(model_monotonic_unfinetuned, test_input)}")

print(f"\nMonotonic (finetuned):")
print(f"  {quick_test(model_monotonic_finetuned, test_input)}")

"""### Load Test Data for UAT"""

print("\n" + "="*80)
print("LOADING TEST DATA")
print("="*80)

# Load the cnn_dailymail dataset for testing
# Use the same logic as the inference comparison cell to ensure consistency
print("\nLoading cnn_dailymail dataset for consistent testing...")
try:
    # Use a consistent seed for reproducibility of the subset selection
    test_dataset_full = load_dataset("cnn_dailymail", '3.0.0', split="test")
    np.random.seed(42) # Ensure the same subset is picked each time
    subset_indices = np.random.choice(len(test_dataset_full), 200, replace=False) # Use 200 samples consistently
    test_dataset = test_dataset_full.select(subset_indices)

    print(f" Loaded {len(test_dataset)} samples from cnn_dailymail test split for consistent evaluation.")
except Exception as e:
    print(f"Error loading cnn_dailymail dataset: {e}")
    test_dataset = [] # Fallback to empty list if loading fails


# Extract texts and summaries
test_texts = [example['article'] for example in test_dataset]
test_summaries = [example['highlights'] for example in test_dataset]

print(f"Test samples extracted: {len(test_texts)}")

# ======================================================================
# PROPER DATA SPLITS FOR ATTACK EVALUATION
# ======================================================================
# CRITICAL FIX: Use validation set for trigger optimization, test set for evaluation
# This prevents data leakage and ensures fair comparison

print("\n" + "="*80)
print("CREATING PROPER ATTACK DATA SPLITS (Held-out Evaluation)")
print("="*80)

# Use validation data for trigger optimization (DISJOINT from test set)
print("\nLoading VALIDATION set for trigger optimization...")
try:
    if ExperimentConfig.USE_FULL_TEST_SETS:
        # Use a subset of validation for trigger optimization (for computational efficiency)
        val_dataset_attack = load_dataset("cnn_dailymail", '3.0.0', split="validation")
        # Use first 500 samples for trigger optimization
        np.random.seed(ExperimentConfig.CURRENT_SEED)
        uat_opt_indices = np.random.choice(len(val_dataset_attack), min(500, len(val_dataset_attack)), replace=False)
        uat_opt_subset = val_dataset_attack.select(uat_opt_indices)
    else:
        # Quick testing: use 80 samples
        val_dataset_attack = load_dataset("cnn_dailymail", '3.0.0', split="validation")
        np.random.seed(ExperimentConfig.CURRENT_SEED)
        uat_opt_indices = np.random.choice(len(val_dataset_attack), 80, replace=False)
        uat_opt_subset = val_dataset_attack.select(uat_opt_indices)
    
    uat_trigger_opt_texts = [ex['article'] for ex in uat_opt_subset]
    uat_trigger_opt_summaries = [ex['highlights'] for ex in uat_opt_subset]
    print(f"✓ Trigger optimization set: {len(uat_trigger_opt_texts)} samples (from VALIDATION)")
except Exception as e:
    print(f"Error loading validation data for trigger optimization: {e}")
    uat_trigger_opt_texts, uat_trigger_opt_summaries = [], []

# Use TEST set for attack evaluation (DISJOINT from trigger optimization)
print("\nUsing TEST set for attack evaluation...")
if ExperimentConfig.USE_FULL_TEST_SETS:
    # Use subset of test set for attack evaluation (for computational efficiency)
    np.random.seed(ExperimentConfig.CURRENT_SEED + 1)  # Different seed to avoid overlap
    uat_eval_indices = np.random.choice(len(test_dataset), min(1000, len(test_dataset)), replace=False)
    uat_eval_subset = test_dataset.select(uat_eval_indices)
else:
    # Quick testing: use 120 samples
    np.random.seed(ExperimentConfig.CURRENT_SEED + 1)
    uat_eval_indices = np.random.choice(len(test_dataset), 120, replace=False)
    uat_eval_subset = test_dataset.select(uat_eval_indices)

uat_test_texts = [ex['article'] for ex in uat_eval_subset]
uat_test_summaries = [ex['highlights'] for ex in uat_eval_subset]
print(f"✓ Attack evaluation set: {len(uat_test_texts)} samples (from TEST, held-out)")

print("\n" + "="*80)
print("DATA SPLIT SUMMARY:")
print("="*80)
print(f"  Trigger Optimization (VAL): {len(uat_trigger_opt_texts)} samples")
print(f"  Attack Evaluation (TEST):   {len(uat_test_texts)} samples")
print(f"  ✓ No overlap - proper held-out evaluation")
print("="*80)

# For backward compatibility with old variable names
uat_train_texts = uat_trigger_opt_texts
uat_train_summaries = uat_trigger_opt_summaries

"""### Summary of Attack"""

print("\n" + "="*80)
print(" SETUP COMPLETE - READY FOR UAT ATTACK")
print("="*80)

print("\nModels loaded:")
print("  1. Standard T5           - Baseline (ReLU, unconstrained)")
print("  2. Monotonic (unfinetuned) - W≥0, no training")
print("  3. Monotonic (finetuned)   - W≥0, trained on 20K samples")

print(f"\nData loaded:")
print(f"  • UAT training: {len(uat_train_texts)} samples")
print(f"  • UAT testing:  {len(uat_test_texts)} samples")

print("\n Ready to run aggressive UAT attack!")

# ======================================================================
# Prepare ALL THREE Models for Fair UAT Evaluation
# ======================================================================

print("\n" + "="*80)
print("PREPARING MODELS FOR UAT EVALUATION")
print("="*80)

# Model 1: Standard T5 (pre-trained only) - already loaded above
print("✓ Model 1: Standard T5 (pre-trained only)")

# Model 2: Baseline T5 (fine-tuned, unconstrained) - load if not already loaded
if model_baseline_finetuned is None:
    print("\nWARNING: Baseline fine-tuned model not found.")
    print("    Attempting to load from checkpoint...")
    baseline_checkpoint_dir = os.path.join(ExperimentConfig.SAVE_PATH, 'baseline_checkpoints')
    baseline_best_path = os.path.join(baseline_checkpoint_dir, 'best_model.pt')
    
    if os.path.exists(baseline_best_path):
        model_baseline_finetuned = T5ForConditionalGeneration.from_pretrained(ExperimentConfig.MODEL_NAME).to(device)
        model_baseline_finetuned.load_state_dict(torch.load(baseline_best_path, map_location=device))
        model_baseline_finetuned.eval()
        print(f"    Loaded from: {baseline_best_path}")
    else:
        print(f"    Checkpoint not found: {baseline_best_path}")
        print("    WARNING: This is NOT a fair comparison without the baseline model!")

if model_baseline_finetuned is not None:
    print("Model 2: Baseline T5 (fine-tuned, unconstrained)")
else:
    print("Model 2: Baseline T5 NOT AVAILABLE - will use Standard as fallback")

# Model 3: Monotonic T5 (fine-tuned, W≥0) - already loaded above
if model_monotonic_finetuned is not None:
    print("Model 3: Monotonic T5 (fine-tuned, W≥0)")
else:
    print("WARNING: Model 3: Monotonic T5 NOT AVAILABLE")

# Build models dict with appropriate labels
models_to_test = {}

# Always include Standard
models_to_test['Standard T5 (pre-trained only)'] = model_standard

# Include Baseline (or mark fallback)
if model_baseline_finetuned is not None:
    models_to_test['Baseline T5 (fine-tuned, unconstrained)'] = model_baseline_finetuned
else:
    models_to_test['Baseline T5 (FALLBACK: pre-trained only)'] = model_standard
    print("    WARNING: Using pre-trained Standard T5 as Baseline fallback - NOT A FAIR COMPARISON!")

# Include Monotonic (or mark fallback)
if model_monotonic_finetuned is not None:
    models_to_test['Monotonic T5 (fine-tuned, W≥0)'] = model_monotonic_finetuned
else:
    models_to_test['Monotonic T5 (FALLBACK: unfinetuned)'] = model_monotonic_unfinetuned
    print("    WARNING: Using unfinetuned Monotonic as fallback - NOT A FAIR COMPARISON!")

print("\n" + "="*80)
print("MODELS FOR ATTACK EVALUATION:")
print("="*80)
for i, (name, model) in enumerate(models_to_test.items(), 1):
    print(f"  {i}. {name}")
print("="*80)

print("\n" + "="*80)

"""### UAT Test Implementation"""

# Ensure rouge-score is installed
try:
    from rouge_score import rouge_scorer
except ImportError:
    import subprocess, sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "rouge-score"])
from rouge_score import rouge_scorer

class AggressiveUATAttack:
    """
    Enhanced UAT attack with multiple strategies for substantial impact.
    """
    def __init__(self, model, tokenizer, device, trigger_length=None):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        # Use config default if not specified
        self.trigger_length = trigger_length if trigger_length is not None else ExperimentConfig.ATTACK_TRIGGER_LENGTH

        # Expand vocabulary to include more disruptive tokens
        self.vocab_candidates = self._get_disruptive_vocab()

    def _insert_ids_prefix(self, input_ids, trigger_ids):
        trig = torch.tensor(trigger_ids, device=input_ids.device).unsqueeze(0)
        return torch.cat([trig, input_ids], dim=1)

    def _encode_source(self, text):
        enc = self.tokenizer("summarize: " + text, return_tensors="pt", truncation=False)
        return {k: v.to(self.device) for k, v in enc.items()}

    def _safe_pack(self, enc, max_len=512):
        # Trim from the **left** (trigger side) so the source content stays intact
        input_ids = enc["input_ids"]; attn = enc["attention_mask"]
        if input_ids.size(1) > max_len:
            extra = input_ids.size(1) - max_len
            input_ids = input_ids[:, extra:]
            attn      = attn[:, extra:]
        return {"input_ids": input_ids, "attention_mask": attn}

    def _get_disruptive_vocab(self):
        """Get vocabulary that's more likely to disrupt the model."""
        candidates = []

        # 1. Rare/unusual words (high token IDs often correspond to rare words)
        vocab_size = self.tokenizer.vocab_size
        candidates.extend(list(range(1000, min(5000, vocab_size))))

        # 2. Common words that might confuse context
        disruptive_words = [
            'not', 'never', 'always', 'must', 'cannot', 'should', 'would',
            'however', 'although', 'despite', 'nevertheless',
            'ignore', 'disregard', 'false', 'incorrect', 'error',
            'random', 'noise', 'gibberish', 'nonsense',
            '!!!', '???', '###', '***',
        ]

        for word in disruptive_words:
            token_ids = self.tokenizer.encode(word, add_special_tokens=False)
            candidates.extend(token_ids)

        # 3. Numbers and special tokens
        for num in ['0', '1', '999', '2024', '####']:
            token_ids = self.tokenizer.encode(num, add_special_tokens=False)
            candidates.extend(token_ids)

        # Remove duplicates and invalid tokens
        candidates = list(set(candidates))
        vocab_size = self.tokenizer.vocab_size
        candidates = [c for c in candidates if 0 < c < vocab_size]

        print(f"  Using {len(candidates)} candidate tokens for attack")
        return candidates

    def compute_loss_batch(self, texts, summaries, trigger_ids, max_src_len=512):
        """
        Auxiliary objective: avg NLL of reference summaries given (trigger ⊕ source).
        Uses ID-level prefix insertion and trims from trigger side to avoid truncating the source.
        """
        losses = []
        for text, summary in zip(texts, summaries):
            # encode source
            enc = self._encode_source(text)
            if len(trigger_ids) > 0:
                enc["input_ids"] = self._insert_ids_prefix(enc["input_ids"], trigger_ids)
                enc["attention_mask"] = torch.cat(
                    [torch.ones(1, len(trigger_ids), device=self.device), enc["attention_mask"]],
                    dim=1
                )
            enc = self._safe_pack(enc, max_len=max_src_len)

            # encode target (reference summary)
            tgt = self.tokenizer(summary, return_tensors="pt", truncation=True, max_length=128).to(self.device)

            with torch.no_grad():
                out = self.model(input_ids=enc["input_ids"],
                                attention_mask=enc["attention_mask"],
                                labels=tgt.input_ids)
                loss = out.loss.item()
            losses.append(loss)

        return float(np.mean(losses)), losses


    def learn_universal_trigger(self, texts, summaries, num_iterations=50, num_restarts=3):
        """
        Learn universal trigger with multiple random restarts and aggressive optimization.
        """
        print(f"\nLearning AGGRESSIVE universal trigger...")
        print(f"  Trigger length:    {self.trigger_length} tokens (longer = more disruptive)")
        print(f"  Training samples:  {len(texts)}")
        print(f"  Optimization runs: {num_iterations} iterations × {num_restarts} restarts")
        print(f"  Insertion:         Multiple positions tested")

        best_global_trigger = None
        best_global_loss = -float('inf')

        for restart in range(num_restarts):
            print(f"\n--- Random Restart {restart + 1}/{num_restarts} ---")

            # Initialize random trigger
            trigger_ids = np.random.choice(self.vocab_candidates, self.trigger_length)
            best_trigger = trigger_ids.copy()
            best_avg_loss = -float('inf')

            # Compute baseline (no trigger)
            baseline_loss, _ = self.compute_loss_batch(texts, summaries, [])
            print(f"  Baseline loss (clean): {baseline_loss:.4f}")

            progress_bar = tqdm(range(num_iterations), desc=f"Restart {restart+1}")

            for iteration in progress_bar:
                # Evaluate current trigger
                avg_loss, _ = self.compute_loss_batch(texts, summaries, trigger_ids)

                # Update best for this restart
                if avg_loss > best_avg_loss:
                    best_avg_loss = avg_loss
                    best_trigger = trigger_ids.copy()

                improvement = (avg_loss / baseline_loss - 1) * 100 if baseline_loss > 0 else 0
                progress_bar.set_postfix({
                    'loss': f'{avg_loss:.3f}',
                    'best': f'{best_avg_loss:.3f}',
                    'impact': f'{improvement:+.1f}%'
                })

                # AGGRESSIVE local search: try replacing MULTIPLE positions
                positions_to_change = np.random.choice(
                    self.trigger_length,
                    size=min(3, self.trigger_length),  # Change up to 3 tokens at once
                    replace=False
                )

                for pos in positions_to_change:
                    original_token = trigger_ids[pos]

                    # Try MORE candidates per position
                    candidates = np.random.choice(
                        self.vocab_candidates,
                        min(50, len(self.vocab_candidates)),  # ← Increased from 30
                        replace=False
                    )

                    best_local_loss = avg_loss
                    best_local_token = original_token

                    # Evaluate on larger subset for better estimates
                    subset_size = min(30, len(texts))  # ← Increased from 20
                    subset_indices = np.random.choice(len(texts), subset_size, replace=False)

                    for candidate in candidates:
                        trigger_ids[pos] = candidate

                        subset_texts = [texts[i] for i in subset_indices]
                        subset_summaries = [summaries[i] for i in subset_indices]

                        subset_loss, _ = self.compute_loss_batch(
                            subset_texts, subset_summaries, trigger_ids
                        )

                        if subset_loss > best_local_loss:
                            best_local_loss = subset_loss
                            best_local_token = candidate

                    trigger_ids[pos] = best_local_token

                # Early stopping only if impact is already substantial
                if iteration > 10 and improvement >= 15:  # Stop if already 15%+ impact
                    print(f"\n  ✓ Achieved {improvement:.1f}% impact - stopping early")
                    break

            # Update global best across all restarts
            if best_avg_loss > best_global_loss:
                best_global_loss = best_avg_loss
                best_global_trigger = best_trigger.copy()
                print(f"  New global best: {best_avg_loss:.4f} ({(best_avg_loss/baseline_loss-1)*100:+.1f}% impact)")

        trigger_text = self.tokenizer.decode(best_global_trigger, skip_special_tokens=True)
        final_impact = (best_global_loss / baseline_loss - 1) * 100 if baseline_loss > 0 else 0

        print(f"\n{'='*80}")
        print(f"✓ BEST Universal Trigger Found:")
        print(f"  '{trigger_text}'")
        print(f"  Baseline loss:  {baseline_loss:.4f}")
        print(f"  Attacked loss:  {best_global_loss:.4f}")
        print(f"  Impact:         {final_impact:+.1f}%")
        print(f"{'='*80}")

        return best_global_trigger, trigger_text

    def evaluate_trigger(self, texts, summaries, trigger_ids):
        """
        PRIMARY evaluation: generation-based ROUGE deltas.
        AUXILIARY: continuation NLL deltas on references.
        """
        # Primary (ROUGE deltas)
        gen_report = self.eval_generation(texts, summaries, trigger_ids=trigger_ids)

        # Auxiliary (reference NLL deltas)
        avg_clean_loss, _  = self.compute_loss_batch(texts, summaries, trigger_ids=[],           max_src_len=512)
        avg_attack_loss, _ = self.compute_loss_batch(texts, summaries, trigger_ids=trigger_ids,  max_src_len=512)
        loss_deg = (avg_attack_loss - avg_clean_loss) / max(avg_clean_loss, 1e-8)

        return {
            "rouge": gen_report,
            "aux_ce": {
                "avg_clean_loss":  avg_clean_loss,
                "avg_attack_loss": avg_attack_loss,
                "relative_increase": loss_deg
            }
        }


    def eval_generation(self, texts, refs, trigger_ids=None, max_src_len=512, gen_kwargs=None):
        """
        Generate summaries with/without trigger and compute ROUGE deltas.
        Returns: dict with per-example ROUGE and aggregated means.
        Uses FIXED decoding parameters from ExperimentConfig for fair comparison.
        """
        # Use fixed decoding parameters from config (CRITICAL for fair comparison)
        if gen_kwargs is None:
            gen_kwargs = dict(
                max_new_tokens=ExperimentConfig.DECODE_MAX_NEW_TOKENS,
                min_new_tokens=ExperimentConfig.DECODE_MIN_NEW_TOKENS,
                num_beams=ExperimentConfig.DECODE_NUM_BEAMS,
                length_penalty=ExperimentConfig.DECODE_LENGTH_PENALTY,
                no_repeat_ngram_size=ExperimentConfig.DECODE_NO_REPEAT_NGRAM_SIZE,
                early_stopping=ExperimentConfig.DECODE_EARLY_STOPPING
            )
        scorer = rouge_scorer.RougeScorer(["rouge1","rouge2","rougeLsum"], use_stemmer=True)

        def encode(text):
            enc = self._encode_source(text)
            if trigger_ids is not None and len(trigger_ids) > 0:
                enc["input_ids"] = self._insert_ids_prefix(enc["input_ids"], trigger_ids)
                enc["attention_mask"] = torch.cat(
                    [torch.ones(1, len(trigger_ids), device=self.device), enc["attention_mask"]],
                    dim=1
                )
            return self._safe_pack(enc, max_len=max_src_len)

        clean_scores, atk_scores = [], []
        clean_outs, atk_outs = [], []

        for x, y_ref in zip(texts, refs):
            with torch.no_grad():
                # clean
                enc_clean = encode(x) if False else self._safe_pack(self._encode_source(x), max_len=max_src_len)
                gen_clean = self.model.generate(**enc_clean, **gen_kwargs)
                y_clean = self.tokenizer.decode(gen_clean[0], skip_special_tokens=True)

                # attacked
                enc_atk = encode(x)
                gen_atk = self.model.generate(**enc_atk, **gen_kwargs)
                y_atk = self.tokenizer.decode(gen_atk[0], skip_special_tokens=True)

            clean_outs.append(y_clean)
            atk_outs.append(y_atk)

            sc_clean = scorer.score(y_ref, y_clean)
            sc_atk   = scorer.score(y_ref, y_atk)

            clean_scores.append({k: v.fmeasure for k, v in sc_clean.items()})
            atk_scores.append({k: v.fmeasure for k, v in sc_atk.items()})

        def mean(lst, key): return float(np.mean([d[key] for d in lst]))

        report = {
            "clean_means":   {k: mean(clean_scores, k) for k in ["rouge1","rouge2","rougeLsum"]},
            "attack_means":  {k: mean(atk_scores,   k) for k in ["rouge1","rouge2","rougeLsum"]},
            "delta":         {k: mean(atk_scores, k) - mean(clean_scores, k) for k in ["rouge1","rouge2","rougeLsum"]},
            "clean_outputs": clean_outs,
            "attack_outputs": atk_outs,
            "per_example":   list(zip(clean_scores, atk_scores))
        }
        return report

"""### Attack Comparison Models"""

import pandas as pd
import json
import os

print("\n" + "="*80)
print("ATTACKING THREE MODELS WITH AGGRESSIVE UAT")
print("="*80)

models_to_test = {
    'Standard T5': model_standard,
    # 'Monotonic (unfinetuned)': model_monotonic_unfinetuned, # Removed unfinetuned model as requested
    'Monotonic (after FT)': model_monotonic_finetuned
}

all_results = {}

# File to store learned triggers (NOW PERSISTENT!)
TRIGGERS_FILE = os.path.join(SAVE_PATH, 'learned_triggers.csv')  # ← CHANGED

# Try to load existing triggers
existing_triggers = {}
if os.path.exists(TRIGGERS_FILE):
    print(f"\n Found existing triggers file: {TRIGGERS_FILE}")
    df_triggers = pd.read_csv(TRIGGERS_FILE)
    for _, row in df_triggers.iterrows():
        trigger_ids_list = json.loads(row['trigger_ids'])
        existing_triggers[row['model_name']] = {
            'trigger_text': row['trigger_text'],
            'trigger_ids': torch.tensor(trigger_ids_list, dtype=torch.long)  # Convert to tensor
        }
    print(f"   Loaded {len(existing_triggers)} saved trigger(s)")

for model_name, model in models_to_test.items():
    print(f"\n{'='*80}")
    print(f"ATTACKING: {model_name}")
    print(f"{'='*80}")

    # Create aggressive attacker (uses config trigger length)
    attacker = AggressiveUATAttack(
        model,
        tokenizer,
        device
        # trigger_length uses ExperimentConfig.ATTACK_TRIGGER_LENGTH by default
    )

    # Check if we have a saved trigger for this model
    if model_name in existing_triggers:
        print(f"\n Using saved trigger for {model_name}")
        trigger_ids = existing_triggers[model_name]['trigger_ids']
        trigger_text = existing_triggers[model_name]['trigger_text']
        print(f"   Trigger: '{trigger_text}'")
    else:
        # Learn universal trigger with multiple restarts
        print(f"\n Learning new trigger for {model_name}...")
        trigger_ids, trigger_text = attacker.learn_universal_trigger(
            uat_train_texts,
            uat_train_summaries,
            num_iterations=50,
            num_restarts=3
        )

        # Save the newly learned trigger
        trigger_data = {
            'model_name': model_name,
            'trigger_text': trigger_text,
            'trigger_ids': json.dumps(trigger_ids.tolist())
        }

        if os.path.exists(TRIGGERS_FILE):
            df_existing = pd.read_csv(TRIGGERS_FILE)
            # Remove old entry for this model if it exists
            df_existing = df_existing[df_existing['model_name'] != model_name]
            df_new = pd.concat([df_existing, pd.DataFrame([trigger_data])], ignore_index=True)
        else:
            df_new = pd.DataFrame([trigger_data])

        df_new.to_csv(TRIGGERS_FILE, index=False)
        print(f"   Saved trigger to {TRIGGERS_FILE}")

    # Evaluate on test set
    print(f"\n Evaluating on test set ({len(uat_test_texts)} samples)...")
    test_results = attacker.evaluate_trigger(
        uat_test_texts,
        uat_test_summaries,
        trigger_ids
    )

    rouge_rep = test_results["rouge"]
    aux_ce    = test_results["aux_ce"]

    print(f"\n Test Set Results (Generation Metrics):")
    print(f"  Trigger: '{trigger_text}'")
    print(f"  ROUGE (clean):  R1={rouge_rep['clean_means']['rouge1']:.3f}  "
          f"R2={rouge_rep['clean_means']['rouge2']:.3f}  RL={rouge_rep['clean_means']['rougeLsum']:.3f}")
    print(f"  ROUGE (attack): R1={rouge_rep['attack_means']['rouge1']:.3f}  "
          f"R2={rouge_rep['attack_means']['rouge2']:.3f}  RL={rouge_rep['attack_means']['rougeLsum']:.3f}")
    print(f"  ΔROUGE:         ΔR1={rouge_rep['delta']['rouge1']:.3f}  "
          f"ΔR2={rouge_rep['delta']['rouge2']:.3f}  ΔRL={rouge_rep['delta']['rougeLsum']:.3f}")

    print(f"\n(Aux) Reference NLL:")
    print(f"  Avg Clean Loss:  {aux_ce['avg_clean_loss']:.4f}")
    print(f"  Avg Attack Loss: {aux_ce['avg_attack_loss']:.4f}")
    print(f"  Rel Increase:    {aux_ce['relative_increase']*100:+.1f}%")

    # Store results for comparative analysis (including trigger IDs for transfer attacks)
    all_results[model_name] = {
        'trigger_text': trigger_text,
        'trigger_ids_json': json.dumps(trigger_ids.tolist()),  # Store for transfer attacks
        'avg_degradation': rouge_rep['delta']['rougeLsum'],
        'test_results': [{'degradation': rouge_rep['delta']['rougeLsum']}],
        'rouge_delta': rouge_rep['delta'],
        'aux_ce': aux_ce
    }

    # define success threshold on task metric (example: ROUGE-L drop ≥ 5 points)
    success = rouge_rep['delta']['rougeLsum'] <= -0.05
    print(f"\n{'✓ Attack successful' if success else '⚠ Impact below target'} "
          f"(criterion: ΔROUGE-L ≤ -0.05)")

# ======================================================================
# TRANSFER ATTACK MATRIX (Trigger from Model A tested on Model B)
# ======================================================================

print("\n" + "="*80)
print("TRANSFER ATTACK EVALUATION")
print("="*80)
print("Testing each model's trigger on ALL other models to measure transferability...")
print("="*80)

# Build transfer attack matrix
transfer_matrix = {}
model_names_list = list(all_results.keys())

for source_model_name in model_names_list:
    source_trigger_ids = torch.tensor(
        json.loads(all_results[source_model_name].get('trigger_ids_json', '[]')), 
        dtype=torch.long
    ) if 'trigger_ids_json' in all_results[source_model_name] else None
    
    # If trigger IDs not stored in all_results, try to reconstruct from trigger_text
    if source_trigger_ids is None or len(source_trigger_ids) == 0:
        print(f"\nWARNING: Cannot perform transfer attack from {source_model_name} - trigger IDs not available")
        continue
    
    print(f"\n--- Transfer from {source_model_name} ---")
    print(f"    Trigger: '{all_results[source_model_name]['trigger_text']}'")
    
    transfer_matrix[source_model_name] = {}
    
    for target_model_name, target_model in models_to_test.items():
        # Create attacker for target model
        target_attacker = AggressiveUATAttack(target_model, tokenizer, device)
        
        # Evaluate source trigger on target model
        transfer_results = target_attacker.evaluate_trigger(
            uat_test_texts[:100] if not ExperimentConfig.USE_FULL_TEST_SETS else uat_test_texts[:500],  # Limit for efficiency
            uat_test_summaries[:100] if not ExperimentConfig.USE_FULL_TEST_SETS else uat_test_summaries[:500],
            source_trigger_ids
        )
        
        transfer_delta = transfer_results['rouge']['delta']['rougeLsum']
        transfer_matrix[source_model_name][target_model_name] = transfer_delta
        
        # Mark same-model (non-transfer) vs cross-model
        is_self = (source_model_name == target_model_name)
        marker = "✓ (self)" if is_self else "→ (transfer)"
        print(f"    {marker} On {target_model_name}: ΔROUGE-L = {transfer_delta:.4f}")

print("\n" + "="*80)
print("TRANSFER ATTACK MATRIX SUMMARY")
print("="*80)
print("\nTransfer effectiveness (ΔROUGE-L):")
print(f"{'Source \\ Target':<40}", end="")
for tgt in model_names_list:
    print(f"{tgt[:20]:>22}", end="")
print()
print("-" * (40 + 22 * len(model_names_list)))

for src in model_names_list:
    print(f"{src:<40}", end="")
    for tgt in model_names_list:
        delta = transfer_matrix.get(src, {}).get(tgt, 0.0)
        # Highlight diagonal (self-attack)
        if src == tgt:
            print(f"  {delta:>8.4f} (self)", end="")
        else:
            print(f"  {delta:>8.4f}      ", end="")
    print()

# Analyze transferability
print("\nTransferability Analysis:")
for src in model_names_list:
    if src not in transfer_matrix:
        continue
    self_attack = transfer_matrix[src].get(src, 0.0)
    cross_attacks = [v for k, v in transfer_matrix[src].items() if k != src]
    if cross_attacks:
        avg_transfer = np.mean(cross_attacks)
        transfer_ratio = avg_transfer / self_attack if self_attack != 0 else 0
        print(f"  {src}:")
        print(f"    Self-attack: {self_attack:.4f}")
        print(f"    Avg transfer: {avg_transfer:.4f}")
        print(f"    Transfer ratio: {transfer_ratio:.2%}")

print("\n" + "="*80)
print("COMPARATIVE ANALYSIS")
print("="*80)

print("\nVulnerability Ranking (Higher = More Vulnerable):")
print("-"*80)

ranked_models = sorted(all_results.items(), key=lambda x: x[1]['avg_degradation'], reverse=True)

for rank, (model_name, results) in enumerate(ranked_models, 1):
    print(f"{rank}. {model_name:30s} | Degradation: {results['avg_degradation']:.2%}")

# Compute robustness improvement
std_degradation = all_results['Standard T5']['avg_degradation']
# Check if Monotonic (unfinetuned) exists before accessing
mono_unfinetuned_degradation = all_results['Monotonic (unfinetuned)']['avg_degradation'] if 'Monotonic (unfinetuned)' in all_results else None
mono_after_degradation = all_results['Monotonic (after FT)']['avg_degradation']

if mono_unfinetuned_degradation is not None:
    improvement_before = (std_degradation - mono_unfinetuned_degradation) / std_degradation * 100
    print(f"\n🛡️ Robustness Improvement:")
    print(f"  Monotonic (unfinetuned) vs Standard: {improvement_before:+.1f}%")
else:
    improvement_before = None


improvement_after = (std_degradation - mono_after_degradation) / std_degradation * 100

print(f"  Monotonic (after FT) vs Standard: {improvement_after:+.1f}%")

# Statistical significance test
from scipy import stats

std_results = [r['degradation'] for r in all_results['Standard T5']['test_results']]
mono_after_results = [r['degradation'] for r in all_results['Monotonic (after FT)']['test_results']]

# Add unfinetuned results if available
if 'Monotonic (unfinetuned)' in all_results:
    mono_unfinetuned_results = [r['degradation'] for r in all_results['Monotonic (unfinetuned)']['test_results']]
else:
    mono_unfinetuned_results = []


# Perform t-tests only if there are enough samples
if len(std_results) > 1 and len(mono_after_results) > 1:
    t_stat, p_value = stats.ttest_ind(std_results, mono_after_results)
    significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
    print(f"\n📈 Statistical Significance (Standard vs Monotonic after FT):")
    print(f"  t-statistic: {t_stat:.4f}")
    print(f"  p-value:     {p_value:.6f} {significance}")
else:
    t_stat, p_value = float('nan'), float('nan')
    significance = 'ns'
    print("\n📈 Statistical Significance: Not enough samples to perform t-test.")

"""# UAT Evaluation Metrics

- **Baseline loss (clean):** Mean teacher-forcing NLL on train data **without** the trigger (lower is better).
- **Attacked loss:** Mean NLL on the **same** train data **with** the trigger.
- **Impact (train-time):** Relative loss inflation from the trigger  
  $$\text{Impact}=\frac{\text{AttackedLoss}-\text{BaselineLoss}}{\text{BaselineLoss}}\times 100\%$$
- **ROUGE (clean/attack):** Test-set R1/R2/RL for generations **without/with** the learned trigger.
- **ΔROUGE:** Attack − Clean (per metric). Negative ⇒ degradation  
  $$\Delta \mathrm{ROUGE}_m=\mathrm{ROUGE}_{\text{attack},m}-\mathrm{ROUGE}_{\text{clean},m}$$
- **(Aux) Reference NLL (test):** Avg NLL of ground-truth summaries on the test set (clean vs attacked).
- **Relative NLL increase:**  
  $$\frac{\text{AvgAttackLoss}-\text{AvgCleanLoss}}{\text{AvgCleanLoss}}\times 100\%$$
- **Success criterion:** Attack “successful” if  
  $$\Delta \mathrm{ROUGE\!-\!Lsum}\le -0.05\;.$$

### Comparative Analysis
"""

print("\n" + "="*80)
print("COMPARATIVE ANALYSIS")
print("="*80)

print("\nVulnerability Ranking (Higher = More Vulnerable):")
print("-"*80)

ranked_models = sorted(all_results.items(), key=lambda x: x[1]['avg_degradation'], reverse=True)

for rank, (model_name, results) in enumerate(ranked_models, 1):
    print(f"{rank}. {model_name:30s} | Degradation: {results['avg_degradation']:.2%}")

# Compute robustness improvement
std_degradation = all_results['Standard T5']['avg_degradation']
mono_after_degradation = all_results['Monotonic (after FT)']['avg_degradation']

improvement_after = (std_degradation - mono_after_degradation) / std_degradation * 100

print(f"\n Robustness Improvement:")
print(f"  Monotonic (after FT) vs Standard: {improvement_after:+.1f}%")

# Statistical significance test
from scipy import stats

std_results = [r['degradation'] for r in all_results['Standard T5']['test_results']]
mono_after_results = [r['degradation'] for r in all_results['Monotonic (after FT)']['test_results']]

t_stat, p_value = stats.ttest_ind(std_results, mono_after_results)
significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"

print(f"\n📈 Statistical Significance (Standard vs Monotonic after FT):")
print(f"  t-statistic: {t_stat:.4f}")
print(f"  p-value:     {p_value:.6f} {significance}")

"""### Visualization"""

print("\n" + "="*80)
print("CREATING VISUALIZATIONS")
print("="*80)

fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

model_names = list(all_results.keys())
colors = ['#e74c3c', '#f39c12', '#27ae60'][:len(model_names)] # Adjust colors based on number of models

# Plot 1: Average Degradation Comparison
ax1 = fig.add_subplot(gs[0, 0])
degradations = [all_results[name]['avg_degradation'] * 100 for name in model_names]
bars = ax1.bar(range(len(model_names)), degradations, color=colors, alpha=0.7, width=0.6)
ax1.set_ylabel('Performance Degradation (%)', fontsize=11)
ax1.set_title('UAT Attack Effectiveness', fontsize=12, fontweight='bold')
ax1.set_xticks(range(len(model_names)))
ax1.set_xticklabels(model_names, fontsize=10)
ax1.grid(True, alpha=0.3, axis='y')
ax1.axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='10% threshold')
ax1.legend(fontsize=9)

for bar, deg in zip(bars, degradations):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
            f'{deg:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Plot 2: Loss Comparison (Clean vs Attacked)
ax2 = fig.add_subplot(gs[0, 1])
x_pos = np.arange(len(model_names))
width = 0.35
# Corrected key access for losses
orig_losses = [all_results[name]['aux_ce']['avg_clean_loss'] for name in model_names]
attack_losses = [all_results[name]['aux_ce']['avg_attack_loss'] for name in model_names]

ax2.bar(x_pos - width/2, orig_losses, width, label='Clean', alpha=0.8, color='#3498db')
ax2.bar(x_pos + width/2, attack_losses, width, label='Attacked', alpha=0.8, color='#e74c3c')
ax2.set_ylabel('Average Loss', fontsize=11)
ax2.set_title('Loss: Clean vs Attacked', fontsize=12, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(model_names, fontsize=10)
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, axis='y')

# Plot 3: Degradation Distribution
ax4 = fig.add_subplot(gs[1, :]) # Using ax4 for the second row span
for model_name, color in zip(model_names, colors):
    degradations = [r['degradation'] * 100 for r in all_results[model_name]['test_results']]
    ax4.hist(degradations, bins=30, alpha=0.5, label=model_name, color=color)

ax4.axvline(x=10, color='red', linestyle='--', linewidth=2, alpha=0.7, label='10% threshold')
ax4.set_xlabel('Degradation (%)', fontsize=11)
ax4.set_ylabel('Frequency', fontsize=11)
ax4.set_title('Degradation Distribution Across Test Set', fontsize=12, fontweight='bold')
ax4.legend(fontsize=10)
ax4.grid(True, alpha=0.3)

# Plot 4: Robustness Improvement (Corrected to plot only available models)
ax5 = fig.add_subplot(gs[2, 0]) # Using ax5 for the third row, first column
if 'Standard T5' in all_results:
    baseline = all_results['Standard T5']['avg_degradation']
    improvement_data = {'Standard T5': 0}
    if 'Monotonic (after FT)' in all_results:
        improvement_data['Monotonic (after FT)'] = (baseline - all_results['Monotonic (after FT)']['avg_degradation']) / baseline * 100
    if 'Monotonic (unfinetuned)' in all_results:
         improvement_data['Monotonic (unfinetuned)'] = (baseline - all_results['Monotonic (unfinetuned)']['avg_degradation']) / baseline * 100

    improvement_model_names = list(improvement_data.keys())
    improvements = [improvement_data[name] for name in improvement_model_names]
    improvement_colors = [colors[model_names.index(name)] for name in improvement_model_names]

    bars = ax5.bar(range(len(improvement_model_names)), improvements, color=improvement_colors, alpha=0.7, width=0.6)
    ax5.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax5.set_ylabel('Robustness Improvement (%)', fontsize=11)
    ax5.set_title('Relative Robustness vs Standard T5', fontsize=12, fontweight='bold')
    ax5.set_xticks(range(len(improvement_model_names)))
    ax5.set_xticklabels([name.replace(" ", "\n").replace("(unfinetuned)", "(untuned)").replace("(after FT)", "(tuned)") for name in improvement_model_names], fontsize=10)
    ax5.grid(True, alpha=0.3, axis='y')

    for bar, imp in zip(bars, improvements):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top',
                fontsize=10, fontweight='bold')
else:
    ax5.set_title('Robustness Improvement (Standard T5 not available)', fontsize=12, fontweight='bold')
    ax5.text(0.5, 0.5, "Insufficient data", horizontalalignment='center', verticalalignment='center', transform=ax5.transAxes, fontsize=12)
    ax5.set_xticks([])
    ax5.set_yticks([])


# Plot 5: Box Plot Comparison
ax6 = fig.add_subplot(gs[2, 1]) # Using ax6 for the third row, second column
data_to_plot = []
box_plot_labels = []
box_plot_colors = []
for model_name in model_names:
    if model_name != 'Monotonic (unfinetuned)': # Exclude unfinetuned from this plot unless explicitly requested
        degradations = [r['degradation'] * 100 for r in all_results[model_name]['test_results']]
        data_to_plot.append(degradations)
        box_plot_labels.append(model_name.replace(" ", "\n").replace("(unfinetuned)", "(untuned)").replace("(after FT)", "(tuned)"))
        box_plot_colors.append(colors[model_names.index(model_name)])

if data_to_plot:
    bp = ax6.boxplot(data_to_plot, labels=box_plot_labels,
                      patch_artist=True, showmeans=True)

    for patch, color in zip(bp['boxes'], box_plot_colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax6.set_ylabel('Degradation (%)', fontsize=11)
    ax6.set_title('Degradation Distribution (Box Plot)', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='y')
    ax6.axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5)
else:
    ax6.set_title('Degradation Distribution (Box Plot)', fontsize=12, fontweight='bold')
    ax6.text(0.5, 0.5, "No data to plot", horizontalalignment='center', verticalalignment='center', transform=ax6.transAxes, fontsize=12)
    ax6.set_xticks([])
    ax6.set_yticks([])


# Plot 6: Per-Example Comparison
ax7 = fig.add_subplot(gs[2, 2]) # Using ax7 for the third row, third column
if model_names:
    x = range(min(20, len(all_results[model_names[0]]['test_results'])))
    for model_name, color in zip(model_names, colors):
        degradations = [r['degradation'] * 100 for r in all_results[model_name]['test_results'][:20]]
        ax7.plot(x, degradations, 'o-', label=model_name, color=color, alpha=0.7, linewidth=2)

    ax7.axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='10% threshold')
    ax7.set_xlabel('Example Index', fontsize=11)
    ax7.set_ylabel('Degradation (%)', fontsize=11)
    ax7.set_title('Per-Example Degradation (First 20)', fontsize=12, fontweight='bold')
    ax7.legend(fontsize=9)
    ax7.grid(True, alpha=0.3)
else:
    ax7.set_title('Per-Example Degradation (First 20)', fontsize=12, fontweight='bold')
    ax7.text(0.5, 0.5, "No data to plot", horizontalalignment='center', verticalalignment='center', transform=ax7.transAxes, fontsize=12)
    ax7.set_xticks([])
    ax7.set_yticks([])

# Identify all created axes
all_axes = [ax1, ax2, ax4, ax5, ax6, ax7]
# Get the list of all subplot axes in the figure
fig_axes = fig.get_axes()

for ax in fig_axes:
    if ax not in all_axes:
        fig.delaxes(ax)


plt.suptitle('UAT Attack: Comprehensive Evaluation', fontsize=14, fontweight='bold', y=0.995)

plot_path = os.path.join(SAVE_PATH, 'uat_attack_results.png')
plt.savefig(plot_path, dpi=150, bbox_inches='tight')
plt.show()

print(f"\n Visualization saved to: {plot_path}")

"""### Example Outputs"""

print("\n" + "="*80)
print("EXAMPLE ADVERSARIAL OUTPUTS")
print("="*80)

# Show examples from each model
example_idx = 5
example_text = uat_test_texts[example_idx]
example_summary = uat_test_summaries[example_idx]

print(f"\n Original Text:")
print(f"  {example_text[:200]}...")
print(f"\n Ground Truth Summary:")
print(f"  {example_summary}")

for model_name, model in models_to_test.items():
    trigger_text = all_results[model_name]['trigger_text']
    attacked_text = example_text + " " + trigger_text

    # Generate summaries using fixed parameters
    summary_clean = generate_summary_fixed_params(model, example_text, tokenizer, device)
    summary_attacked = generate_summary_fixed_params(model, attacked_text, tokenizer, device)

    print(f"\n{model_name}:")
    print(f"  Trigger: '{trigger_text}'")
    print(f"  Clean Summary:    {summary_clean}")
    print(f"  Attacked Summary: {summary_attacked}")

"""### Save Results"""

print("\n" + "="*80)
print("SAVING UAT ATTACK RESULTS WITH EXPERIMENT METADATA")
print("="*80)

import json

# ======================================================================
# COMPREHENSIVE EXPERIMENT METADATA LOGGING
# ======================================================================

def create_experiment_metadata():
    """
    Create comprehensive experiment metadata for reproducibility.
    Includes: environment, hyperparameters, datasets, model configurations.
    """
    metadata = {
        "experiment_info": {
            "version": "v1.7 - Fair Comparison Edition",
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "seed": ExperimentConfig.CURRENT_SEED,
            "all_seeds": ExperimentConfig.RANDOM_SEEDS,
            "use_full_test_sets": ExperimentConfig.USE_FULL_TEST_SETS
        },
        "environment": env_info if 'env_info' in globals() else {},
        "model_config": {
            "base_checkpoint": ExperimentConfig.MODEL_NAME,
            "models_trained": ["Baseline T5 (unconstrained)", "Monotonic T5 (W≥0)"],
            "models_evaluated": list(models_to_test.keys()) if 'models_to_test' in globals() else []
        },
        "training_hyperparameters": {
            "learning_rate": ExperimentConfig.LEARNING_RATE,
            "weight_decay": ExperimentConfig.WEIGHT_DECAY,
            "num_epochs": ExperimentConfig.NUM_EPOCHS,
            "batch_size": ExperimentConfig.BATCH_SIZE,
            "gradient_accumulation_steps": ExperimentConfig.GRADIENT_ACCUMULATION_STEPS,
            "max_grad_norm": ExperimentConfig.MAX_GRAD_NORM,
            "warmup_ratio": ExperimentConfig.WARMUP_RATIO,
            "max_input_length": ExperimentConfig.MAX_INPUT_LENGTH,
            "max_target_length": ExperimentConfig.MAX_TARGET_LENGTH
        },
        "decoding_parameters": {
            "num_beams": ExperimentConfig.DECODE_NUM_BEAMS,
            "length_penalty": ExperimentConfig.DECODE_LENGTH_PENALTY,
            "min_new_tokens": ExperimentConfig.DECODE_MIN_NEW_TOKENS,
            "max_new_tokens": ExperimentConfig.DECODE_MAX_NEW_TOKENS,
            "no_repeat_ngram_size": ExperimentConfig.DECODE_NO_REPEAT_NGRAM_SIZE,
            "early_stopping": ExperimentConfig.DECODE_EARLY_STOPPING
        },
        "evaluation_config": {
            "rouge_metrics": ExperimentConfig.ROUGE_METRICS,
            "rouge_use_stemmer": ExperimentConfig.ROUGE_USE_STEMMER,
            "bootstrap_samples": ExperimentConfig.ROUGE_BOOTSTRAP_SAMPLES,
            "eval_batch_size": ExperimentConfig.EVAL_BATCH_SIZE
        },
        "attack_config": {
            "trigger_length": ExperimentConfig.ATTACK_TRIGGER_LENGTH,
            "num_candidates": ExperimentConfig.ATTACK_NUM_CANDIDATES,
            "num_grad_steps": ExperimentConfig.ATTACK_NUM_GRAD_STEPS
        },
        "datasets": {
            "training": {
                "sources": ["DialogSum", "SAMSum", "MEETING_SUMMARY", "XSUM", "AMI", "HighlightSum", "arXiv"],
                "total_samples": len(train_texts) if 'train_texts' in globals() else 0
            },
            "validation": {
                "total_samples": len(val_texts) if 'val_texts' in globals() else 0
            },
            "test": {
                "cnn_dailymail": len(cnn_dm_test_texts) if 'cnn_dm_test_texts' in globals() else 0,
                "xsum": len(xsum_test_texts) if 'xsum_test_texts' in globals() else 0,
                "samsum": len(samsum_test_texts) if 'samsum_test_texts' in globals() else 0
            },
            "attack": {
                "trigger_optimization": len(uat_trigger_opt_texts) if 'uat_trigger_opt_texts' in globals() else 0,
                "attack_evaluation": len(uat_test_texts) if 'uat_test_texts' in globals() else 0,
                "data_split": "validation_for_optimization_test_for_evaluation"
            }
        },
        "fairness_guarantees": {
            "identical_starting_checkpoint": True,
            "identical_training_data": True,
            "identical_hyperparameters": True,
            "identical_decoding_params": True,
            "held_out_attack_evaluation": True,
            "bootstrap_confidence_intervals": True,
            "length_bias_controlled": True
        }
    }
    return metadata

# Create and save metadata
experiment_metadata = create_experiment_metadata()
metadata_path = os.path.join(ExperimentConfig.SAVE_PATH, 'experiment_metadata.json')
with open(metadata_path, 'w') as f:
    json.dump(experiment_metadata, f, indent=2)
print(f"\n✓ Experiment metadata saved to: {metadata_path}")

# Prepare results for JSON
results_dict = {
    'summary': {
        model_name: {
            'trigger_text': results['trigger_text'],
            'avg_degradation': float(results['avg_degradation']),
            # 'std_degradation': float(results['std_degradation']),
            'avg_clean_loss': float(results['aux_ce']['avg_clean_loss']),
            'avg_attack_loss': float(results['aux_ce']['avg_attack_loss'])
        }
        for model_name, results in all_results.items()
    },
    'robustness_improvement': {
        'before_finetuning': float(improvement_before) if improvement_before is not None else None,
        'after_finetuning': float(improvement_after)
    },
    'statistical_test': {
        't_statistic': float(t_stat) if not np.isnan(t_stat) else None,
        'p_value': float(p_value) if not np.isnan(p_value) else None,
        'significance': significance
    }
}

results_path = os.path.join(SAVE_PATH, 'uat_attack_results.json')
with open(results_path, 'w') as f:
    json.dump(results_dict, f, indent=2)

print(f"✓ Results saved to: {results_path}")

"""### Final Summary"""

print("\n" + "="*80)
print("FINAL SUMMARY")
print("="*80)

print("\nKey Findings:")
print(f"  1. Standard T5 degradation:           {std_degradation:.2%}")

if 'Monotonic (before FT)' in all_results:
    print(f"  2. Monotonic (before FT) degradation: {all_results['Monotonic (before FT)']['avg_degradation']:.2%}")

print(f"  3. Monotonic (after FT) degradation:  {mono_after_degradation:.2%}")

print(f"\n Monotonicity Effect:")
if improvement_after > 0:
    print(f"  Fine-tuned monotonic model shows {improvement_after:.1f}% IMPROVED robustness")
    print(f"    against universal adversarial triggers!")
elif improvement_after > -5:
    print(f"  Fine-tuned monotonic model shows similar vulnerability ({improvement_after:+.1f}%)")
else:
    print(f"  Fine-tuned monotonic model shows {abs(improvement_after):.1f}% WORSE robustness")

print(f"\n Statistical Validity:")
if p_value is not None and not np.isnan(p_value) and p_value < 0.05:
    print(f"  Difference is statistically significant (p={p_value:.4f})")
elif p_value is not None and not np.isnan(p_value):
    print(f"  Difference is NOT statistically significant (p={p_value:.4f})")
else:
    print("  Statistical test not performed or results not available.")


print("\n" + "="*80)
print(" UAT ATTACK ANALYSIS COMPLETE")
print("="*80)

"""## Hotflip Attack

### Imports and Setup
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from scipy import stats
import json

print("="*80)
print("HOTFLIP ATTACK ON T5 MODELS")
print("="*80)

"""### Hotflip Attack Class Definition"""

class HotFlipT5Attack:
    """
    Gradient-based HotFlip attack for T5 summarization models.
    Replaces tokens by maximizing loss using gradient information.
    """
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

        # Get embedding layer
        self.embedding_layer = model.get_input_embeddings()
        self.vocab_size = self.embedding_layer.num_embeddings

        print(f"  Initialized HotFlip for model with vocab size: {self.vocab_size}")

    def compute_loss(self, text, summary):
        """Compute loss for text-summary pair"""
        inputs = self.tokenizer(
            "summarize: " + text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(self.device)

        targets = self.tokenizer(
            summary,
            return_tensors="pt",
            max_length=128,
            truncation=True
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                labels=targets.input_ids
            )

        return outputs.loss.item()

    def get_embedding_gradients(self, text, summary):
        """
        Compute gradients w.r.t. input embeddings.
        Returns: (gradients, input_ids, attention_mask)
        """
        # Tokenize input
        inputs = self.tokenizer(
            "summarize: " + text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(self.device)

        targets = self.tokenizer(
            summary,
            return_tensors="pt",
            max_length=128,
            truncation=True
        ).to(self.device)

        input_ids = inputs.input_ids
        attention_mask = inputs.attention_mask

        # Get embeddings and enable gradients
        embeddings = self.embedding_layer(input_ids)
        embeddings = embeddings.clone().detach().requires_grad_(True)

        # Forward pass through encoder with custom embeddings
        encoder = self.model.get_encoder()
        encoder_outputs = encoder(
            inputs_embeds=embeddings,
            attention_mask=attention_mask,
            return_dict=True
        )

        # Forward through decoder with target labels
        outputs = self.model(
            encoder_outputs=encoder_outputs,
            attention_mask=attention_mask,
            labels=targets.input_ids
        )

        # Compute loss and backprop
        loss = outputs.loss
        loss.backward()

        # Get gradients
        gradients = embeddings.grad.data

        return gradients, input_ids, attention_mask

    def find_best_replacement(self, position, gradient_vector, current_token_id, top_k=10):
        """
        Find the token that maximizes loss increase at given position.
        Uses dot product between gradient and embedding vectors.
        """
        # Get all embedding vectors
        embedding_matrix = self.embedding_layer.weight.data  # [vocab_size, embedding_dim]

        # Compute dot products: higher = more loss increase
        scores = torch.matmul(embedding_matrix, gradient_vector)  # [vocab_size]

        # Get top-k candidates (excluding current token and special tokens)
        scores[current_token_id] = -float('inf')  # Don't replace with same token
        scores[0] = -float('inf')  # Don't use PAD
        scores[1] = -float('inf')  # Don't use EOS

        # Get top-k tokens
        top_values, top_indices = torch.topk(scores, top_k)

        return top_indices.cpu().numpy(), top_values.cpu().numpy()

    def attack_single(self, text, summary, num_flips=5, beam_size=1):
        """
        Attack a single example by flipping num_flips tokens.

        Args:
            text: Input text
            summary: Target summary
            num_flips: Number of tokens to flip
            beam_size: Number of candidate replacements to try per position

        Returns:
            attacked_text, attack_info
        """
        # Get gradients
        gradients, input_ids, attention_mask = self.get_embedding_gradients(text, summary)

        # Compute gradient norms for each position
        grad_norms = gradients.norm(dim=-1).squeeze()  # [seq_len]

        # Mask special tokens (don't flip them)
        special_token_ids = [
            self.tokenizer.pad_token_id,
            self.tokenizer.eos_token_id,
            self.tokenizer.bos_token_id,
        ]

        tokens = input_ids.squeeze().cpu().numpy()
        for idx, token in enumerate(tokens):
            if token in special_token_ids or idx == 0:  # Also protect first token
                grad_norms[idx] = 0

        # Find top-k positions with highest gradient norms
        top_positions = grad_norms.argsort(descending=True)[:num_flips]

        # Create attacked version
        attacked_ids = input_ids.clone()
        flip_info = []

        for pos in top_positions:
            pos_idx = pos.item()
            current_token = tokens[pos_idx]
            gradient_vec = gradients[0, pos_idx]

            # Find best replacement token
            candidate_tokens, candidate_scores = self.find_best_replacement(
                pos_idx, gradient_vec, current_token, top_k=beam_size
            )

            # Use the best candidate
            best_token = candidate_tokens[0]
            attacked_ids[0, pos_idx] = best_token

            flip_info.append({
                'position': pos_idx,
                'original_token': self.tokenizer.decode([current_token]),
                'new_token': self.tokenizer.decode([best_token]),
                'gradient_norm': grad_norms[pos_idx].item(),
                'score': candidate_scores[0]
            })

        # Decode attacked text
        attacked_text = self.tokenizer.decode(attacked_ids[0], skip_special_tokens=True)
        attacked_text = attacked_text.replace("summarize: ", "").strip()

        return attacked_text, flip_info

    def evaluate_attack_batch(self, texts, summaries, num_flips=5):
        """
        Attack multiple examples and compute statistics.
        """
        results = []

        progress_bar = tqdm(zip(texts, summaries), total=len(texts), desc="HotFlip Attack")

        for text, summary in progress_bar:
            # Compute original loss
            orig_loss = self.compute_loss(text, summary)

            # Perform attack
            attacked_text, flip_info = self.attack_single(text, summary, num_flips=num_flips)

            # Compute attacked loss
            attack_loss = self.compute_loss(attacked_text, summary)

            # Compute degradation
            degradation = (attack_loss - orig_loss) / orig_loss if orig_loss > 0 else 0

            results.append({
                'original_text': text,
                'attacked_text': attacked_text,
                'original_summary': summary,
                'orig_loss': orig_loss,
                'attack_loss': attack_loss,
                'degradation': degradation,
                'num_flips': len(flip_info),
                'flip_info': flip_info
            })

            # Update progress bar
            avg_deg = np.mean([r['degradation'] for r in results])
            progress_bar.set_postfix({
                'avg_degradation': f'{avg_deg:.2%}',
                'last_degradation': f'{degradation:.2%}'
            })

        return results

"""### Comparative Analysis"""

print("\n" + "="*80)
print("COMPARATIVE ANALYSIS - HOTFLIP")
print("="*80)

# Check if all_attack_results exists (from HotFlip attack evaluation)
if 'all_attack_results' not in globals() or not all_attack_results:
    print("\nWARNING: HotFlip attack results not available.")
    print("Please run the HotFlip attack section first.")
else:
print("\nVulnerability Ranking (Higher = More Vulnerable):")
print("-"*80)

ranked_models = sorted(
    all_attack_results.items(),
    key=lambda x: x[1]['avg_degradation'],
    reverse=True
)

for rank, (model_name, stats) in enumerate(ranked_models, 1):
    print(f"{rank}. {model_name:30s} | Degradation: {stats['avg_degradation']:.2%} | Success: {stats['success_rate']:.1%}")

# Compute robustness improvement
std_deg = all_attack_results['Standard T5']['avg_degradation']
# Check if Monotonic (unfinetuned) exists before accessing - removed as it's no longer included
# mono_unfinetuned_deg = all_attack_results['Monotonic (unfinetuned)']['avg_degradation']
mono_finetuned_deg = all_attack_results['Monotonic (finetuned)']['avg_degradation']

# Removed improvement calculation for unfinetuned model
# improvement_unfinetuned = (std_deg - mono_unfinetuned_deg) / std_deg * 100
improvement_finetuned = (std_deg - mono_finetuned_deg) / std_deg * 100

print(f"\n🛡️ Robustness Improvement vs Standard T5:")
# Removed print statement for unfinetuned model
# print(f"  Monotonic (unfinetuned): {improvement_unfinetuned:+.1f}%")
print(f"  Monotonic (finetuned):   {improvement_finetuned:+.1f}%")

# Statistical significance tests
print(f"\n📈 Statistical Significance Testing:")

std_degradations = [r['degradation'] for r in all_attack_results['Standard T5']['results']]
# Removed unfinetuned degradations
# mono_unfinetuned_degradations = [r['degradation'] for r in all_attack_results['Monotonic (unfinetuned)']['results']]
mono_finetuned_degradations = [r['degradation'] for r in all_attack_results['Monotonic (finetuned)']['results']]

# Removed Test 1: Standard vs Monotonic (unfinetuned)
# t_stat1, p_value1 = stats.ttest_ind(std_degradations, mono_unfinetuned_degradations)
# sig1 = "***" if p_value1 < 0.001 else "**" if p_value1 < 0.01 else "*" if p_value1 < 0.05 else "ns"

# Test 2: Standard vs Monotonic (finetuned)
t_stat2, p_value2 = stats.ttest_ind(std_degradations, mono_finetuned_degradations)
sig2 = "***" if p_value2 < 0.001 else "**" if p_value2 < 0.01 else "*" if p_value2 < 0.05 else "ns"

# Removed Test 3: Monotonic (unfinetuned) vs Monotonic (finetuned)
# t_stat3, p_value3 = stats.ttest_ind(mono_unfinetuned_degradations, mono_finetuned_degradations)
# sig3 = "***" if p_value3 < 0.001 else "**" if p_value3 < 0.01 else "*" if p_value3 < 0.05 else "ns"

# Removed print statement for Test 1
# print(f"\n  Standard vs Monotonic (unfinetuned):")
# print(f"    t-stat: {t_stat1:.4f}, p-value: {p_value1:.6f} {sig1}")

print(f"\n  Standard vs Monotonic (finetuned):")
print(f"    t-stat: {t_stat2:.4f}, p-value: {p_value2:.6f} {sig2}")

# Removed print statement for Test 3
# print(f"\n  Monotonic (unfinetuned) vs Monotonic (finetuned):")
# print(f"    t-stat: {t_stat3:.4f}, p-value: {p_value3:.6f} {sig3}")

print("\n" + "="*80)
print("RUNNING HOTFLIP ATTACK ON MODELS")
print("="*80)

# Ensure models are loaded (from previous cells)
if 'model_standard' not in globals() or 'model_monotonic_unfinetuned' not in globals() or 'model_monotonic_finetuned' not in globals():
    print("WARNING: Models not loaded. Please run the model loading cells first.")
else:
    models_to_attack = {
        'Standard T5': model_standard,
        # 'Monotonic (unfinetuned)': model_monotonic_unfinetuned, # Removed unfinetuned model as requested
        'Monotonic (finetuned)': model_monotonic_finetuned
    }

    # Ensure test data for attack is loaded (from previous cells)
    if 'uat_test_texts' not in globals() or 'uat_test_summaries' not in globals():
         print("WARNING: Test data for attack not loaded. Please run the data loading cell first.")
    else:
        # Use a subset of the test data for the attack to keep it tractable
        num_attack_samples = 100 # Using 100 samples for attack
        attack_texts = uat_test_texts[:num_attack_samples]
        attack_summaries = uat_test_summaries[:num_attack_samples]

        all_attack_results = {}
        HOTFLIP_RESULTS_FILE = os.path.join(SAVE_PATH, 'hotflip_attack_results.json') # Define results file path

        # Try to load existing results
        if os.path.exists(HOTFLIP_RESULTS_FILE):
            print(f"\nFound existing Hotflip results file: {HOTFLIP_RESULTS_FILE}")
            try:
                with open(HOTFLIP_RESULTS_FILE, 'r') as f:
                    loaded_results = json.load(f)
                # Convert list of dicts back to expected structure if needed
                # Assuming the saved format is compatible or can be easily converted
                # For now, assume it saves the all_attack_results dict directly
                all_attack_results = loaded_results
                print(f"   Loaded results for {len(all_attack_results)} model(s).")

                # Check if results for all *required* models are loaded
                missing_models = [name for name in models_to_attack.keys() if name not in all_attack_results]
                if not missing_models:
                    print("\n✓ All required Hotflip results loaded. Skipping attack run.")
                    # Ensure necessary variables for subsequent cells are defined from loaded data
                    # This depends on the exact structure saved in Eu-fIC0cPepo
                    # For now, assume the loaded all_attack_results is sufficient
                else:
                    print(f"\nPartial Hotflip results loaded. Re-running attack for missing models: {missing_models}")
                    # Proceed to run attack only for missing models
                    temp_models_to_attack = {name: models_to_attack[name] for name in missing_models}
                    models_to_attack = temp_models_to_attack # Update models_to_attack for the loop below

            except Exception as e:
                print(f"   WARNING: Error loading results: {e}. Re-running attack for all models.")
                all_attack_results = {} # Clear partial results if loading failed
                # models_to_attack remains the original full list


        # Run attack for models not loaded or if loading failed
        if not all_attack_results or missing_models: # Only run if no results loaded or some results were missing
            for model_name, model in models_to_attack.items(): # Iterate through (potentially updated) models_to_attack
                if model_name not in all_attack_results: # Only attack if not already loaded
                    print(f"\n--- Attacking: {model_name} ---")
                    attacker = HotFlipT5Attack(model, tokenizer, device)

                    # Run the attack on the selected test samples
                    attack_results = attacker.evaluate_attack_batch(attack_texts, attack_summaries, num_flips=5)

                    # Calculate aggregate statistics
                    degradations = [r['degradation'] for r in attack_results]
                    avg_degradation = np.mean(degradations)
                    std_degradation = np.std(degradations)
                    median_degradation = np.median(degradations)
                    max_degradation = np.max(degradations)

                    avg_orig_loss = np.mean([r['orig_loss'] for r in attack_results])
                    avg_attack_loss = np.mean([r['attack_loss'] for r in attack_results])

                    # Define success based on a threshold (e.g., > 10% loss increase or ROUGE drop)
                    # Using loss increase for HotFlip as it's the direct optimization objective
                    success_threshold = 0.10 # 10% relative increase in loss
                    success_rate = np.mean([r['degradation'] > success_threshold for r in attack_results])

                    all_attack_results[model_name] = {
                        'results': attack_results, # Keep per-example results for distribution plotting
                        'avg_degradation': avg_degradation,
                        'std_degradation': std_degradation,
                        'median_degradation': median_degradation,
                        'max_degradation': max_degradation,
                        'avg_orig_loss': avg_orig_loss,
                        'avg_attack_loss': avg_attack_loss,
                        'success_rate': success_rate
                    }
                    print(f"\n✓ Attack complete for {model_name}. Avg Degradation: {avg_degradation:.2%} (Success Rate: {success_rate:.1%})")


            # Save results after potentially running attack(s)
            try:
                with open(HOTFLIP_RESULTS_FILE, 'w') as f:
                    json.dump(all_attack_results, f, indent=2) # Save the full dict
                print(f"\nHotflip results saved to: {HOTFLIP_RESULTS_FILE}")
            except Exception as e:
                 print(f"\nWARNING: Error saving Hotflip results to {HOTFLIP_RESULTS_FILE}: {e}")


        print("\n" + "="*80)
        print("✓ HOTFLIP ATTACK RUN/LOAD COMPLETE")
        print("="*80)

# Ensure variables needed for subsequent cells are defined even if loaded
# This needs to be consistent with what Eu-fIC0cPepo expects
# Based on Eu-fIC0cPepo, it needs: all_attack_results, num_attack_samples, improvement_finetuned, t_stat2, p_value2, sig2
# num_attack_samples is already defined outside the else block
# all_attack_results is populated by loading or running

# Recalculate comparative stats and significance outside the conditional block
if all_attack_results:
    if 'Standard T5' in all_attack_results and 'Monotonic (finetuned)' in all_attack_results:
        std_deg = all_attack_results['Standard T5']['avg_degradation']
        mono_finetuned_deg = all_attack_results['Monotonic (finetuned)']['avg_degradation']
        improvement_finetuned = (std_deg - mono_finetuned_deg) / std_deg * 100

        std_degradations = [r['degradation'] for r in all_attack_results['Standard T5']['results']]
        mono_finetuned_degradations = [r['degradation'] for r in all_attack_results['Monotonic (finetuned)']['results']]

        # Perform t-test
        if len(std_degradations) > 1 and len(mono_finetuned_degradations) > 1:
            t_stat2, p_value2 = stats.ttest_ind(std_degradations, mono_finetuned_degradations)
            sig2 = "***" if p_value2 < 0.001 else "**" if p_value2 < 0.01 else "*" if p_value2 < 0.05 else "ns"
        else:
            t_stat2, p_value2 = float('nan'), float('nan')
            sig2 = 'ns'
            print("\nWARNING: Not enough samples to perform statistical significance test.")

    else:
        print("\nWARNING: Could not calculate comparative stats (missing Standard T5 or Monotonic (finetuned) results).")
        std_deg = float('nan')
        mono_finetuned_deg = float('nan')
        improvement_finetuned = float('nan')
        t_stat2, p_value2 = float('nan'), float('nan')
        sig2 = 'ns'
else:
     print("\nWARNING: No Hotflip results available to calculate comparative stats.")
     std_deg = float('nan')
     mono_finetuned_deg = float('nan')
     improvement_finetuned = float('nan')
     t_stat2, p_value2 = float('nan'), float('nan')
     sig2 = 'ns'

"""### Visualization"""

print("\n" + "="*80)
print("CREATING VISUALIZATIONS")
print("="*80)

fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

model_names = list(all_attack_results.keys())
colors = ['#e74c3c', '#f39c12', '#27ae60'][:len(model_names)] # Adjust colors based on number of models

# Dynamically generate x-tick labels
x_tick_labels = [name.replace(" ", "\n").replace("(unfinetuned)", "(untuned)").replace("(after FT)", "(tuned)") for name in model_names]


# Plot 1: Average Degradation Comparison
ax1 = fig.add_subplot(gs[0, 0])
degradations = [all_attack_results[name]['avg_degradation'] * 100 for name in model_names]
bars = ax1.bar(range(len(model_names)), degradations, color=colors, alpha=0.7, width=0.6)
ax1.set_ylabel('Performance Degradation (%)', fontsize=11)
ax1.set_title('HotFlip Attack Effectiveness', fontsize=12, fontweight='bold')
ax1.set_xticks(range(len(model_names)))
ax1.set_xticklabels(x_tick_labels, fontsize=10) # Use dynamic labels
ax1.grid(True, alpha=0.3, axis='y')
ax1.axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='10% threshold')
ax1.legend(fontsize=9)

for bar, deg in zip(bars, degradations):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
            f'{deg:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Plot 2: Loss Comparison
ax2 = fig.add_subplot(gs[0, 1])
x_pos = np.arange(len(model_names))
width = 0.35
orig_losses = [all_attack_results[name]['avg_orig_loss'] for name in model_names]
attack_losses = [all_attack_results[name]['avg_attack_loss'] for name in model_names]

ax2.bar(x_pos - width/2, orig_losses, width, label='Clean', alpha=0.8, color='#3498db')
ax2.bar(x_pos + width/2, attack_losses, width, label='Attacked', alpha=0.8, color='#e74c3c')
ax2.set_ylabel('Average Loss', fontsize=11)
ax2.set_title('Loss: Clean vs Attacked', fontsize=12, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(x_tick_labels, fontsize=10) # Use dynamic labels
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, axis='y')

# Plot 3: Success Rate
ax3 = fig.add_subplot(gs[0, 2])
success_rates = [all_attack_results[name]['success_rate'] * 100 for name in model_names]
bars = ax3.bar(range(len(model_names)), success_rates, color=colors, alpha=0.7, width=0.6)
ax3.set_ylabel('Success Rate (%)', fontsize=11)
ax3.set_title('Attack Success Rate (≥10% degradation)', fontsize=12, fontweight='bold')
ax3.set_xticks(range(len(model_names)))
ax3.set_xticklabels(x_tick_labels, fontsize=10) # Use dynamic labels
ax3.grid(True, alpha=0.3, axis='y')
ax3.set_ylim([0, 100])

for bar, sr in zip(bars, success_rates):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height,
            f'{sr:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Plot 4: Degradation Distribution
ax4 = fig.add_subplot(gs[1, :])
for model_name, color in zip(model_names, colors):
    degradations = [r['degradation'] * 100 for r in all_attack_results[model_name]['results']]
    ax4.hist(degradations, bins=30, alpha=0.5, label=model_name, color=color)

ax4.axvline(x=10, color='red', linestyle='--', linewidth=2, alpha=0.7, label='10% threshold')
ax4.set_xlabel('Degradation (%)', fontsize=11)
ax4.set_ylabel('Frequency', fontsize=11)
ax4.set_title('Degradation Distribution Across Test Set', fontsize=12, fontweight='bold')
ax4.legend(fontsize=10)
ax4.grid(True, alpha=0.3)

# Plot 5: Robustness Improvement (Corrected to plot only available models)
ax5 = fig.add_subplot(gs[2, 0])
if 'Standard T5' in all_attack_results and len(model_names) > 1: # Only plot if Standard T5 and at least one other model is present
    baseline = all_attack_results['Standard T5']['avg_degradation']
    improvement_data = {}
    improvement_model_names = []
    improvement_colors = []

    for i, name in enumerate(model_names):
        if name == 'Standard T5':
            improvement_data[name] = 0
        else:
            improvement_data[name] = (baseline - all_attack_results[name]['avg_degradation']) / baseline * 100

        improvement_model_names.append(name)
        improvement_colors.append(colors[i])


    bars = ax5.bar(range(len(improvement_model_names)), improvements, color=improvement_colors, alpha=0.7, width=0.6)
    ax5.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax5.set_ylabel('Robustness Improvement (%)', fontsize=11)
    ax5.set_title('Relative Robustness vs Standard T5', fontsize=12, fontweight='bold')
    ax5.set_xticks(range(len(improvement_model_names)))
    ax5.set_xticklabels([name.replace(" ", "\n").replace("(unfinetuned)", "(untuned)").replace("(after FT)", "(tuned)") for name in improvement_model_names], fontsize=10)
    ax5.grid(True, alpha=0.3, axis='y')

    for bar, imp in zip(bars, improvements):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top',
                fontsize=10, fontweight='bold')
else:
    ax5.set_title('Relative Robustness vs Standard T5', fontsize=12, fontweight='bold')
    ax5.text(0.5, 0.5, "Insufficient data for comparison", horizontalalignment='center', verticalalignment='center', transform=ax5.transAxes, fontsize=12)
    ax5.set_xticks([])
    ax5.set_yticks([])


# Plot 6: Box Plot Comparison
ax6 = fig.add_subplot(gs[2, 1])
data_to_plot = []
box_plot_labels = []
box_plot_colors = []
for model_name, color in zip(model_names, colors):
    # if model_name != 'Monotonic (unfinetuned)': # Exclude unfinetuned from this plot unless explicitly requested
    degradations = [r['degradation'] * 100 for r in all_attack_results[model_name]['results']]
    data_to_plot.append(degradations)
    box_plot_labels.append(model_name.replace(" ", "\n").replace("(unfinetuned)", "(untuned)").replace("(after FT)", "(tuned)"))
    box_plot_colors.append(color) # Use color corresponding to model_name

if data_to_plot:
    bp = ax6.boxplot(data_to_plot, labels=box_plot_labels,
                      patch_artist=True, showmeans=True)

    for patch, color in zip(bp['boxes'], box_plot_colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax6.set_ylabel('Degradation (%)', fontsize=11)
    ax6.set_title('Degradation Distribution (Box Plot)', fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='y')
    ax6.axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5)
else:
    ax6.set_title('Degradation Distribution (Box Plot)', fontsize=12, fontweight='bold')
    ax6.text(0.5, 0.5, "No data to plot", horizontalalignment='center', verticalalignment='center', transform=ax6.transAxes, fontsize=12)
    ax6.set_xticks([])
    ax6.set_yticks([])


# Plot 7: Per-Example Comparison
ax7 = fig.add_subplot(gs[2, 2]) # Using ax7 for the third row, third column
if model_names and all(len(all_attack_results[name]['results']) > 0 for name in model_names): # Check if any models and results exist
    x = range(min(20, len(all_attack_results[model_names[0]]['results']))) # Use results length for x range
    for model_name, color in zip(model_names, colors):
        degradations = [r['degradation'] * 100 for r in all_attack_results[model_name]['results'][:20]]
        ax7.plot(x, degradations, 'o-', label=model_name, color=color, alpha=0.7, linewidth=2)

    ax7.axhline(y=10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='10% threshold')
    ax7.set_xlabel('Example Index', fontsize=11)
    ax7.set_ylabel('Degradation (%)', fontsize=11)
    ax7.set_title('Per-Example Degradation (First 20)', fontsize=12, fontweight='bold')
    ax7.legend(fontsize=9)
    ax7.grid(True, alpha=0.3)
else:
    ax7.set_title('Per-Example Degradation (First 20)', fontsize=12, fontweight='bold')
    ax7.text(0.5, 0.5, "No data to plot", horizontalalignment='center', verticalalignment='center', transform=ax7.transAxes, fontsize=12)
    ax7.set_xticks([])
    ax7.set_yticks([])


# Remove unused subplots if any - Corrected logic
# Identify all created axes
all_axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7] # Added ax3
# Get the list of all subplot axes in the figure
fig_axes = fig.get_axes()

# Remove axes that were not explicitly created
for ax in fig_axes:
    if ax not in all_axes:
        fig.delaxes(ax)


plt.suptitle('HotFlip Attack: Comprehensive Evaluation', fontsize=14, fontweight='bold', y=0.995)

plot_path = os.path.join(SAVE_PATH, 'hotflip_attack_results.png')
plt.savefig(plot_path, dpi=150, bbox_inches='tight')
plt.show()

print(f"\n✓ Visualization saved to: {plot_path}")

"""### Example Outputs"""

print("\n" + "="*80)
print("EXAMPLE ADVERSARIAL OUTPUTS")
print("="*80)

# Show 3 examples
for example_idx in [0, 10, 20]:
    print(f"\n{'='*80}")
    print(f"EXAMPLE {example_idx + 1}")
    print(f"{'='*80}")

    original_text = attack_texts[example_idx]
    original_summary = attack_summaries[example_idx]

    print(f"\nOriginal Text:")
    print(f"  {original_text[:200]}...")

    print(f"\nGround Truth Summary:")
    print(f"  {original_summary}")

    for model_name in model_names:
        result = all_attack_results[model_name]['results'][example_idx]

        print(f"\n{model_name}:")
        print(f"  Attacked Text:  {result['attacked_text'][:200]}...")
        print(f"  Degradation:    {result['degradation']:.2%}")
        print(f"  Flips made:     {result['num_flips']}")

        # Show flip details
        if result['flip_info']:
            print(f"  Token changes:")
            for flip in result['flip_info'][:3]:  # Show first 3 flips
                print(f"    - '{flip['original_token']}' → '{flip['new_token']}' (grad: {flip['gradient_norm']:.3f})")

"""### Save Results"""

print("\n" + "="*80)
print("SAVING RESULTS")
print("="*80)

# Prepare results for JSON (exclude full results to save space)
summary_results = {
    'attack_config': {
        'method': 'HotFlip',
        'num_samples': num_attack_samples,
        'num_flips_per_sample': 5
    },
    'models': {
        model_name: {
            'avg_degradation': float(stats['avg_degradation']),
            'std_degradation': float(stats['std_degradation']),
            'median_degradation': float(stats['median_degradation']),
            'max_degradation': float(stats['max_degradation']),
            'avg_orig_loss': float(stats['avg_orig_loss']),
            'avg_attack_loss': float(stats['avg_attack_loss']),
            'success_rate': float(stats['success_rate'])
        }
        for model_name, stats in all_attack_results.items()
    },
    'robustness_improvement': {
        # 'unfinetuned_vs_standard': float(improvement_unfinetuned), # Removed as unfinetuned is excluded
        'finetuned_vs_standard': float(improvement_finetuned)
    },
    'statistical_tests': {
        # 'standard_vs_unfinetuned': { # Removed as unfinetuned is excluded
        #     't_statistic': float(t_stat1),
        #     'p_value': float(p_value1),
        #     'significance': sig1
        # },
        'standard_vs_finetuned': {
            't_statistic': float(t_stat2),
            'p_value': float(p_value2),
            'significance': sig2
        },
        # 'unfinetuned_vs_finetuned': { # Removed as unfinetuned is excluded
        #     't_statistic': float(t_stat3),
        #     'p_value': float(p_value3),
        #     'significance': sig3
        # }
    }
}

results_path = os.path.join(SAVE_PATH, 'hotflip_attack_summary.json')
with open(results_path, 'w') as f:
    json.dump(summary_results, f, indent=2)

print(f"✓ Summary saved to: {results_path}")

# ======================================================================
# MULTI-SEED EVALUATION FRAMEWORK
# ======================================================================

"""
## Multi-Seed Evaluation Instructions

To run proper multi-seed evaluation (≥5 seeds) as recommended for robust comparisons:

### Method 1: Sequential Runs (Manual)

1. For each seed in ExperimentConfig.RANDOM_SEEDS:
   - Set ExperimentConfig.CURRENT_SEED to that seed
   - Re-run the entire notebook from the beginning
   - Results will be saved with seed-specific identifiers

2. After all runs complete, aggregate results:
   ```python
   # Aggregate results from multiple seeds
   all_seed_results = []
   for seed in ExperimentConfig.RANDOM_SEEDS:
       seed_results_path = os.path.join(SAVE_PATH, f'results_seed_{seed}.json')
       if os.path.exists(seed_results_path):
           with open(seed_results_path, 'r') as f:
               all_seed_results.append(json.load(f))
   
   # Compute mean ± std for each metric
   rouge_scores = defaultdict(list)
   for result in all_seed_results:
       for metric in ExperimentConfig.ROUGE_METRICS:
           rouge_scores[metric].append(result['rouge'][metric]['mean'])
   
   final_results = {
       metric: {
           'mean': np.mean(scores),
           'std': np.std(scores),
           'individual_seeds': scores
       }
       for metric, scores in rouge_scores.items()
   }
   ```

### Method 2: Automated Multi-Seed Runner (Advanced)

```python
def run_experiment_with_seed(seed):
    '''Run full experiment with given seed'''
    # Set seed
    ExperimentConfig.CURRENT_SEED = seed
    set_all_seeds(seed)
    
    # Train models
    # ... (training code)
    
    # Evaluate models
    # ... (evaluation code)
    
    # Return results
    return {
        'seed': seed,
        'rouge_scores': ...,
        'attack_results': ...,
        'training_time': ...,
    }

# Run for all seeds
all_results = []
for seed in ExperimentConfig.RANDOM_SEEDS:
    print(f"\n{'='*80}")
    print(f"RUNNING EXPERIMENT WITH SEED {seed} ({ExperimentConfig.RANDOM_SEEDS.index(seed)+1}/{len(ExperimentConfig.RANDOM_SEEDS)})")
    print(f"{'='*80}")
    
    results = run_experiment_with_seed(seed)
    all_results.append(results)
    
    # Save intermediate results
    torch.save(results, os.path.join(SAVE_PATH, f'results_seed_{seed}.pt'))

# Aggregate and report
aggregate_results = aggregate_multi_seed_results(all_results)
print_final_comparison_table(aggregate_results)
```

### What to Report

For each metric and model:
- **Mean** across seeds
- **Standard deviation** across seeds  
- **95% Confidence Interval** (mean ± 1.96*std/sqrt(n))
- **Min/Max** values across seeds
- Individual seed results

Example final table:

| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | Attack Robustness |
|-------|---------|---------|---------|------------------|
| Standard T5 | 0.412 ± 0.008 | 0.189 ± 0.006 | 0.285 ± 0.007 | 0.234 ± 0.012 |
| Baseline T5 | 0.428 ± 0.007 | 0.201 ± 0.005 | 0.298 ± 0.006 | 0.241 ± 0.011 |
| Monotonic T5 | 0.425 ± 0.009 | 0.198 ± 0.007 | 0.295 ± 0.008 | 0.189 ± 0.010 |

### Statistical Significance Testing

```python
from scipy import stats

# Paired t-test between models across seeds
baseline_scores = [result['baseline_rouge1'] for result in all_results]
monotonic_scores = [result['monotonic_rouge1'] for result in all_results]

t_stat, p_value = stats.ttest_rel(baseline_scores, monotonic_scores)
print(f"Baseline vs Monotonic: t={t_stat:.3f}, p={p_value:.4f}")

if p_value < 0.05:
    print("✓ Difference is statistically significant at α=0.05")
else:
    print("⚠ Difference is NOT statistically significant")
```

### Current Status

**Note:** This notebook currently runs with a SINGLE seed (configurable via ExperimentConfig.CURRENT_SEED).
To get robust, publication-quality results, run the experiment with all 5 seeds and aggregate.

The framework above provides a template for multi-seed evaluation. For automated multi-seed runs:
1. Package the experiment logic into functions
2. Use the seed loop above
3. Aggregate results across seeds
4. Report mean ± std with confidence intervals

"""

print("\n" + "="*80)
print("EXPERIMENT COMPLETE - METHODOLOGICALLY FAIR COMPARISON")
print("="*80)
print("""
✓ All major methodological issues addressed:
  1. Fair baseline training (unconstrained T5 with identical hyperparameters)
  2. Full test set evaluation (or configurable subset)
  3. Comprehensive determinism and reproducibility
  4. Bootstrap confidence intervals for ROUGE
  5. Length statistics and brevity penalty
  6. Held-out attack evaluation (validation for optimization, test for evaluation)
  7. Comprehensive experiment metadata logging
  8. Fixed decoding parameters across all models
  9. Multi-seed evaluation framework provided (run manually for each seed)

⚠ For publication: Run with all 5 seeds and report mean ± std
  - Current run uses seed: {ExperimentConfig.CURRENT_SEED}
  - See multi-seed evaluation instructions above

Results saved in: {ExperimentConfig.SAVE_PATH}
  - experiment_metadata.json: Complete experiment configuration
  - Model checkpoints: baseline_checkpoints/, monotonic_checkpoints/
  - Evaluation results: Various JSON files with comprehensive metrics
  - Attack results: UAT and HotFlip with held-out evaluation

📖 Review the METHODOLOGICAL FAIRNESS CHECKLIST at the top of this file
   for a complete list of improvements made.
""".format(ExperimentConfig=ExperimentConfig))

print("="*80)