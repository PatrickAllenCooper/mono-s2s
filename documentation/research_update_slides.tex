\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{xspace}

\title{Monotone LLMs}
\subtitle{Research Group Update}
\date{}
\author{}
\institute{}

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents
\end{frame}

\section{Method}

%==============================================================================
% SLIDE 1: MONOTONIC CONSTRAINTS
%==============================================================================
\begin{frame}{Monotonic Feed-Forward Layers}

\begin{block}{Key Idea}
Enforce $W \geq 0$ in FFN sublayers via softplus reparameterization:
\[
W = \log(1 + \exp(V)) \geq 0
\]
\end{block}

\vspace{0.3cm}

\begin{itemize}
    \item \textbf{Scope:} T5 FFN projections (\texttt{wi}, \texttt{wo})
    \item \textbf{Unconstrained:} Attention, LayerNorm, residuals
    \item \textbf{Init:} $V_{\text{init}} = \text{softplus}^{-1}(|W_{\text{pre}}| + \epsilon)$
\end{itemize}

\end{frame}

%==============================================================================
% SLIDE 2: TRAINING PROTOCOL
%==============================================================================
\begin{frame}{Training Protocol}

\begin{table}
    \caption{Training configuration (T5-small)}
    \begin{tabular}{lcc}
      \toprule
      & Baseline & Monotonic \\
      \midrule
      Epochs & 5 & 7 \\
      Warmup & 10\% & 15\% \\
      Learning Rate & \multicolumn{2}{c}{$5\times10^{-5}$} \\
      Best Val Loss & 2.25 & 2.56 \\
      \bottomrule
    \end{tabular}
\end{table}

\vspace{0.2cm}

\textbf{Data:} DialogSum, HighlightSum, arXiv ($\sim$50K examples)\\
\textbf{Eval:} CNN/DailyMail, ROUGE with 95\% bootstrap CIs

\end{frame}

%==============================================================================
% SLIDE 3: CLEAN PERFORMANCE
%==============================================================================
\begin{frame}{Clean Performance}

\begin{table}
    \caption{ROUGE scores on CNN/DailyMail}
    \begin{tabular}{lccc}
      \toprule
      Model & ROUGE-1 & ROUGE-2 & ROUGE-L \\
      \midrule
      Standard T5  & \textbf{0.325} & \textbf{0.119} & \textbf{0.265} \\
      Baseline T5  & 0.286 & 0.105 & 0.229 \\
      Monotonic T5 & 0.270 & 0.088 & 0.217 \\
      \bottomrule
    \end{tabular}
\end{table}

\vspace{0.3cm}

Monotonic model incurs $\sim$6\% relative ROUGE-1 drop vs baseline.

\end{frame}

\section{Results}

%==============================================================================
% SLIDE 4: WHY HOTFLIP
%==============================================================================
\begin{frame}{Why HotFlip Attacks?}

\begin{block}{HotFlip is a First-Order Attack}
Uses gradient $\nabla_e \mathcal{L}$ w.r.t.\ input embeddings to find adversarial token replacements that maximally increase loss.
\end{block}

\vspace{0.3cm}

\textbf{Why this matters for monotonicity:}
\begin{itemize}
    \item Monotone FFNs constrain gradient sign: $\frac{\partial f}{\partial x} \geq 0$
    \item Adversarial gradients cannot exploit sign-flipping directions
    \item If monotonicity helps, HotFlip should be \emph{less effective}
\end{itemize}

\vspace{0.3cm}

\alert{HotFlip directly tests whether non-negative weights reduce gradient exploitability.}

\end{frame}

%==============================================================================
% SLIDE 5: HOTFLIP PROCEDURE
%==============================================================================
\begin{frame}{HotFlip Attack Procedure}

\begin{enumerate}
    \item \textbf{Compute gradient:} $g = \nabla_{e_i} \mathcal{L}(x, y)$ for each input token $i$
    \item \textbf{Score replacements:} For candidate token $t'$, compute $g^\top (e_{t'} - e_i)$
    \item \textbf{Select flip:} Choose $(i, t')$ maximizing score
    \item \textbf{Repeat:} Apply up to $k=5$ flips per example
\end{enumerate}

\vspace{0.3cm}

\begin{exampleblock}{Intuition}
Gradient points toward embedding changes that increase loss most rapidly. HotFlip follows this direction in the discrete token space.
\end{exampleblock}

\end{frame}

%==============================================================================
% SLIDE 6: HOTFLIP RESULTS
%==============================================================================
\begin{frame}{HotFlip Results}

\begin{table}
    \caption{HotFlip attack on CNN/DailyMail (lower = more robust)}
    \begin{tabular}{lccc}
      \toprule
      Model & Degradation & Success Rate & $\Delta$Loss \\
      \midrule
      Standard T5  & 21.4\% & 70\% & +0.41 \\
      Baseline T5  & 14.6\% & 54\% & +0.31 \\
      \alert{Monotonic T5} & \alert{9.4\%} & \alert{39\%} & \alert{+0.24} \\
      \bottomrule
    \end{tabular}
\end{table}

\vspace{0.3cm}

\begin{itemize}
    \item \textbf{Degradation:} Average ROUGE-L drop under attack
    \item \textbf{Success Rate:} Fraction with $>$10\% degradation
    \item \textbf{$\Delta$Loss:} Mean NLL increase from flips
\end{itemize}

\end{frame}

%==============================================================================
% SLIDE 7: INTERPRETATION
%==============================================================================
\begin{frame}{Interpretation}

\begin{columns}[T]
\column{0.5\textwidth}
\textbf{Monotonic vs Standard:}
\begin{itemize}
    \item 56\% less ROUGE degradation
    \item 44\% lower attack success
    \item 41\% smaller loss increase
\end{itemize}

\column{0.5\textwidth}
\textbf{Monotonic vs Baseline:}
\begin{itemize}
    \item 36\% less degradation
    \item 28\% lower success rate
    \item 22\% smaller loss increase
\end{itemize}
\end{columns}

\vspace{0.5cm}

\begin{alertblock}{Takeaway}
Non-negative weight constraints reduce gradient-based attack effectiveness without adversarial training.
\end{alertblock}

\end{frame}

%==============================================================================
% SLIDE 8: SUMMARY
%==============================================================================
\begin{frame}{Summary}

\textbf{Clean performance:} Monotonic model pays $\sim$6\% ROUGE cost

\vspace{0.3cm}

\textbf{Robustness gain:} Under HotFlip attack:
\begin{itemize}
    \item ROUGE degradation: 21.4\% $\rightarrow$ 9.4\%
    \item Attack success: 70\% $\rightarrow$ 39\%
\end{itemize}

\vspace{0.3cm}

\textbf{Next steps:}
\begin{itemize}
    \item Scale to T5-base/large
    \item Full test set evaluation
    \item Certified robustness bounds
\end{itemize}

\end{frame}

\section{Theory}

%==============================================================================
% SLIDE 9: THEORY PLACEHOLDER 1
%==============================================================================
\begin{frame}{Theory: Monotonicity Fundamentals}
% To be filled
\end{frame}

%==============================================================================
% SLIDE 10: THEORY PLACEHOLDER 2
%==============================================================================
\begin{frame}{Theory: Robustness Implications}
% To be filled
\end{frame}

{\setbeamercolor{palette primary}{fg=black, bg=yellow}
\begin{frame}[standout]
  Questions?
\end{frame}
}

\end{document}
