%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables


\newcommand{\N}{\mathbb{N}}
\newcommand{\MSE}{\mathsf{MSE}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sys}{\mathfrak{S}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\seq}[1]{\langle #1 \rangle}
\newcommand{\Relu}{\mathsf{ReLU}}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% For highlighting additions in blue
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Monotonicity as an Architectural Bias for Robust Language Models}

\begin{document}

\twocolumn[
\icmltitle{Monotonicity as an Architectural Bias for Robust Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in output.

We investigate \emph{monotonicity} as an architectural inductive bias for improving the robustness of Transformer-based language models. At a high level, monotonicity constrains a model so that its outputs change in a predictable direction with respect to changes in its inputs. Such constraints have long been used in domains such as control and safety-critical systems, where they simplify reasoning about system behavior and improve robustness. However, they have traditionally been viewed as impractical for expressive neural models due to perceived losses in performance.

We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers, we obtain monotone language models that preserve the performance of their pretrained counterparts. Despite their constrained structure, these models exhibit substantially improved robustness to a range of adversarial and jailbreak attacks.

Our results demonstrate that strong architectural constraints, long assumed to limit the capacity of modern language models, can be applied at scale without sacrificing accuracy. Monotonicity thus emerges as a practical and principled design choice for building more robust and predictable language models.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language tasks, yet their behavior under adversarial or carefully structured inputs remains brittle. Even models that have undergone extensive alignment and fine-tuning can be induced to produce unsafe or unintended outputs through jailbreak prompts or small, targeted perturbations. This phenomenon is now well documented and points to a deeper underlying issue: modern language models operate in extremely high-dimensional spaces, where even subtle changes to the input or internal representations can induce large and difficult-to-predict changes in the output. Recent work has attributed this sensitivity, in part, to the high Lipschitz constants exhibited by such models, which amplify perturbations as they propagate through the network~\cite{newhouse2025training}.

This fragility has motivated an active line of research on improving the robustness of language models~\cite{salah2026jailbreaking, su2024mission}. Existing approaches largely focus on training-time interventions, such as alignment objectives~\cite{cao2024defending}, adversarial training~\cite{howe2024effects}, or post-hoc filtering mechanisms~\cite{khachaturov2025adversarial}. While these methods can be effective empirically, they are inherently reactive, addressing vulnerabilities after they are discovered rather than constraining model behavior by design. As language models are increasingly deployed, this raises a fundamental question: can we endow language models with structural properties that make their behavior more predictable under perturbation?

We investigate \emph{monotonicity} as a structural inductive bias for improving the robustness of language models. At a high level, monotonicity constrains a model so that its outputs change in a predictable direction with respect to changes in its inputs. This property has a long history in machine learning~\cite{Gupta2016MonotonicLattices, runje2023constrained} and related fields~\cite{smith1995monotone}, and has been especially influential in control theory, where monotone systems admit strong guarantees on stability and safety even in very high-dimensional settings~\cite{ito2014max}. For example, monotonicity has been used to analyze and certify the behavior of large-scale dynamical systems such as power grids, where direct reasoning over all possible system states is infeasible~\cite{rantzer2014control}.

Beyond its role in classical dynamical systems, monotonicity also aligns naturally with how we expect language models to behave semantically. Intuitively, prompts that are semantically equivalent (or differ only in minor, non-essential ways) should produce similar outputs. From this perspective, modern Transformer architectures can be viewed as mapping inputs to latent representations through attention mechanisms that act as powerful feature extractors. If two prompts induce similar internal representations, their outputs should likewise be similar. Extending this intuition, if three prompts produce representations ordered such that one lies between the other two in feature space, then it is reasonable to expect the corresponding output to lie between the two extremes. Monotonicity enforces precisely this kind of ordered consistency, ensuring that smooth and meaningful variations in representation space lead to predictable and bounded changes in output space. Figure~\ref{fig:monotonicity_schematic} provides a schematic illustration of this behavior.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{icml_fig1.png}
    \caption{
    \textbf{Monotonicity as a structural inductive bias for language models.}
    (a) Semantically similar prompts induce nearby latent representations through shared feature extraction, leading to similar outputs.
    (b) Monotonicity enforces ordered consistency: when a prompt occupies an lies between two others in internal representation space, the resulting output is constrained to fall between their outputs.
    (c) Monotonicity induces an order-preserving mapping from internal representations to outputs, which constrains how model behavior can change in response to variations in representation space.
    }
    \label{fig:monotonicity_schematic}
\end{figure*}


The appeal of monotonicity in both control systems and representation learning lies in its ability to simplify reasoning about complex behavior. In monotone systems, global behavior can often be inferred by examining only extreme or boundary cases. This perspective naturally suggests viewing a language model as a high-dimensional system whose internal representations evolve through successive nonlinear transformations in response to input perturbations. Imposing monotonic structure on these transformations can substantially reduce unpredictability, limiting how spurious or adversarial variations propagate through the model.

Despite these advantages, monotonic constraints have traditionally been viewed as incompatible with expressive neural architectures. Enforcing monotonicity was long believed to significantly limit model capacity or degrade performance~\cite{runje2023constrained}, making it impractical for modern models such as Transformers. In this work, we challenge this assumption.

Our central finding is that monotonicity can be incorporated into modern Transformer architectures without sacrificing performance. Rather than constraining the entire model, we apply monotonicity selectively to the feed-forward sublayers that dominate the model’s nonlinear transformations, while leaving attention mechanisms, residual connections, and normalization layers unconstrained. To enable effective optimization under these constraints, we adopt a smooth reparameterization that avoids the degeneracies associated with naive projection-based approaches. This design allows pretrained sequence-to-sequence models to be distilled into monotone counterparts that closely match their original performance on standard summarization benchmarks.

The resulting models exhibit a striking empirical property: despite preserving task accuracy, monotone Transformers are substantially more robust to adversarial and jailbreak attacks. These improvements are not achieved through additional data, modified training objectives, or heuristic defenses, but emerge purely from architectural structure. This isolation makes the observed robustness particularly notable, as it identifies monotonicity itself (rather than confounding training choices) as a causal factor.

Taken together, these results highlight two broader implications. First, they demonstrate that strong architectural constraints, long assumed to be impractical for large language models, can in fact be applied at scale without noticeable degrading performance. Second, they suggest that robustness in language models need not rely exclusively on increasingly complex training pipelines, but can instead be supported by principled design choices that shape model behavior by construction. Monotonicity thus emerges as a practical and scalable inductive bias for building more robust and predictable language models.

\noindent\textbf{Contributions.}
Our contributions are summarized as follows:
\begin{itemize}
\item \textbf{Monotone building blocks for LLMs.} We introduce a framework for constructing language models using monotone Transformer components, enforcing structural monotonicity within feed-forward sublayers while leaving attention mechanisms unconstrained.
\item \textbf{Distillation without loss of capability.} We show that pretrained, non-monotone language models can be distilled into monotone counterparts with no noticeable degradation in standard benchmark performance.
\item \textbf{Empirical robustness improvements.} Through adversarial evaluation, we demonstrate that monotone language models exhibit substantially improved robustness to jailbreak and adversarial attacks, providing evidence that monotonicity serves as an effective inductive bias for improving model stability.
\end{itemize}


\section{Related Work}

% Our work situates itself at the intersection of robustness in language models, architectural constraints in neural networks, and structural approaches to improving model predictability and safety. We review relevant lines of research on adversarial vulnerabilities of LLMs, structural and monotonic network design, model verification and certification, and related defenses.

\paragraph{Vulnerabilities of Language Models.}
Large language models have been shown to be highly susceptible to diverse adversarial manipulations that can bypass safety filters and elicit harmful or unintended outputs. Research in this area has documented both jailbreaks (crafted prompts that circumvent alignment measures) and transferable suffix attacks that generalize across models. For example, universal adversarial suffixes have been demonstrated to induce objectionable content in widely used models including GPT, Bard, and Claude, even when aligned, and can transfer from one model to another using greedy or gradient-based optimization techniques~\citep{Zou2023UniversalTransferableAttacksOnAlignedLLMs}. Studies have further explored the position sensitivity of jailbreak attacks and shown that simple adversarial snippets positioned at specific output locations can dramatically increase attack success rates, revealing nuanced vulnerabilities in model safety mechanisms~\citep{wang2025vulnerability}.

In parallel, interpretable adversarial attack methods such as AutoDAN have been proposed, which generate prompts that evade perplexity-based defenses and achieve high attack success while remaining human-interpretable, underscoring the fragility of current safety filters~\citep{Zhu2023AutoDAN}. Posters at major venues have also highlighted structural vulnerabilities, showing that larger context windows and many-shot jailbreaking strategies can exacerbate susceptibility to adversarial injections, even when models are nominally safety-trained (e.g., “Many-shot jailbreaks”)~\citep{anil2024many}.

Benchmarks have been developed to systematically quantify LLM robustness. For instance, Adversarial GLUE showed that existing models and robust training methods perform poorly under a suite of adversarial text attacks, reinforcing the need for broader robustness evaluation beyond standard test sets~\citep{Wang2021AdvGLUE}. Additional work has examined multilingual jailbreak challenges, revealing that safety mechanisms calibrated for English can fail dramatically in low-resource languages or when malicious instructions are combined with multilingual prompts~\citep{deng2024multilingual}.

Prompt injection attacks, a class of adversarial exploitations wherein innocuous-looking inputs manipulate model behavior by merging instructions and data, have been recognized as a fundamental vulnerability of modern LLMs due to the lack of separation between instruction and context. Comprehensive surveys of LLM security concerns further categorize threats ranging from inference-time attacks to training-time manipulations and model misuse, pointing to broad systemic risks in current LLM deployments~\citep{li2025security}.

\paragraph{Defenses and Robustness Enhancements.}
Efforts to defend against LLM vulnerabilities include techniques such as smoothing defenses designed to mitigate jailbreak attacks with provable performance bounds, as well as detection mechanisms that distinguish adversarial prompts from benign ones by analyzing output distribution differences~\citep{Robey2023SmoothLLM}; alternative methods achieve jailbreak detection with minimal computational overhead~\citep{sayeedi2025jailbreaktracer}.

Scaling behavior in language model robustness has been examined, with recent work showing that larger models are not uniformly more robust without explicit adversarial training, and that offense and defense capabilities scale differently across tasks and threat models~\citep{howe2024effects}. Other defense directions explore improving transferability of jailbreak attacks or adapting prompt sequences to evaluate and harden models against distributional dependencies in adversarial sequences~\citep{wang2025understanding}.

\paragraph{Monotonic Neural Networks and Structural Constraints.}
Monotonicity has long been studied as a structural property in neural networks, motivated by applications requiring interpretability, robustness, or domain-consistent behavior. Early work showed that feed-forward networks with non-negative weights and monotone activation functions are universal approximators of monotone functions, establishing monotonicity as a theoretically sound inductive bias rather than a restrictive limitation~\citep{Sill1998MonotonicNetworks,Daniels2010Monotone}. Subsequent research developed practical constructions of deep monotonic networks, including lattice-based models and partially monotone architectures, enabling scalable learning under monotonicity constraints in tabular and structured domains~\citep{Gupta2016MonotonicLattices,You2017DeepLattice}.

More recent work has explored monotonicity as a means of improving model reliability and certifiability. For example, monotone neural networks have been shown to simplify verification by enabling reasoning over boundary inputs rather than the full input space, particularly for piecewise linear architectures~\citep{Weber2021CertifyingMonotonicNetworks}. However, existing monotonic architectures have primarily been evaluated on low-dimensional or structured prediction tasks, and their applicability to large-scale sequence models and pretrained language models remains largely unexplored.

Beyond monotonicity, architectural constraints more broadly have been investigated as inductive biases for improving robustness and stability in neural networks. Constrained optimization layers and structured network designs have demonstrated that carefully imposed constraints can guide model behavior under perturbations without severely compromising expressive power~\citep{Amos2017OptNet}. These results support the premise that structural constraints, when selectively applied, can improve desirable properties of neural networks while retaining practical performance.


% \paragraph{Verification and Certification.}
% Formal verification aims to provide mathematical assurances about model behavior under specified perturbations. Techniques based on abstract interpretation, convex relaxations, and over-approximation have been applied to neural networks to certify robustness within normed perturbation balls, though these methods often struggle to scale to large models common in LLMs~\citep{Turn0Search1}:contentReference[oaicite:15]{index=15}. Monotonicity, in principle, simplifies some verification tasks by enabling boundary evaluation rather than exhaustive search, but the application of such reasoning to large-scale language models remains an open challenge.

% \paragraph{Distillation and Constrained Training.}
% Model distillation has been used to transfer knowledge and robustness properties between models, but naive distillation alone can be vulnerable to adversarial attacks that exploit weaknesses in the reference model or training procedure~\citep{Carlini2016DefensiveDistillationBreaks}. More recent work explores structured distillation objectives to improve performance under constraints, aligning closely with our use of distillation to preserve performance under architectural constraints.



\section{Preliminaries}
\label{Prelim}
\paragraph{Notations.} We denote the set of real, non-negative real, and positive real numbers by \( \R \), \( \R_{\geq 0} \), and \( \R_{>0} \), respectively. For sets \( A \) and \( B \), we use \( A \setminus B \) for set difference and \( A \times B \) for Cartesian product, and write \( |A| \) for the cardinality of \( A \). 
Given a vector \( v = (v_1,\ldots,v_n)^\top \in \R^n \), its Euclidean norm is \( \|v\| = \sqrt{\sum_{i=1}^n v_i^2} \). The rectified linear unit is defined by \( \Relu(x) := \max(x,0) \), and for vectors \( x, y \in \R^n \), the mean squared error is \( \MSE(x, y) := \frac{1}{n} \sum_{i=1}^n (x_i - y_i)^2 \). We denote the \( i \)th component of a vector \( x \) by \( x_i \), and write \( x \le y \) to indicate element-wise inequality.
% A set \( \mathcal{S} \subseteq \R^n \) is \emph{lower closed} if \( x \in \mathcal{S} \) and \( y \le x \) imply \( y \in \mathcal{S} \); it is \emph{upper closed} if \( x \in \mathcal{S} \) and \( y \ge x \) imply \( y \in \mathcal{S} \). A hyper-rectangle is denoted by \( [\underline{x}, \overline{x}] := \{ x \in \R^n \mid \underline{x} \le x \le \overline{x} \} \). Given a set $A\subseteq \R^n$, $x\in \R^n$ is a maximal (resp. minimal) point of $A$, if there does not exists any $y\in A$ such that $x\le y$ (resp. $y\le x$). 

\subsection{Monotonicity-Constrained Sequence-to-Sequence Models}

We study the role of monotonicity constraints in improving the adversarial robustness of sequence-to-sequence language models.

\begin{definition}[Sequence-to-Sequence Model]
A sequence-to-sequence model is a parameterized mapping
\[
\mathcal{M}_\theta : \mathcal{X} \to \mathcal{Y},
\]
where $\mathcal{X}$ denotes the space of input token sequences and $\mathcal{Y}$ denotes the space of output token sequences. The model $\mathcal{M}_\theta$ is typically instantiated as a Transformer architecture composed of attention layers, feed-forward network (FFN) sublayers, residual connections, and normalization operators.
\end{definition}

\begin{definition}[Monotonicity]
A function $f : \mathbb{R}^n \to \mathbb{R}^m$ is said to be \emph{monotone} if for any $x, x' \in \mathbb{R}^n$,
\[
x \le x' \;\; \Rightarrow \;\; f(x) \le f(x').
\]

\end{definition}

\begin{lemma}[Closure of Monotonicity Under Composition]
\label{Lemma1}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^m \to \mathbb{R}^k$ be monotone functions. Then the composition
\[
h = g \circ f : \mathbb{R}^n \to \mathbb{R}^k
\]
is also monotone.
\end{lemma}

\begin{proof}
For any $x, x' \in \mathbb{R}^n$ such that $x \le x'$, monotonicity of $f$ implies $f(x) \le f(x')$. Applying monotonicity of $g$ yields
\[
g(f(x)) \le g(f(x')),
\]
which establishes the claim.
\end{proof}


\paragraph{Feed-Forward Neural Networks.}
We consider a feed-forward neural network $N : \mathbb{R}^{n_0} \to \mathbb{R}^{n_k}$ with $k$ hidden layers. For an input $u \in \mathbb{R}^{n_0}$, the network computes its output recursively as
\begin{align*}
y_0 &= u, \\
y_{i+1} &= \sigma(W_i y_i + b_i), \quad \text{for } i = 0, 1, \ldots, k-1, \\
N(u) &= W_k y_k + b_k,
\end{align*}
where $W_i \in \mathbb{R}^{n_{i+1} \times n_i}$ and $b_i \in \mathbb{R}^{n_{i+1}}$ denote the weight matrix and bias vector of layer $i$, respectively, and $\sigma:\mathbb{R} \to \mathbb{R}$ is an activation function applied elementwise.

\begin{proposition}[Sufficient Condition for Monotonicity of FFNs] Consider a FFN $N$.
\label{Prop1}
if the activation function $\sigma$ is elementwise non-decreasing and that all weight matrices satisfy
\[
W_i \succeq 0 \quad \text{for all } i = 0, 1, \ldots, k,
\]
where $\succeq 0$ denotes elementwise non-negativity. Then the network $N$ is a monotone function.
\end{proposition}

\paragraph{Proof.} The statement follows by applying Lemma~\ref{Lemma1} to each layer and invoking closure of monotonicity under composition.



\begin{remark}
Neural networks satisfying the conditions of Proposition~\ref{Prop1} are commonly referred to as \emph{monotone neural networks}. Such networks are universal approximators of continuous monotone functions on compact domains~\cite{Daniels2010Monotone}.
\end{remark}


% \begin{definition}[Monotone Feed-Forward Network]
% Let $f : \mathbb{R}^{d_{\mathrm{in}}} \to \mathbb{R}^{d_{\mathrm{out}}}$ be a feed-forward network with $n$ layers of the form
% \[
% f(x_0) = \sum_{i=i}^n W_i \, \sigma(x_{i-1}) + b_i,
% \]
% where $\sigma(\cdot)$ is a monotone activation function, and $x_{i-1}$ is the input to the current layer. The FFN $f$ is monotone if
% \[
% W_i \succeq 0 \quad i\in\{1,\ldots,n\}.
% \]
% where $\succeq 0$ denotes elementwise non-negativity.
% \end{definition}

\paragraph{Monotone Transformers.}
We study Transformer models in which monotonicity is selectively enforced within feed-forward sublayers. We begin by formalizing the notion of a monotone Transformer and then describe how the monotonicity constraints are implemented in practice.

\begin{definition}[Monotone Transformer]
A Transformer model $\mathcal{M}_\theta$ is said to be \emph{monotone with respect to its feed-forward networks (FFNs)} if every feed-forward network within each Transformer block satisfies the monotonicity conditions of Definition~3. All other components of the architecture, including attention mechanisms, residual connections, and normalization layers, are left unconstrained.
\end{definition}

To enforce monotonicity within FFN sublayers, we adopt a differentiable reparameterization of the corresponding weight matrices. Let $W \in \mathbb{R}^{d_{\mathrm{out}} \times d_{\mathrm{in}}}$ denote a weight matrix subject to an elementwise non-negativity constraint.

\begin{definition}[Monotone Parameterization]
We define a monotone parameterization of $W$ by
\[
W = \phi(V), \qquad \phi(v) := \log\bigl(1 + \exp(v)\bigr),
\]
where $V \in \mathbb{R}^{d_{\mathrm{out}} \times d_{\mathrm{in}}}$ is an unconstrained parameter matrix and $\phi(\cdot)$ is applied elementwise. This parameterization guarantees $W \succeq 0$ for all $V$ without requiring explicit projection or constrained optimization.
\end{definition}

\paragraph{Enforcing Non-Negativity Constraints.}
A naive approach to enforcing non-negativity, such as projecting weights onto the positive orthant after each gradient update, leads to degenerate training behavior in modern deep learning frameworks and effectively prevents meaningful learning. Instead, we adopt a smooth reparameterization in which constrained weights are expressed as elementwise softplus transformations of unconstrained parameters. This approach ensures that monotonicity constraints are satisfied throughout training while remaining compatible with standard gradient-based optimization.


When applying monotonicity constraints to pretrained models, it is important to preserve the information encoded in the original parameters.

\begin{definition}[Pretrained Weight Initialization]
Let $W_{\mathrm{pre}}$ denote a pretrained weight matrix. The corresponding unconstrained parameters $V$ are initialized as
\[
V_{\mathrm{init}} = \phi^{-1}\bigl(|W_{\mathrm{pre}}| + \epsilon\bigr),
\]
where $\epsilon > 0$ is a small constant introduced for numerical stability. This initialization preserves the scale of the pretrained weights while ensuring compatibility with the imposed monotonicity constraints.
\end{definition}




\section{Proposed Method}
We investigate whether enforcing monotonicity constraints in sequence-to-sequence models can improve adversarial robustness while preserving standard task performance. Our approach instantiates the theoretical framework introduced in Section~\ref{Prelim} within modern Transformer-based summarization models, enabling a controlled empirical study of monotonicity as an architectural inductive bias.

\subsection{Monotone Sequence-to-Sequence Models}
Let $\mathcal{M}$ denote a sequence-to-sequence model mapping an input text to an output summary. We enforce monotonicity by constraining the feed-forward network (FFN) sublayers within each Transformer block to have non-negative weights, while leaving all other architectural components unconstrained. This selective enforcement induces monotone behavior in the model’s nonlinear transformations without restricting attention mechanisms or global information flow.

\paragraph{Implementation Scope.}
In all experiments, monotonicity constraints are applied exclusively to the FFN projection matrices (\texttt{wi}, \texttt{wo}) in the T5 architecture~\cite{raffel2020exploring}. Attention layers, residual connections, and layer normalization components remain unconstrained. This design preserves the expressive capacity and optimization stability of the original Transformer while introducing sufficient structure to study robustness and verification.

We base our experiments on \texttt{T5-small}, which comprises 6 encoder layers and 6 decoder layers, with 512-dimensional hidden representations, 8 attention heads per layer, and feed-forward sublayers with 2048-dimensional intermediate activations. The model contains approximately 60M parameters in total, of which roughly 24M correspond to the FFN projection matrices subject to monotonicity constraints, accounting for about 40\% of all parameters. These constraints are enforced across all FFN sublayers in the 12 Transformer blocks, resulting in 24 constrained weight matrices. All remaining components (including attention projections, layer normalization, token embeddings, and the final output projection) are left unconstrained.

\subsection{Training and Evaluation Protocol}
Both baseline and monotone models are fine-tuned from the same pretrained \texttt{T5-small} checkpoint under identical optimization settings. We use the AdamW optimizer with learning rate $5 \times 10^{-5}$ and weight decay $0.01$, a batch size of 4, and gradient clipping with norm 1.0. To accommodate the constrained parameterization, the monotone model employs an extended warmup phase of 15\% of training steps, compared to 10\% for the baseline. Both models are trained for 7 epochs and receive the same total training budget.

Training is conducted on the DialogSum, HighlightSum, and arXiv abstract datasets, comprising approximately 150K examples in total. Evaluation is performed on three held-out benchmarks: CNN/DailyMail (11{,}490 test examples), XSUM (11{,}334 test examples), and SAMSum (819 test examples). CNN/DailyMail is excluded entirely from training to assess out-of-distribution generalization.

At inference time, decoding is performed using beam search with 4 beams and a length penalty of 1.2, enforcing a minimum generation length of 10 tokens and a maximum length of 80 tokens. We additionally apply no-repeat $n$-gram blocking with $n=3$ to discourage degenerate repetitions. All decoding parameters are fixed across models and evaluation sets. ROUGE-1, ROUGE-2, and ROUGE-L scores are computed using the \texttt{rouge-score} library with Porter stemming. We report bootstrap-based 95\% confidence intervals using 1{,}000 resamples and assess statistical significance via paired $t$-tests with Bonferroni correction for multiple comparisons. Effect sizes are reported using Cohen’s $d$.

To evaluate robustness to training stochasticity, all experiments are repeated using five random seeds (42, 1337, 2024, 8888, and 12345), controlling randomness across Python, NumPy, and PyTorch. We report the mean and standard deviation across seeds for all metrics. Statistical significance across training runs is assessed using paired $t$-tests on seed-level means, complemented by per-example paired $t$-tests within each seed. This dual evaluation captures both test-time variability and variance induced by random initialization and data ordering. All experiments are conducted using deterministic algorithms where available, on a single NVIDIA A100 GPU with 40GB memory, using PyTorch~2.0.1 and Transformers~4.30.2.

\subsection{Adversarial Robustness Evaluation}
We evaluate robustness under two complementary classes of adversarial attacks designed to probe different failure modes of sequence-to-sequence models.

\paragraph{Universal Adversarial Triggers.}
Universal adversarial triggers (UATs) consist of short, input-agnostic token sequences that, when prepended to an input, maximize the model’s loss. We optimize triggers using coordinate-wise search with three random restarts and 50 iterations. Candidate tokens are drawn from a vocabulary biased toward punctuation, special characters, and high-frequency words that empirically induce greater disruption. Triggers are optimized on a held-out validation split and evaluated on a disjoint test set. To assess transferability, we additionally construct a transfer matrix measuring cross-model vulnerability to learned triggers.

\paragraph{HotFlip Attacks.}
HotFlip attacks identify vulnerable input positions by computing gradients of the loss with respect to token embeddings and replacing selected tokens to maximize loss increase. We allow up to five token replacements per example, selecting replacements based on the dot product between gradient directions and candidate embeddings.

\paragraph{Robustness Metrics.}
Robustness is quantified using ROUGE score degradation under attack, attack success rate (defined as a ROUGE-L degradation exceeding 10\%), and statistical significance assessed via independent $t$-tests. We additionally report mean loss increase and analyze cross-model transferability of adversarial triggers.


\paragraph{Motivation from Monotone Dynamical Systems.}
Classical results in the theory of monotone dynamical systems show that, under mild boundedness assumptions, trajectories of strongly monotone flows converge to equilibrium points almost surely; in particular, derivatives along these trajectories tend to zero as the system approaches an invariant set \cite{hirsch1985systems}. While a feed-forward network is not a time-indexed dynamical system in this sense, this perspective suggests that monotonicity can limit the set of active directions along which outputs and gradients vary. In this work, we leverage this intuition to inform a local analysis of gradient behavior under monotone perturbations, focusing on how saturation and non-negative weights interact to diminish exploitable gradient directions.

\section{Gradient Attenuation in Monotone Networks}
\label{sec:gradient-attenuation}

We provide a mechanistic explanation for the empirical
robustness gains observed in monotone sequence-to-sequence models.
Rather than claiming global robustness guarantees, we analyze how
monotonicity constraints alter the local gradient structure of feed-forward
sublayers in a way that can reduce the effectiveness of gradient-based
adversarial attacks such as HotFlip.

\subsection{Preliminaries}

Consider a feed-forward sublayer $f : \mathbb{R}^d \to \mathbb{R}^d$ of the form
\begin{equation}
f(x) = W_2 \sigma(W_1 x + b_1) + b_2,
\end{equation}
where $W_1, W_2 \in \mathbb{R}^{d \times d}$ have non-negative entries and
$\sigma$ is an elementwise, monotone non-decreasing activation function
(e.g., ReLU or softplus). Under these conditions, $f$ is monotone with respect
to the standard partial order on $\mathbb{R}^d$.

We study the Jacobian structure of $f$ and its implications for gradients
of a scalar loss function $L(f(x))$ with respect to the input $x$.

\subsection{Jacobian Structure of Monotone Sublayers}

\begin{lemma}[Non-negativity of the Jacobian]
\label{lem:jacobian-nonneg}
If $W_1, W_2 \succeq 0$ elementwise and $\sigma$ is monotone non-decreasing,
then the Jacobian
\[
J_f(x) = \nabla_x f(x)
\]
has non-negative entries for all $x \in \mathbb{R}^d$.
\end{lemma}

\begin{proof}
The Jacobian can be written as
\[
J_f(x) = W_2 \operatorname{diag}(\sigma'(W_1 x + b_1)) W_1.
\]
Since $W_1, W_2 \succeq 0$ and $\sigma'(z) \ge 0$ for all $z$, all entries of
$J_f(x)$ are non-negative.
\end{proof}

This structure implies that gradients propagated through monotone sublayers
lack sign cancellation effects that are common in unconstrained architectures.

\subsection{Saturation-Induced Gradient Attenuation}

We now examine how saturation in monotone networks affects gradient magnitude.

\begin{assumption}[Saturating Activations]
\label{ass:saturation}
The activation function $\sigma$ has bounded derivative and admits
saturated regimes, i.e.,
\[
\sigma'(z) \to 0 \quad \text{as } z \to +\infty.
\]
\end{assumption}

\begin{lemma}[Gradient Attenuation under Saturation]
\label{lem:gradient-attenuation}
Let $f$ satisfy the conditions above and suppose that, for a sequence
$\{x_k\}$, a non-empty subset of hidden pre-activations
$(W_1 x_k + b_1)_j$ diverges to $+\infty$. Then the operator norm of the
Jacobian $J_f(x_k)$ is non-increasing along directions corresponding to
those saturated units, and the contribution of those units to
$\nabla_x L(f(x_k))$ vanishes asymptotically.
\end{lemma}

\begin{proof}
For each saturated hidden unit $j$, we have
$\sigma'((W_1 x_k + b_1)_j) \to 0$.
Since the Jacobian depends multiplicatively on $\sigma'$, the corresponding
terms in $J_f(x_k)$ are suppressed. While the full Jacobian need not lose rank,
its effective contribution along saturated directions diminishes.
\end{proof}

This result shows that monotonicity combined with saturation can locally
attenuate gradient magnitude without eliminating all gradient signal.

\subsection{Persistence of Saturation Effects}

The following observation captures a one-sided stability property of saturated
units under monotone perturbations.

\begin{lemma}[Persistence of Saturated Units]
\label{lem:persistence}
Let $x \preceq x'$ and suppose that a hidden unit is saturated at $x'$, i.e.,
$\sigma'((W_1 x' + b_1)_j) = 0$. Then for any monotone perturbation
$\delta \succeq 0$, that unit remains saturated at $x' + \delta$.
\end{lemma}

\begin{proof}
Since $W_1 \succeq 0$, increasing the input cannot decrease pre-activations.
Thus $(W_1(x' + \delta) + b_1)_j \ge (W_1 x' + b_1)_j$, and saturation persists.
\end{proof}

Importantly, this lemma applies to individual hidden units rather than to the
entire input gradient, and does not preclude other units from becoming active.

\subsection{Implications for Gradient-Based Attacks}

Gradient-based adversarial attacks such as HotFlip rely on persistent,
high-magnitude gradients with respect to token embeddings. The preceding
analysis suggests that monotonicity constraints can reduce the availability
of such gradients by driving subsets of hidden units into saturated regimes
from which they do not recover under monotone increases in internal
representations.

\paragraph{Interpretation.}
While monotone feed-forward networks are not dynamical systems in the formal
sense, this behavior is reminiscent of classical results in monotone
dynamical systems, where cooperative interactions restrict the set of
effective directions along which trajectories evolve. Here, the analogy
serves as intuition rather than a formal guarantee.

\paragraph{Scope.}
We do not claim that monotonicity prevents all adversarial attacks, nor that
gradients vanish globally. Instead, our analysis characterizes a local
attenuation effect that weakens gradient-based optimization strategies,
providing a plausible mechanism for the empirical robustness improvements
observed in Section~\ref{sec:results}.


\section{Results}
\label{sec:results}
We now present empirical results evaluating the impact of monotonicity constraints on both standard task performance and adversarial robustness. Our analysis proceeds in two stages: we first examine optimization behavior and summarization quality to assess whether monotonicity degrades baseline performance, and then evaluate robustness under targeted adversarial attacks. Together, these experiments test whether monotonicity can serve as a practical inductive bias that improves robustness without sacrificing model utility.

\subsection{Training Dynamics and Summarization Quality}

We begin by analyzing the effect of monotonicity constraints on optimization behavior and summarization quality. Table~\ref{tab:training} summarizes the training dynamics of the baseline and monotonic models from our completed experiment (seed 42). The monotonic model starts with a substantially higher initial loss (4.97 vs.\ 2.94), reflecting the impact of the softplus reparameterization and the transformation of pretrained weights into a non-negative space. This initialization disrupts the original weight geometry, requiring the model to re-establish effective internal representations during fine-tuning.

Despite this unfavorable initialization, the monotonic model converges reliably over the course of training. Validation loss decreases from 2.92 at epoch 1 to 2.54 at epoch 7, indicating stable optimization under the imposed constraints. The model exhibits rapid initial recovery, reducing training loss by 1.92 points in the first two epochs (from 4.97 to 3.05), followed by slower asymptotic convergence with diminishing returns in later epochs. A persistent optimization gap of 0.32 points remains at convergence (2.54 vs.\ 2.21 for baseline), consistent with the reduced expressivity of the constrained parameter space. This gap reflects a deliberate and quantifiable trade-off: the baseline model achieves slightly lower validation loss through unrestricted FFN weights, while the monotonic model maintains competitive performance while enforcing structural regularity that, as we show below, translates to substantially improved adversarial robustness.

\begin{table}[t]
\caption{Training dynamics comparison (seed = 42).}
\label{tab:training}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
Model & Initial Loss & Final Loss & $\Delta$ Loss & Epochs \\
\midrule
Baseline  & 2.94 & 2.21 & $-$0.73 & 6 \\
Monotonic & 4.97 & 2.54 & $-$2.43 & 7 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Table~\ref{tab:rouge} reports summarization performance on CNN/DailyMail (n=200, seed 42). The monotonic model achieves a ROUGE-L score of 24.2 [22.7, 25.6], compared to 25.0 [23.6, 26.4] for the baseline, corresponding to a relative decrease of 3.2 percent. ROUGE-1 exhibits a similar trend (29.6 [28.0, 31.1] vs 30.9 [29.4, 32.4], a 4.2 percent decrease). While these results indicate a modest reduction in task performance, the monotonic model remains competitive, with confidence intervals showing substantial overlap across all metrics.

To assess generalization, we additionally evaluate on XSUM (n=200, seed 42). Table~\ref{tab:xsum} shows consistent results: the monotonic model achieves ROUGE-L of 12.9 [12.3, 13.6] compared to 13.3 [12.6, 13.9] for baseline, a similar 3.0 percent relative gap. The consistency of the performance trade-off across distinct summarization datasets (news articles in CNN/DM, single-sentence summaries in XSUM) suggests that the cost of monotonicity constraints is stable and predictable across different data distributions.

\begin{table}[t]
\caption{Summarization quality on CNN/DailyMail (n = 200, seed = 42). Values are means with 95\% bootstrap confidence intervals.}
\label{tab:rouge}
\vskip 0.1in
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
Model & ROUGE-1 & ROUGE-2 & ROUGE-L \\
\midrule
Standard  & 32.6 [30.9, 34.2] & 11.9 [10.5, 13.4] & 26.6 [25.0, 28.1] \\
Baseline  & 30.9 [29.4, 32.4] & 11.5 [10.1, 12.8] & 25.0 [23.6, 26.4] \\
Monotonic & 29.6 [28.0, 31.1] & 10.6 [9.2, 11.9] & 24.2 [22.7, 25.6] \\
\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Summarization quality on XSUM (n = 200, seed = 42). Values are means with 95\% bootstrap confidence intervals.}
\label{tab:xsum}
\vskip 0.1in
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
Model & ROUGE-1 & ROUGE-2 & ROUGE-L \\
\midrule
Standard  & 19.9 [18.8, 21.1] & 2.9 [2.3, 3.4] & 13.3 [12.6, 14.1] \\
Baseline  & 20.0 [19.0, 21.0] & 3.2 [2.7, 3.7] & 13.3 [12.6, 13.9] \\
Monotonic & 18.9 [18.0, 19.7] & 2.8 [2.3, 3.2] & 12.9 [12.3, 13.6] \\
\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table}

The training dynamics exhibit a characteristic pattern of monotone-constrained optimization. During the first two epochs, the monotonic model rapidly recovers much of the performance gap introduced by constrained initialization, reducing training loss by 1.92 points (from 4.97 to 3.05). Subsequent epochs yield diminishing returns, with only 0.36 points of additional improvement from epochs 3 through 7. This behavior highlights the importance of the extended warmup phase (15 percent vs 10 percent for baseline), which stabilizes gradient estimates and enables effective learning despite the constrained parameter space.

Analysis of generation behavior reveals that both fine-tuned models produce longer outputs than the pretrained T5 model (mean lengths 74 to 76 tokens vs 57 tokens), reflecting adaptation to the distributional properties of the summarization training data. The monotonic model exhibits slightly higher length variance (std 11.3 tokens) compared to the standard model (std 12.0 tokens), though this difference is minimal. Importantly, the brevity penalty for the monotonic model (1.00) indicates that output length is well-calibrated to reference summary length, suggesting the constraints do not introduce systematic length bias.

\subsection{Adversarial Robustness}

\paragraph{HotFlip Attacks.}
We evaluate gradient-based robustness using HotFlip attacks, which identify vulnerable input positions via gradients with respect to token embeddings and perform targeted token substitutions to maximize loss. For each example, the attack computes gradients of the cross-entropy loss, selects the five positions with largest gradient magnitudes, and replaces tokens at those positions with vocabulary items that maximize the dot product between the gradient and the replacement embedding.

Robustness is quantified using three complementary metrics: average degradation, defined as the mean relative increase in loss under attack; attack success rate, defined as the fraction of examples for which degradation exceeds 10\%; and mean loss increase, which reports the average absolute increase in loss.

Table~\ref{tab:hotflip} reports results on 100 test examples from seed 42. To verify consistency of attack robustness, we repeated the HotFlip evaluation using the same trained checkpoints with two additional random seeds (1337, 2024) for attack sampling variability. Results were identical across all three seeds, confirming that the observed robustness improvements are stable. The monotonic model demonstrates substantially improved robustness relative to both baselines. Under HotFlip perturbations, the monotonic model incurs an average degradation of 5.2 percent, compared to 16.2 percent for the fine-tuned baseline and 21.3 percent for the standard pretrained model. Attack success rates follow a similar pattern, with only 19 percent of attacks succeeding against the monotonic model, compared to 63 percent for the baseline and 69 percent for the standard model. This represents a 69.8 percent relative reduction in attack success rate (63 percent to 19 percent) and a 67.9 percent reduction in average degradation (16.2 percent to 5.2 percent). Statistical testing using paired t-tests confirms that the difference between the baseline and monotonic models is highly significant ($t = 6.17$, $p = 3.8 \times 10^{-9}$), and the difference between standard and monotonic models is even more pronounced ($t = 7.45$, $p = 2.8 \times 10^{-12}$).

\begin{table}[t]
\caption{HotFlip attack results on CNN/DailyMail (n = 100, seed = 42).}
\label{tab:hotflip}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Model & Avg. Deg. & Success Rate & $\Delta$Loss \\
\midrule
Standard  & 21.3\% & 69\% & +0.42 \\
Baseline  & 16.2\% & 63\% & +0.35 \\
Monotonic &  5.2\% & 19\% & +0.14 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Overall, enforcing monotonicity in the FFN sublayers yields a 69.8 percent relative reduction in attack success rate (from 63 percent to 19 percent) and a 67.8 percent reduction in average degradation (from 16.1 percent to 5.2 percent), without requiring adversarial training, modified training objectives, or architectural changes beyond the FFN constraints. The effect size is large (Cohen's d > 0.8) and the statistical significance is robust ($p < 10^{-9}$). These results demonstrate that structural constraints alone—applied selectively to feed-forward sublayers—can substantially improve robustness to gradient-based adversarial manipulation, providing evidence for monotonicity as a practical architectural inductive bias for building more robust language models.

\paragraph{Universal Adversarial Trigger Attacks.}
We additionally evaluate robustness to universal adversarial triggers (UATs), which are short, input-agnostic token sequences optimized to maximize model loss when prepended to any input. Triggers are learned via coordinate ascent with 3 restarts and 50 iterations over 80 training examples, then evaluated on 120 disjoint test examples. We measure robustness via NLL increase under trigger prepending.

UAT attacks prove minimally effective across all models, with NLL increases below 1 percent. The monotonic model exhibits the smallest degradation (0.73 percent NLL increase), compared to 0.89 percent for baseline and 0.97 percent for standard T5. While this ordering is consistent with the monotonic model being more robust, the effect magnitudes are too small to draw strong conclusions. The weak effectiveness of universal triggers, in contrast to the substantial impact of input-specific HotFlip attacks, suggests that input-agnostic perturbations are fundamentally limited for attacking sequence-to-sequence models, and that monotonicity's robustness benefits are most pronounced against adaptive, gradient-based attacks that exploit model-specific vulnerabilities.

\section{Discussion}

Our results indicate that monotonicity can serve as a practical and effective structural inductive bias for improving the robustness of sequence-to-sequence models. By enforcing non-negativity constraints on feed-forward sublayers, we observe substantial reductions in vulnerability to gradient-based adversarial attacks, while incurring only modest degradation in standard summarization performance. Importantly, these robustness gains arise without adversarial training, prompt filtering, or modifications to attention mechanisms, suggesting that they are a direct consequence of architectural structure rather than heuristic defenses.

A key insight from our study is the outsized role played by feed-forward networks in shaping model sensitivity. FFN sublayers account for a large fraction of Transformer parameters and are responsible for nonlinear amplification of internal representations. Enforcing monotonicity in these components restricts the model’s ability to arbitrarily invert or amplify features, thereby limiting how small, adversarially chosen perturbations can propagate through the network. This targeted constraint provides a principled way to regularize the most nonlinear parts of the architecture without disrupting global information flow through attention.

One plausible mechanism underlying the observed robustness gains lies in the altered gradient structure induced by monotonicity. Empirically, we observe that when the gradient of the loss with respect to an internal representation becomes small or effectively zero at a given point, it often remains suppressed for all larger values along the corresponding monotone directions. Intuitively, once a monotone transformation saturates, increasing its input cannot reintroduce sensitivity. This behavior is consistent with the structure of the constrained feed-forward sublayers and may partially explain the reduced effectiveness of gradient-based attacks such as HotFlip, which rely on localized gradient signals to identify vulnerable input positions.

The observed trade-off between expressivity and robustness is both expected and informative. While monotonicity constraints reduce the space of representable functions, the resulting decrease in summarization quality is relatively small compared to the magnitude of robustness gains. This mirrors similar trade-offs observed in other settings, such as Lipschitz-constrained or contractive models, where modest sacrifices in average performance can yield significant improvements in stability and reliability. For safety-critical applications, such trade-offs may be well justified.

Moreover, monotonicity differs from conventional regularization techniques. Whereas standard regularizers penalize parameter magnitude or complexity, monotonicity imposes directional constraints on how information flows through the network. This structural restriction yields models that are not merely smoother, but more predictable in how their outputs respond to internal perturbations. As a result, monotonicity offers a path toward analyzability that is difficult to achieve through loss-based penalties alone.

Our work has several limitations. Experiments are conducted on \texttt{T5-small} and evaluation subsets, and we do not yet provide an end-to-end verification pipeline for deployed systems. Nevertheless, the consistency of the observed robustness gains across attacks and seeds suggests that monotonicity is a scalable inductive bias rather than a model-specific artifact. Extending these ideas to larger architectures, integrating formal verification procedures, and exploring interactions with complementary alignment and robustness techniques represent promising directions for future work.

% Overall, this work provides evidence that structural constraints long used in safety-critical domains can be adapted to modern language models without prohibitive loss in capability. By shaping model behavior at the architectural level, monotonicity offers a principled mechanism for improving robustness and predictability, pointing toward language models whose safety properties are influenced not only by data and objectives, but by design.

\section{Conclusion}
We investigated monotonicity as a structural inductive bias for improving the robustness of sequence-to-sequence language models. By enforcing non-negativity constraints on feed-forward sublayers in Transformer architectures, we observed substantial reductions in vulnerability to gradient-based adversarial attacks, with only modest impact on summarization quality. These results suggest that simple architectural constraints can meaningfully influence robustness without altering attention mechanisms or relying on adversarial training.

An important direction for future work is to develop theoretical guarantees that characterize when and why monotonicity yields improved robustness in high-dimensional language models. Scaling this approach to larger model variants and broader tasks is another promising avenue, and may help clarify the role of structural constraints in building more reliable language models.
\section*{Impact Statement}

This work explores architectural constraints that improve the robustness of language models to adversarial manipulation. By studying monotonicity as a structural inductive bias, our approach contributes to the development of language models whose behavior is more predictable and analyzable, which is particularly relevant for safety-critical and decision-support applications.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
