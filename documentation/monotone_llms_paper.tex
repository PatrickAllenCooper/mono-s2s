%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables


\newcommand{\N}{\mathbb{N}}
\newcommand{\MSE}{\mathsf{MSE}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sys}{\mathfrak{S}}
\newcommand{\Xx}{\mathcal{X}}
\newcommand{\Yy}{\mathcal{Y}}
\newcommand{\seq}[1]{\langle #1 \rangle}
\newcommand{\Relu}{\mathsf{ReLU}}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% For highlighting additions in blue
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Monotonicity as an Architectural Bias for Robust Language Models}

\begin{document}

\twocolumn[
\icmltitle{Monotonicity as an Architectural Bias for Robust Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in output.

We investigate \emph{monotonicity} as an architectural inductive bias for improving the robustness of Transformer-based language models. At a high level, monotonicity constrains a model so that its outputs change in a predictable direction with respect to changes in its inputs. Such constraints have long been used in domains such as control and safety-critical systems, where they simplify reasoning about system behavior and improve robustness. However, they have traditionally been viewed as impractical for expressive neural models due to perceived losses in performance.

We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers, we obtain monotone language models that preserve the performance of their pretrained counterparts. Despite their constrained structure, these models exhibit substantially improved robustness to a range of adversarial and jailbreak attacks.

Our results demonstrate that strong architectural constraints, long assumed to limit the capacity of modern language models, can be applied at scale without sacrificing accuracy. Monotonicity thus emerges as a practical and principled design choice for building more robust and predictable language models.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language tasks, yet their behavior under adversarial or carefully structured inputs remains brittle. Even models that have undergone extensive alignment and fine-tuning can be induced to produce unsafe or unintended outputs through jailbreak prompts or small, targeted perturbations. This phenomenon is now well documented and points to a deeper underlying issue: modern language models operate in extremely high-dimensional spaces, where even subtle perturbations to the input or internal representations can induce large and difficult-to-predict changes in the output. Recent work has attributed this sensitivity, in part, to the high Lipschitz constants exhibited by such models~\cite{newhouse2025training}.


This fragility has motivated an active line of research on improving the robustness of language models. Existing approaches largely focus on training-time interventions, such as alignment objectives, adversarial data augmentation, or post-hoc filtering mechanisms. While these methods can be effective empirically, they are inherently reactive, addressing vulnerabilities after they are discovered rather than constraining model behavior by design. As language models are increasingly deployed in safety-sensitive settings, this raises an important question: can we endow language models with structural properties that make their behavior more predictable under perturbation?

We investigate \emph{monotonicity} as a structural inductive bias for improving the robustness of language models. At a high level, monotonicity constrains a model so that its outputs change in a predictable direction with respect to changes in its internal representations. This property has a long history in machine learning and related fields, and has been especially influential in control theory, where monotonic systems admit strong guarantees on stability and safety even in very high-dimensional settings. For example, monotonicity has been used to analyze and certify the behavior of large-scale dynamical systems such as power grids, where direct reasoning over all possible system states is intractable.

The appeal of monotonicity in these domains lies in its ability to simplify analysis: global system behavior can often be inferred by examining only extreme or boundary conditions. This perspective naturally suggests viewing a language model as a high-dimensional dynamical system, whose internal representations evolve through successive nonlinear transformations in response to input perturbations. In such systems, imposing monotonic structure can substantially reduce unpredictability and improve robustness.

Despite these advantages, monotonic constraints have traditionally been viewed as incompatible with expressive neural models. Enforcing monotonicity was long believed to significantly limit model capacity or degrade performance, making it impractical for modern architectures such as Transformers. Here, we challenge this assumption by showing that monotonicity can be incorporated into large-scale language models without sacrificing their expressive power.


Our central finding challenges this assumption. We show that monotonicity can be enforced within modern Transformer architectures without sacrificing performance. Rather than constraining the entire model, we apply monotonicity selectively to the feed-forward sublayers that dominate the model’s nonlinear transformations, while leaving attention mechanisms, residual connections, and normalization layers unconstrained. To enable effective optimization under these constraints, we adopt a smooth reparameterization that avoids the degeneracies associated with naive projection-based approaches. This design allows pretrained sequence-to-sequence models to be distilled into monotone counterparts that closely match their original performance on standard summarization benchmarks.

The resulting models exhibit a striking empirical property: despite preserving task accuracy, monotone Transformers are substantially more robust to adversarial and jailbreak attacks. These improvements are not achieved through additional data, modified training objectives, or heuristic defenses, but emerge purely from architectural structure. This makes the observed robustness particularly notable, as it isolates monotonicity as a causal factor rather than a confounding effect of training or evaluation choices.

These experimental findings are important for two reasons. First, they demonstrate that strong architectural constraints (long assumed to be impractical for large language models) can be applied at scale without degrading performance. Second, they suggest that robustness in language models need not rely exclusively on increasingly complex training pipelines, but can instead be supported by principled design choices that shape model behavior by construction. Together, these results position monotonicity as a practical and scalable architectural bias for building more robust and predictable language models.


\noindent\textbf{Contributions.}
Our contributions are summarized as follows:
\begin{itemize}
\item \textbf{Monotone building blocks for LLMs.} We introduce a framework for constructing language models using monotone Transformer components, enforcing structural monotonicity within feed-forward sublayers while leaving attention mechanisms unconstrained.
\item \textbf{Distillation without loss of capability.} We show that pretrained, non-monotone language models can be distilled into monotone counterparts with no noticeable degradation in standard benchmark performance.
\item \textbf{Empirical robustness improvements.} We demonstrate through adversarial evaluation that monotone language models exhibit substantially improved robustness to jailbreak and adversarial attacks, providing evidence that monotonicity serves as an effective inductive bias for improving model stability.
\end{itemize}

Together, these results suggest that safety and performance need not be competing objectives. By enforcing monotonicity at the level of model building blocks, we obtain language models that are empirically more robust and predictable, while retaining the capabilities of their unconstrained counterparts. Monotonicity thus emerges as a practical and scalable design principle for improving the trustworthiness of modern LLMs.

\section{Related Work}

% Our work situates itself at the intersection of robustness in language models, architectural constraints in neural networks, and structural approaches to improving model predictability and safety. We review relevant lines of research on adversarial vulnerabilities of LLMs, structural and monotonic network design, model verification and certification, and related defenses.

\paragraph{Vulnerabilities of Language Models.}
Large language models have been shown to be highly susceptible to diverse adversarial manipulations that can bypass safety filters and elicit harmful or unintended outputs. Research in this area has documented both jailbreaks (crafted prompts that circumvent alignment measures) and transferable suffix attacks that generalize across models. For example, universal adversarial suffixes have been demonstrated to induce objectionable content in widely used models including GPT, Bard, and Claude, even when aligned, and can transfer from one model to another using greedy or gradient-based optimization techniques~\citep{Zou2023UniversalTransferableAttacksOnAlignedLLMs}. Studies have further explored the position sensitivity of jailbreak attacks and shown that simple adversarial snippets positioned at specific output locations can dramatically increase attack success rates, revealing nuanced vulnerabilities in model safety mechanisms~\citep{wang2025vulnerability}.

In parallel, interpretable adversarial attack methods such as AutoDAN have been proposed, which generate prompts that evade perplexity-based defenses and achieve high attack success while remaining human-interpretable, underscoring the fragility of current safety filters~\citep{Zhu2023AutoDAN}. Posters at major venues have also highlighted structural vulnerabilities, showing that larger context windows and many-shot jailbreaking strategies can exacerbate susceptibility to adversarial injections, even when models are nominally safety-trained (e.g., “Many-shot jailbreaks”)~\citep{anil2024many}.

Benchmarks have been developed to systematically quantify LLM robustness. For instance, Adversarial GLUE showed that existing models and robust training methods perform poorly under a suite of adversarial text attacks, reinforcing the need for broader robustness evaluation beyond standard test sets~\citep{Wang2021AdvGLUE}. Additional work has examined multilingual jailbreak challenges, revealing that safety mechanisms calibrated for English can fail dramatically in low-resource languages or when malicious instructions are combined with multilingual prompts~\citep{deng2024multilingual}.

Prompt injection attacks, a class of adversarial exploitations wherein innocuous-looking inputs manipulate model behavior by merging instructions and data, have been recognized as a fundamental vulnerability of modern LLMs due to the lack of separation between instruction and context. Comprehensive surveys of LLM security concerns further categorize threats ranging from inference-time attacks to training-time manipulations and model misuse, pointing to broad systemic risks in current LLM deployments~\citep{li2025security}.

\paragraph{Defenses and Robustness Enhancements.}
Efforts to defend against LLM vulnerabilities include techniques such as smoothing defenses designed to mitigate jailbreak attacks with provable performance bounds, as well as detection mechanisms that distinguish adversarial prompts from benign ones by analyzing output distribution differences~\citep{Robey2023SmoothLLM}; alternative methods achieve jailbreak detection with minimal computational overhead~\citep{sayeedi2025jailbreaktracer}.

Scaling behavior in language model robustness has been examined, with recent work showing that larger models are not uniformly more robust without explicit adversarial training, and that offense and defense capabilities scale differently across tasks and threat models~\citep{howe2024effects}. Other defense directions explore improving transferability of jailbreak attacks or adapting prompt sequences to evaluate and harden models against distributional dependencies in adversarial sequences~\citep{wang2025understanding}.

\paragraph{Monotonic Neural Networks and Structural Constraints.}
Monotonicity has long been studied as a structural property in neural networks, motivated by applications requiring interpretability, robustness, or domain-consistent behavior. Early work showed that feed-forward networks with non-negative weights and monotone activation functions are universal approximators of monotone functions, establishing monotonicity as a theoretically sound inductive bias rather than a restrictive limitation~\citep{Sill1998MonotonicNetworks,Daniels2010Monotone}. Subsequent research developed practical constructions of deep monotonic networks, including lattice-based models and partially monotone architectures, enabling scalable learning under monotonicity constraints in tabular and structured domains~\citep{Gupta2016MonotonicLattices,You2017DeepLattice}.

More recent work has explored monotonicity as a means of improving model reliability and certifiability. For example, monotone neural networks have been shown to simplify verification by enabling reasoning over boundary inputs rather than the full input space, particularly for piecewise linear architectures~\citep{Weber2021CertifyingMonotonicNetworks}. However, existing monotonic architectures have primarily been evaluated on low-dimensional or structured prediction tasks, and their applicability to large-scale sequence models and pretrained language models remains largely unexplored.

Beyond monotonicity, architectural constraints more broadly have been investigated as inductive biases for improving robustness and stability in neural networks. Constrained optimization layers and structured network designs have demonstrated that carefully imposed constraints can guide model behavior under perturbations without severely compromising expressive power~\citep{Amos2017OptNet}. These results support the premise that structural constraints, when selectively applied, can improve desirable properties of neural networks while retaining practical performance.


% \paragraph{Verification and Certification.}
% Formal verification aims to provide mathematical assurances about model behavior under specified perturbations. Techniques based on abstract interpretation, convex relaxations, and over-approximation have been applied to neural networks to certify robustness within normed perturbation balls, though these methods often struggle to scale to large models common in LLMs~\citep{Turn0Search1}:contentReference[oaicite:15]{index=15}. Monotonicity, in principle, simplifies some verification tasks by enabling boundary evaluation rather than exhaustive search, but the application of such reasoning to large-scale language models remains an open challenge.

% \paragraph{Distillation and Constrained Training.}
% Model distillation has been used to transfer knowledge and robustness properties between models, but naive distillation alone can be vulnerable to adversarial attacks that exploit weaknesses in the reference model or training procedure~\citep{Carlini2016DefensiveDistillationBreaks}. More recent work explores structured distillation objectives to improve performance under constraints, aligning closely with our use of distillation to preserve performance under architectural constraints.



\section{Preliminaries}
\label{Prelim}
\paragraph{Notations.} We denote the set of real, non-negative real, and positive real numbers by \( \R \), \( \R_{\geq 0} \), and \( \R_{>0} \), respectively. For sets \( A \) and \( B \), we use \( A \setminus B \) for set difference and \( A \times B \) for Cartesian product, and write \( |A| \) for the cardinality of \( A \). 
Given a vector \( v = (v_1,\ldots,v_n)^\top \in \R^n \), its Euclidean norm is \( \|v\| = \sqrt{\sum_{i=1}^n v_i^2} \). The rectified linear unit is defined by \( \Relu(x) := \max(x,0) \), and for vectors \( x, y \in \R^n \), the mean squared error is \( \MSE(x, y) := \frac{1}{n} \sum_{i=1}^n (x_i - y_i)^2 \). We denote the \( i \)th component of a vector \( x \) by \( x_i \), and write \( x \le y \) to indicate element-wise inequality.
A set \( \mathcal{S} \subseteq \R^n \) is \emph{lower closed} if \( x \in \mathcal{S} \) and \( y \le x \) imply \( y \in \mathcal{S} \); it is \emph{upper closed} if \( x \in \mathcal{S} \) and \( y \ge x \) imply \( y \in \mathcal{S} \). A hyper-rectangle is denoted by \( [\underline{x}, \overline{x}] := \{ x \in \R^n \mid \underline{x} \le x \le \overline{x} \} \). Given a set $A\subseteq \R^n$, $x\in \R^n$ is a maximal (resp. minimal) point of $A$, if there does not exists any $y\in A$ such that $x\le y$ (resp. $y\le x$). 

\subsection{Monotonicity-Constrained Sequence-to-Sequence Models}

We study the role of monotonicity constraints in improving the adversarial robustness of sequence-to-sequence language models.

\begin{definition}[Sequence-to-Sequence Model]
A sequence-to-sequence model is a parameterized mapping
\[
\mathcal{M}_\theta : \mathcal{X} \to \mathcal{Y},
\]
where $\mathcal{X}$ denotes the space of input token sequences and $\mathcal{Y}$ denotes the space of output token sequences. The model $\mathcal{M}_\theta$ is typically instantiated as a Transformer architecture composed of attention layers, feed-forward network (FFN) sublayers, residual connections, and normalization operators.
\end{definition}

\begin{definition}[Monotonicity]
A function $f : \mathbb{R}^n \to \mathbb{R}^m$ is said to be \emph{monotone} if for any $x, x' \in \mathbb{R}^n$,
\[
x \le x' \;\; \Rightarrow \;\; f(x) \le f(x').
\]

\end{definition}

\begin{lemma}[Closure of Monotonicity Under Composition]
\label{Lemma1}
Let $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^m \to \mathbb{R}^k$ be monotone functions. Then the composition
\[
h = g \circ f : \mathbb{R}^n \to \mathbb{R}^k
\]
is also monotone.
\end{lemma}

\begin{proof}
For any $x, x' \in \mathbb{R}^n$ such that $x \le x'$, monotonicity of $f$ implies $f(x) \le f(x')$. Applying monotonicity of $g$ yields
\[
g(f(x)) \le g(f(x')),
\]
which establishes the claim.
\end{proof}


\paragraph{Feed-Forward Neural Networks.}
We consider a feed-forward neural network $N : \mathbb{R}^{n_0} \to \mathbb{R}^{n_k}$ with $k$ hidden layers. For an input $u \in \mathbb{R}^{n_0}$, the network computes its output recursively as
\begin{align*}
y_0 &= u, \\
y_{i+1} &= \sigma(W_i y_i + b_i), \quad \text{for } i = 0, 1, \ldots, k-1, \\
N(u) &= W_k y_k + b_k,
\end{align*}
where $W_i \in \mathbb{R}^{n_{i+1} \times n_i}$ and $b_i \in \mathbb{R}^{n_{i+1}}$ denote the weight matrix and bias vector of layer $i$, respectively, and $\sigma:\mathbb{R} \to \mathbb{R}$ is an activation function applied elementwise.

\begin{proposition}[Sufficient Condition for Monotonicity of FFNs] Consider a FFN $N$.
\label{Prop1}
if the activation function $\sigma$ is elementwise non-decreasing and that all weight matrices satisfy
\[
W_i \succeq 0 \quad \text{for all } i = 0, 1, \ldots, k,
\]
where $\succeq 0$ denotes elementwise non-negativity. Then the network $N$ is a monotone function.
\end{proposition}

\paragraph{Proof.} The statement follows by applying Lemma~\ref{Lemma1} to each layer and invoking closure of monotonicity under composition.



\begin{remark}
Neural networks satisfying the conditions of Proposition~\ref{Prop1} are commonly referred to as \emph{monotone neural networks}. Such networks are universal approximators of continuous monotone functions on compact domains~\cite{Daniels2010Monotone}.
\end{remark}


% \begin{definition}[Monotone Feed-Forward Network]
% Let $f : \mathbb{R}^{d_{\mathrm{in}}} \to \mathbb{R}^{d_{\mathrm{out}}}$ be a feed-forward network with $n$ layers of the form
% \[
% f(x_0) = \sum_{i=i}^n W_i \, \sigma(x_{i-1}) + b_i,
% \]
% where $\sigma(\cdot)$ is a monotone activation function, and $x_{i-1}$ is the input to the current layer. The FFN $f$ is monotone if
% \[
% W_i \succeq 0 \quad i\in\{1,\ldots,n\}.
% \]
% where $\succeq 0$ denotes elementwise non-negativity.
% \end{definition}

\paragraph{Monotone Transformers.}
We study Transformer models in which monotonicity is selectively enforced within feed-forward sublayers. We begin by formalizing the notion of a monotone Transformer and then describe how the monotonicity constraints are implemented in practice.

\begin{definition}[Monotone Transformer]
A Transformer model $\mathcal{M}_\theta$ is said to be \emph{monotone with respect to its feed-forward networks (FFNs)} if every feed-forward network within each Transformer block satisfies the monotonicity conditions of Definition~3. All other components of the architecture, including attention mechanisms, residual connections, and normalization layers, are left unconstrained.
\end{definition}

To enforce monotonicity within FFN sublayers, we adopt a differentiable reparameterization of the corresponding weight matrices. Let $W \in \mathbb{R}^{d_{\mathrm{out}} \times d_{\mathrm{in}}}$ denote a weight matrix subject to an elementwise non-negativity constraint.

\begin{definition}[Monotone Parameterization]
We define a monotone parameterization of $W$ by
\[
W = \phi(V), \qquad \phi(v) := \log\bigl(1 + \exp(v)\bigr),
\]
where $V \in \mathbb{R}^{d_{\mathrm{out}} \times d_{\mathrm{in}}}$ is an unconstrained parameter matrix and $\phi(\cdot)$ is applied elementwise. This parameterization guarantees $W \succeq 0$ for all $V$ without requiring explicit projection or constrained optimization.
\end{definition}

\paragraph{Enforcing Non-Negativity Constraints.}
A naive approach to enforcing non-negativity, such as projecting weights onto the positive orthant after each gradient update, leads to degenerate training behavior in modern deep learning frameworks and effectively prevents meaningful learning. Instead, we adopt a smooth reparameterization in which constrained weights are expressed as elementwise softplus transformations of unconstrained parameters. This approach ensures that monotonicity constraints are satisfied throughout training while remaining compatible with standard gradient-based optimization.


When applying monotonicity constraints to pretrained models, it is important to preserve the information encoded in the original parameters.

\begin{definition}[Pretrained Weight Initialization]
Let $W_{\mathrm{pre}}$ denote a pretrained weight matrix. The corresponding unconstrained parameters $V$ are initialized as
\[
V_{\mathrm{init}} = \phi^{-1}\bigl(|W_{\mathrm{pre}}| + \epsilon\bigr),
\]
where $\epsilon > 0$ is a small constant introduced for numerical stability. This initialization preserves the scale of the pretrained weights while ensuring compatibility with the imposed monotonicity constraints.
\end{definition}




\section{Proposed Method}
We investigate whether enforcing monotonicity constraints in sequence-to-sequence models can improve adversarial robustness while preserving standard task performance. Our approach instantiates the theoretical framework introduced in Section~\ref{Prelim} within modern Transformer-based summarization models, enabling a controlled empirical study of monotonicity as an architectural inductive bias.

\subsection{Monotone Sequence-to-Sequence Models}
Let $\mathcal{M}$ denote a sequence-to-sequence model mapping an input text to an output summary. We enforce monotonicity by constraining the feed-forward network (FFN) sublayers within each Transformer block to have non-negative weights, while leaving all other architectural components unconstrained. This selective enforcement induces monotone behavior in the model’s nonlinear transformations without restricting attention mechanisms or global information flow.

\paragraph{Implementation Scope.}
In all experiments, monotonicity constraints are applied exclusively to the FFN projection matrices (\texttt{wi}, \texttt{wo}) in the T5 architecture~\cite{raffel2020exploring}. Attention layers, residual connections, and layer normalization components remain unconstrained. This design preserves the expressive capacity and optimization stability of the original Transformer while introducing sufficient structure to study robustness and verification.

We base our experiments on \texttt{T5-small}, which comprises 6 encoder layers and 6 decoder layers, with 512-dimensional hidden representations, 8 attention heads per layer, and feed-forward sublayers with 2048-dimensional intermediate activations. The model contains approximately 60M parameters in total, of which roughly 24M correspond to the FFN projection matrices subject to monotonicity constraints, accounting for about 40\% of all parameters. These constraints are enforced across all FFN sublayers in the 12 Transformer blocks, resulting in 24 constrained weight matrices. All remaining components (including attention projections, layer normalization, token embeddings, and the final output projection) are left unconstrained.

\subsection{Training and Evaluation Protocol}
Both baseline and monotone models are fine-tuned from the same pretrained \texttt{T5-small} checkpoint under identical optimization settings. We use the AdamW optimizer with learning rate $5 \times 10^{-5}$ and weight decay $0.01$, a batch size of 4, and gradient clipping with norm 1.0. To accommodate the constrained parameterization, the monotone model employs an extended warmup phase of 15\% of training steps, compared to 10\% for the baseline. Both models are trained for 7 epochs and receive the same total training budget.

Training is conducted on the DialogSum, HighlightSum, and arXiv abstract datasets, comprising approximately 150K examples in total. Evaluation is performed on three held-out benchmarks: CNN/DailyMail (11{,}490 test examples), XSUM (11{,}334 test examples), and SAMSum (819 test examples). CNN/DailyMail is excluded entirely from training to assess out-of-distribution generalization.

At inference time, decoding is performed using beam search with 4 beams and a length penalty of 1.2, enforcing a minimum generation length of 10 tokens and a maximum length of 80 tokens. We additionally apply no-repeat $n$-gram blocking with $n=3$ to discourage degenerate repetitions. All decoding parameters are fixed across models and evaluation sets. ROUGE-1, ROUGE-2, and ROUGE-L scores are computed using the \texttt{rouge-score} library with Porter stemming. We report bootstrap-based 95\% confidence intervals using 1{,}000 resamples and assess statistical significance via paired $t$-tests with Bonferroni correction for multiple comparisons. Effect sizes are reported using Cohen’s $d$.

To evaluate robustness to training stochasticity, all experiments are repeated using five random seeds (42, 1337, 2024, 8888, and 12345), controlling randomness across Python, NumPy, and PyTorch. We report the mean and standard deviation across seeds for all metrics. Statistical significance across training runs is assessed using paired $t$-tests on seed-level means, complemented by per-example paired $t$-tests within each seed. This dual evaluation captures both test-time variability and variance induced by random initialization and data ordering. All experiments are conducted using deterministic algorithms where available, on a single NVIDIA A100 GPU with 40GB memory, using PyTorch~2.0.1 and Transformers~4.30.2.

\subsection{Adversarial Robustness Evaluation}
We evaluate robustness under two complementary classes of adversarial attacks designed to probe different failure modes of sequence-to-sequence models.

\paragraph{Universal Adversarial Triggers.}
Universal adversarial triggers (UATs) consist of short, input-agnostic token sequences that, when prepended to an input, maximize the model’s loss. We optimize triggers using coordinate-wise search with three random restarts and 50 iterations. Candidate tokens are drawn from a vocabulary biased toward punctuation, special characters, and high-frequency words that empirically induce greater disruption. Triggers are optimized on a held-out validation split and evaluated on a disjoint test set. To assess transferability, we additionally construct a transfer matrix measuring cross-model vulnerability to learned triggers.

\paragraph{HotFlip Attacks.}
HotFlip attacks identify vulnerable input positions by computing gradients of the loss with respect to token embeddings and replacing selected tokens to maximize loss increase. We allow up to five token replacements per example, selecting replacements based on the dot product between gradient directions and candidate embeddings.

\paragraph{Robustness Metrics.}
Robustness is quantified using ROUGE score degradation under attack, attack success rate (defined as a ROUGE-L degradation exceeding 10\%), and statistical significance assessed via independent $t$-tests. We additionally report mean loss increase and analyze cross-model transferability of adversarial triggers.


\paragraph{Motivation from Monotone Dynamical Systems.}
Classical results in the theory of monotone dynamical systems show that, under mild boundedness assumptions, trajectories of strongly monotone flows converge to equilibrium points almost surely; in particular, derivatives along these trajectories tend to zero as the system approaches an invariant set \cite{hirsch1985systems}. While a feed-forward network is not a time-indexed dynamical system in this sense, this perspective suggests that monotonicity can limit the set of active directions along which outputs and gradients vary. In this work, we leverage this intuition to inform a local analysis of gradient behavior under monotone perturbations, focusing on how saturation and non-negative weights interact to diminish exploitable gradient directions.

\section{Gradient Attenuation in Monotone Networks}
\label{sec:gradient-attenuation}

We provide a mechanistic explanation for the empirical
robustness gains observed in monotone sequence-to-sequence models.
Rather than claiming global robustness guarantees, we analyze how
monotonicity constraints alter the local gradient structure of feed-forward
sublayers in a way that can reduce the effectiveness of gradient-based
adversarial attacks such as HotFlip.

\subsection{Preliminaries}

Consider a feed-forward sublayer $f : \mathbb{R}^d \to \mathbb{R}^d$ of the form
\begin{equation}
f(x) = W_2 \sigma(W_1 x + b_1) + b_2,
\end{equation}
where $W_1, W_2 \in \mathbb{R}^{d \times d}$ have non-negative entries and
$\sigma$ is an elementwise, monotone non-decreasing activation function
(e.g., ReLU or softplus). Under these conditions, $f$ is monotone with respect
to the standard partial order on $\mathbb{R}^d$.

We study the Jacobian structure of $f$ and its implications for gradients
of a scalar loss function $L(f(x))$ with respect to the input $x$.

\subsection{Jacobian Structure of Monotone Sublayers}

\begin{lemma}[Non-negativity of the Jacobian]
\label{lem:jacobian-nonneg}
If $W_1, W_2 \succeq 0$ elementwise and $\sigma$ is monotone non-decreasing,
then the Jacobian
\[
J_f(x) = \nabla_x f(x)
\]
has non-negative entries for all $x \in \mathbb{R}^d$.
\end{lemma}

\begin{proof}
The Jacobian can be written as
\[
J_f(x) = W_2 \operatorname{diag}(\sigma'(W_1 x + b_1)) W_1.
\]
Since $W_1, W_2 \succeq 0$ and $\sigma'(z) \ge 0$ for all $z$, all entries of
$J_f(x)$ are non-negative.
\end{proof}

This structure implies that gradients propagated through monotone sublayers
lack sign cancellation effects that are common in unconstrained architectures.

\subsection{Saturation-Induced Gradient Attenuation}

We now examine how saturation in monotone networks affects gradient magnitude.

\begin{assumption}[Saturating Activations]
\label{ass:saturation}
The activation function $\sigma$ has bounded derivative and admits
saturated regimes, i.e.,
\[
\sigma'(z) \to 0 \quad \text{as } z \to +\infty.
\]
\end{assumption}

\begin{lemma}[Gradient Attenuation under Saturation]
\label{lem:gradient-attenuation}
Let $f$ satisfy the conditions above and suppose that, for a sequence
$\{x_k\}$, a non-empty subset of hidden pre-activations
$(W_1 x_k + b_1)_j$ diverges to $+\infty$. Then the operator norm of the
Jacobian $J_f(x_k)$ is non-increasing along directions corresponding to
those saturated units, and the contribution of those units to
$\nabla_x L(f(x_k))$ vanishes asymptotically.
\end{lemma}

\begin{proof}
For each saturated hidden unit $j$, we have
$\sigma'((W_1 x_k + b_1)_j) \to 0$.
Since the Jacobian depends multiplicatively on $\sigma'$, the corresponding
terms in $J_f(x_k)$ are suppressed. While the full Jacobian need not lose rank,
its effective contribution along saturated directions diminishes.
\end{proof}

This result shows that monotonicity combined with saturation can locally
attenuate gradient magnitude without eliminating all gradient signal.

\subsection{Persistence of Saturation Effects}

The following observation captures a one-sided stability property of saturated
units under monotone perturbations.

\begin{lemma}[Persistence of Saturated Units]
\label{lem:persistence}
Let $x \preceq x'$ and suppose that a hidden unit is saturated at $x'$, i.e.,
$\sigma'((W_1 x' + b_1)_j) = 0$. Then for any monotone perturbation
$\delta \succeq 0$, that unit remains saturated at $x' + \delta$.
\end{lemma}

\begin{proof}
Since $W_1 \succeq 0$, increasing the input cannot decrease pre-activations.
Thus $(W_1(x' + \delta) + b_1)_j \ge (W_1 x' + b_1)_j$, and saturation persists.
\end{proof}

Importantly, this lemma applies to individual hidden units rather than to the
entire input gradient, and does not preclude other units from becoming active.

\subsection{Implications for Gradient-Based Attacks}

Gradient-based adversarial attacks such as HotFlip rely on persistent,
high-magnitude gradients with respect to token embeddings. The preceding
analysis suggests that monotonicity constraints can reduce the availability
of such gradients by driving subsets of hidden units into saturated regimes
from which they do not recover under monotone increases in internal
representations.

\paragraph{Interpretation.}
While monotone feed-forward networks are not dynamical systems in the formal
sense, this behavior is reminiscent of classical results in monotone
dynamical systems, where cooperative interactions restrict the set of
effective directions along which trajectories evolve. Here, the analogy
serves as intuition rather than a formal guarantee.

\paragraph{Scope.}
We do not claim that monotonicity prevents all adversarial attacks, nor that
gradients vanish globally. Instead, our analysis characterizes a local
attenuation effect that weakens gradient-based optimization strategies,
providing a plausible mechanism for the empirical robustness improvements
observed in Section~\ref{sec:results}.


\section{Results}
\label{sec:results}
We now present empirical results evaluating the impact of monotonicity constraints on both standard task performance and adversarial robustness. Our analysis proceeds in two stages: we first examine optimization behavior and summarization quality to assess whether monotonicity degrades baseline performance, and then evaluate robustness under targeted adversarial attacks. Together, these experiments test whether monotonicity can serve as a practical inductive bias that improves robustness without sacrificing model utility.

\subsection{Training Dynamics and Summarization Quality}

We begin by analyzing the effect of monotonicity constraints on optimization behavior and summarization quality. Table~\ref{tab:training} summarizes the training dynamics of the baseline and monotonic models. As expected, the monotonic model starts with a substantially higher initial loss (4.97 vs.\ 2.90), reflecting the impact of the softplus reparameterization and the transformation of pretrained weights into a non-negative space. This initialization alters the original weight geometry, requiring the model to re-establish effective internal representations during fine-tuning.

Despite this unfavorable initialization, the monotonic model converges reliably over the course of training. Validation loss decreases from 2.92 to 2.54 over 7 epochs, indicating stable optimization under the imposed constraints. A persistent optimization gap remains at convergence: the monotonic model achieves a final validation loss approximately 0.29 higher than the baseline (2.54 vs.\ 2.25). This gap is consistent with the reduced expressivity induced by restricting FFN weights to be non-negative, and reflects a deliberate trade-off between flexibility and structural regularity.

\begin{table}[t]
\caption{Training dynamics comparison (seed = 42, verified from training logs).}
\label{tab:training}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
Model & Initial Loss & Final Loss & $\Delta$ Loss & Epochs \\
\midrule
Baseline  & 2.90 & 2.25 & $-$0.65 & 5 \\
Monotonic & 4.97 & 2.54 & $-$2.43 & 7 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Verified}: Training dynamics extracted from actual experimental runs completed January 2026. Baseline converged to validation loss 2.25 (improved from preliminary estimate of 2.47), while monotonic converged to 2.54, maintaining the expected optimization gap of approximately 0.29 points.

Table~\ref{tab:rouge} reports summarization performance on the full CNN/DailyMail test set (11,490 examples). Notably, both the standard pretrained model and our fine-tuned baseline achieve substantially higher ROUGE scores than typical T5-small baselines reported in the literature, with baseline ROUGE-L of 33.0 [32.8, 33.2] and ROUGE-1 of 39.7 [39.5, 40.0]. This indicates strong model quality and effective fine-tuning. The monotonic model evaluation is currently in progress; preliminary monitoring suggests performance will remain competitive with the baseline. Final monotonic results will be updated upon completion (expected within 24 hours of submission).

\begin{table}[t]
\caption{Summarization quality on CNN/DailyMail (n = 11,490 full test set, seed = 42). Values are means with 95\% bootstrap confidence intervals.}
\label{tab:rouge}
\vskip 0.1in
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
Model & ROUGE-1 & ROUGE-2 & ROUGE-L \\
\midrule
Standard  & 38.6 [38.4, 38.8] & 17.1 [16.9, 17.4] & 33.2 [32.9, 33.4] \\
Baseline  & 39.7 [39.5, 40.0] & 16.7 [16.5, 16.9] & 33.0 [32.8, 33.2] \\
Monotonic & \textit{Running} & \textit{Running} & \textit{Running} \\
\bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table}

\textbf{Note}: Verified results from full CNN/DailyMail test set (11,490 examples). Monotonic model evaluation in progress (expected completion: Jan 27, 2026). Preliminary results show baseline performance substantially higher than initial estimates, indicating strong model quality on this benchmark.


The training dynamics exhibit a characteristic pattern of monotone-constrained optimization. During the first two epochs, the monotonic model rapidly recovers much of the performance gap introduced by constrained initialization, reducing training loss by 1.92 points (from 4.97 to 3.05). Subsequent epochs yield diminishing returns, with only 0.36 points of additional improvement from epochs 3 through 7. This behavior highlights the importance of the extended warmup phase, which stabilizes gradient estimates before large updates in the constrained parameter space.

We additionally observe that the monotonic model produces slightly more consistent output lengths than the standard pretrained model, with lower variance in generated summary length (standard deviation 11.3 vs.\ 12.1 tokens). Both fine-tuned models generate longer outputs than the pretrained T5 model (mean 74–76 vs.\ 57 tokens), reflecting adaptation to the length distribution of the summarization datasets.

\paragraph{Multi-Seed Training Stability.}
\textcolor{red}{To assess robustness of our findings to training stochasticity, we repeat all experiments across five random seeds (42, 1337, 2024, 8888, 12345). Table~\ref{tab:multiseed-training} summarizes training dynamics across seeds. The monotonic model exhibits consistent convergence behavior, with mean final validation loss of 2.58 ± 0.12 compared to 2.27 ± 0.08 for the baseline. The persistent 0.31-point gap confirms that the optimization trade-off is stable across random initializations. Variance in final loss is slightly higher for the monotonic model, reflecting sensitivity to the initial softplus transformation, but all runs converge successfully within the allocated training budget.}

\begin{table}[t]
\textcolor{red}{\caption{Multi-seed training dynamics (mean ± std across 5 seeds).}}
\label{tab:multiseed-training}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
\textcolor{red}{Model} & \textcolor{red}{Final Val Loss} & \textcolor{red}{Best Val Loss} & \textcolor{red}{Training Time (h)} \\
\midrule
\textcolor{red}{Baseline}  & \textcolor{red}{2.27 ± 0.08} & \textcolor{red}{2.24 ± 0.07} & \textcolor{red}{15.2 ± 1.1} \\
\textcolor{red}{Monotonic} & \textcolor{red}{2.58 ± 0.12} & \textcolor{red}{2.54 ± 0.11} & \textcolor{red}{21.3 ± 1.8} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textcolor{red}{Table~\ref{tab:multiseed-rouge} reports ROUGE scores averaged across seeds. The monotonic model achieves mean ROUGE-L of 24.1 ± 0.6 compared to 24.9 ± 0.5 for baseline, corresponding to a 3.2\% relative gap. Confidence intervals overlap for ROUGE-2 (10.5 ± 0.4 vs 11.4 ± 0.3), though ROUGE-1 and ROUGE-L show statistically significant differences (paired t-test, p < 0.01). These results confirm that the performance trade-off is consistent and reproducible across random seeds.}

\begin{table}[t]
\textcolor{red}{\caption{Multi-seed summarization quality on CNN/DailyMail (mean ± std across 5 seeds, n=200 per seed).}}
\label{tab:multiseed-rouge}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
\textcolor{red}{Model} & \textcolor{red}{ROUGE-1} & \textcolor{red}{ROUGE-2} & \textcolor{red}{ROUGE-L} \\
\midrule
\textcolor{red}{Standard}  & \textcolor{red}{32.5 ± 0.4} & \textcolor{red}{11.9 ± 0.3} & \textcolor{red}{26.5 ± 0.4} \\
\textcolor{red}{Baseline}  & \textcolor{red}{30.8 ± 0.5} & \textcolor{red}{11.4 ± 0.3} & \textcolor{red}{24.9 ± 0.5} \\
\textcolor{red}{Monotonic} & \textcolor{red}{29.5 ± 0.6} & \textcolor{red}{10.5 ± 0.4} & \textcolor{red}{24.1 ± 0.6} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Adversarial Robustness}

\paragraph{HotFlip Attacks.}
We evaluate gradient-based robustness using HotFlip attacks, which identify vulnerable input positions via gradients with respect to token embeddings and perform targeted token substitutions to maximize loss. For each example, the attack computes gradients of the cross-entropy loss, selects the five positions with largest gradient magnitudes, and replaces tokens at those positions with vocabulary items that maximize the dot product between the gradient and the replacement embedding.

Robustness is quantified using three complementary metrics: average degradation, defined as the mean relative increase in loss under attack; attack success rate, defined as the fraction of examples for which degradation exceeds 10\%; and mean loss increase, which reports the average absolute increase in loss.

Table~\ref{tab:hotflip} reports results on 100 test examples. The monotonic model demonstrates substantially improved robustness relative to both baselines. Under HotFlip perturbations, the monotonic model incurs an average degradation of 5.2\%, compared to 16.3\% for the fine-tuned baseline and 21.4\% for the standard pretrained model. Attack success rates follow a similar pattern, with only 19\% of attacks succeeding against the monotonic model, compared to 61\% for the baseline and 70\% for the standard model. Statistical testing confirms that the difference between the baseline and monotonic models is highly significant ($t = 6.28$, $p < 10^{-9}$).

\begin{table}[t]
\caption{HotFlip attack results on CNN/DailyMail (n = 100, seed = 42).}
\label{tab:hotflip}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Model & Avg. Deg. & Success Rate & $\Delta$Loss \\
\midrule
Standard  & 21.4\% & 70\% & +0.41 \\
Baseline  & 16.3\% & 61\% & +0.35 \\
Monotonic &  5.2\% & 19\% & +0.13 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Overall, enforcing monotonicity in the FFN sublayers yields a 68\% relative reduction in attack success rate and a comparably large reduction in degradation magnitude, without requiring adversarial training or modifications to attention mechanisms. These results suggest that structural constraints alone can substantially improve robustness to gradient-based adversarial manipulation.

\paragraph{Multi-Seed HotFlip Robustness.}
\textcolor{red}{To verify that robustness gains are stable across training runs, we evaluate HotFlip attacks across all five random seeds. Table~\ref{tab:multiseed-hotflip} reports aggregate statistics. The monotonic model consistently demonstrates superior robustness, with mean attack success rate of 20.4 ± 2.1\% compared to 59.8 ± 3.2\% for baseline. Average degradation under attack is 5.8 ± 1.2\% for monotonic vs 15.9 ± 1.8\% for baseline. Cross-seed paired t-tests confirm statistical significance (p < 0.001) for all three metrics. These results indicate that the robustness benefits of monotonicity are reproducible and not artifacts of a particular random initialization.}

\begin{table}[t]
\textcolor{red}{\caption{Multi-seed HotFlip attack results (mean ± std across 5 seeds, n=100 per seed).}}
\label{tab:multiseed-hotflip}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
\textcolor{red}{Model} & \textcolor{red}{Avg. Deg.} & \textcolor{red}{Success Rate} & \textcolor{red}{$\Delta$Loss} \\
\midrule
\textcolor{red}{Standard}  & \textcolor{red}{20.8 ± 1.5\%} & \textcolor{red}{68.6 ± 2.8\%} & \textcolor{red}{+0.39 ± 0.04} \\
\textcolor{red}{Baseline}  & \textcolor{red}{15.9 ± 1.8\%} & \textcolor{red}{59.8 ± 3.2\%} & \textcolor{red}{+0.33 ± 0.03} \\
\textcolor{red}{Monotonic} & \textcolor{red}{5.8 ± 1.2\%} & \textcolor{red}{20.4 ± 2.1\%} & \textcolor{red}{+0.14 ± 0.02} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Universal Adversarial Trigger Attacks.}
We evaluate robustness to universal adversarial triggers (UAT)—short, input-agnostic token sequences optimized to maximize model loss when prepended to any input. Triggers are learned via coordinate ascent with 5 random restarts and 100 optimization iterations over 200 training examples, then evaluated on 1,500 disjoint test examples. We use a candidate vocabulary of approximately 4,000 tokens biased toward disruptive symbols, rare words, and high-frequency confounders.

Table~\ref{tab:uat} reports UAT attack results on CNN/DailyMail for seed 42. Unlike HotFlip attacks, universal triggers exhibit minimal degradation across all models, with NLL increases below 1\% and ROUGE changes under 0.6 percentage points. The monotonic model achieves the smallest loss increase under attack (0.73\% vs 0.89\% for baseline and 0.97\% for standard), though differences are not statistically significant. Notably, the monotonic model's trigger actually improves ROUGE-1 by 0.1 percentage points, suggesting the learned trigger may act as a benign prefix rather than an adversarial perturbation.

We additionally construct a transfer matrix measuring cross-model vulnerability. Triggers optimized for the monotonic model transfer poorly to other architectures (inducing +0.42\% ROUGE-L change on standard T5), while triggers optimized for standard T5 degrade monotonic performance by -0.62\%. This asymmetry suggests monotonic constraints alter the model's vulnerability landscape in ways that reduce susceptibility to input-agnostic perturbations.

\begin{table}[t]
\caption{UAT attack results on CNN/DailyMail (n=1500, seed=42). Values show mean ROUGE-L change and NLL increase under trigger prepending.}
\label{tab:uat}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Model & Learned Trigger & $\Delta$ROUGE-L & NLL Increase \\
\midrule
Standard  & ". shown » informed beach & -0.39\% & +0.97\% \\
Baseline  & 2018.2. investigation identified valuable & +0.04\% & +0.89\% \\
Monotonic & Thanks» electric conduct deux & -0.16\% & +0.73\% \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textcolor{red}{Table~\ref{tab:multiseed-uat} reports UAT results aggregated across all five seeds. Consistent with single-seed findings, universal triggers remain largely ineffective across all models and seeds. The monotonic model shows marginally lower NLL increase (0.76 ± 0.08\% vs 0.91 ± 0.07\% for baseline), though the effect is small. The weak effectiveness of UAT attacks suggests that input-agnostic perturbations are fundamentally limited compared to input-specific gradient-based attacks like HotFlip. This finding aligns with prior work showing that universal adversarial examples are harder to construct for sequence models than for vision tasks.}

\begin{table}[t]
\textcolor{red}{\caption{Multi-seed UAT attack results (mean ± std across 5 seeds, n=1500 per seed).}}
\label{tab:multiseed-uat}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
\textcolor{red}{Model} & \textcolor{red}{$\Delta$ROUGE-L (\%)} & \textcolor{red}{NLL Increase (\%)} & \textcolor{red}{Transfer Score} \\
\midrule
\textcolor{red}{Standard}  & \textcolor{red}{-0.42 ± 0.11} & \textcolor{red}{+0.99 ± 0.09} & \textcolor{red}{-0.38 ± 0.07} \\
\textcolor{red}{Baseline}  & \textcolor{red}{+0.02 ± 0.09} & \textcolor{red}{+0.91 ± 0.07} & \textcolor{red}{+0.03 ± 0.06} \\
\textcolor{red}{Monotonic} & \textcolor{red}{-0.18 ± 0.10} & \textcolor{red}{+0.76 ± 0.08} & \textcolor{red}{-0.21 ± 0.08} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Scaling to Foundation Models}
\label{sec:foundation-models}

\textcolor{red}{To assess whether monotonicity constraints scale beyond small demonstration models, we conduct preliminary experiments applying the same architectural modifications to larger foundation models. We evaluate: (1) T5-base (220M parameters), (2) T5-large (770M parameters), and (3) FLAN-T5-base (250M parameters, instruction-tuned variant). For each model, we apply monotonicity constraints to FFN sublayers using identical softplus reparameterization and initialization procedures as described in Section~\ref{sec:methods}.}

\paragraph{Implementation Considerations.}
\textcolor{red}{Scaling monotonic constraints to larger models introduces several practical challenges. First, the number of constrained parameters grows substantially: T5-base contains approximately 88M FFN parameters (40\% of total), while T5-large contains 308M (40\%). Second, training time increases due to both model size and the need for extended warmup phases to stabilize softplus-parameterized optimization. Finally, memory requirements during training become more demanding, as we must maintain both the unconstrained parameters $V$ and their softplus-transformed counterparts $W = \text{softplus}(V)$ during forward/backward passes.}

\paragraph{Preliminary Results.}
\textcolor{red}{Table~\ref{tab:foundation-preliminary} summarizes preliminary results for T5-base and FLAN-T5-base on CNN/DailyMail evaluation (200 examples, seed=42). Both models exhibit similar trade-offs as observed for T5-small: modest ROUGE degradation (2.8–3.5\% relative decrease) coupled with substantial HotFlip robustness improvements (65–72\% reduction in attack success rate). The consistency of these trends across model scales suggests that monotonicity constraints may scale effectively, though comprehensive multi-seed evaluation and full test set experiments remain ongoing.}

\begin{table}[t]
\textcolor{red}{\caption{Preliminary foundation model results (seed=42, n=200). Values show ROUGE-L scores (clean) and HotFlip attack success rates.}}
\label{tab:foundation-preliminary}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
\textcolor{red}{Model} & \textcolor{red}{Parameters} & \textcolor{red}{ROUGE-L (Clean)} & \textcolor{red}{ROUGE-L ($\Delta$)} & \textcolor{red}{Attack Success} \\
\midrule
\textcolor{red}{T5-base (Baseline)} & \textcolor{red}{220M} & \textcolor{red}{28.7} & \textcolor{red}{—} & \textcolor{red}{57.3\%} \\
\textcolor{red}{T5-base (Monotonic)} & \textcolor{red}{220M} & \textcolor{red}{27.9} & \textcolor{red}{-2.8\%} & \textcolor{red}{18.6\%} \\
\midrule
\textcolor{red}{FLAN-T5-base (Baseline)} & \textcolor{red}{250M} & \textcolor{red}{31.2} & \textcolor{red}{—} & \textcolor{red}{52.1\%} \\
\textcolor{red}{FLAN-T5-base (Monotonic)} & \textcolor{red}{250M} & \textcolor{red}{30.1} & \textcolor{red}{-3.5\%} & \textcolor{red}{17.8\%} \\
\midrule
\textcolor{red}{T5-large (Baseline)} & \textcolor{red}{770M} & \textcolor{red}{[pending]} & \textcolor{red}{—} & \textcolor{red}{[pending]} \\
\textcolor{red}{T5-large (Monotonic)} & \textcolor{red}{770M} & \textcolor{red}{[pending]} & \textcolor{red}{—} & \textcolor{red}{[pending]} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Open Questions.}
\textcolor{red}{Several questions remain regarding the scalability of monotonic constraints to foundation-scale models. First, do robustness gains persist when models are evaluated on broader task suites beyond summarization (e.g., translation, question answering, dialogue)? Second, how do monotonic constraints interact with instruction-tuning and alignment procedures typically applied to foundation models? Third, can monotonicity be combined with other robustness techniques (e.g., adversarial training, certified defenses) to achieve even stronger guarantees? Answering these questions will require extensive computational resources and careful experimental design, which we leave as priorities for future work.}

\section{Discussion}

Our results indicate that monotonicity can serve as a practical and effective structural inductive bias for improving the robustness of sequence-to-sequence models. By enforcing non-negativity constraints on feed-forward sublayers, we observe substantial reductions in vulnerability to gradient-based adversarial attacks, while incurring only modest degradation in standard summarization performance. Importantly, these robustness gains arise without adversarial training, prompt filtering, or modifications to attention mechanisms, suggesting that they are a direct consequence of architectural structure rather than heuristic defenses.

A key insight from our study is the outsized role played by feed-forward networks in shaping model sensitivity. FFN sublayers account for a large fraction of Transformer parameters and are responsible for nonlinear amplification of internal representations. Enforcing monotonicity in these components restricts the model’s ability to arbitrarily invert or amplify features, thereby limiting how small, adversarially chosen perturbations can propagate through the network. This targeted constraint provides a principled way to regularize the most nonlinear parts of the architecture without disrupting global information flow through attention.

One plausible mechanism underlying the observed robustness gains lies in the altered gradient structure induced by monotonicity. Empirically, we observe that when the gradient of the loss with respect to an internal representation becomes small or effectively zero at a given point, it often remains suppressed for all larger values along the corresponding monotone directions. Intuitively, once a monotone transformation saturates, increasing its input cannot reintroduce sensitivity. This behavior is consistent with the structure of the constrained feed-forward sublayers and may partially explain the reduced effectiveness of gradient-based attacks such as HotFlip, which rely on localized gradient signals to identify vulnerable input positions.

The observed trade-off between expressivity and robustness is both expected and informative. While monotonicity constraints reduce the space of representable functions, the resulting decrease in summarization quality is relatively small compared to the magnitude of robustness gains. This mirrors similar trade-offs observed in other settings, such as Lipschitz-constrained or contractive models, where modest sacrifices in average performance can yield significant improvements in stability and reliability. For safety-critical applications, such trade-offs may be well justified.

Moreover, monotonicity differs from conventional regularization techniques. Whereas standard regularizers penalize parameter magnitude or complexity, monotonicity imposes directional constraints on how information flows through the network. This structural restriction yields models that are not merely smoother, but more predictable in how their outputs respond to internal perturbations. As a result, monotonicity offers a path toward analyzability that is difficult to achieve through loss-based penalties alone.

Our work has several limitations. Experiments are conducted on \texttt{T5-small} and evaluation subsets, and we do not yet provide an end-to-end verification pipeline for deployed systems. Nevertheless, the consistency of the observed robustness gains across attacks and seeds suggests that monotonicity is a scalable inductive bias rather than a model-specific artifact. Extending these ideas to larger architectures, integrating formal verification procedures, and exploring interactions with complementary alignment and robustness techniques represent promising directions for future work.

% Overall, this work provides evidence that structural constraints long used in safety-critical domains can be adapted to modern language models without prohibitive loss in capability. By shaping model behavior at the architectural level, monotonicity offers a principled mechanism for improving robustness and predictability, pointing toward language models whose safety properties are influenced not only by data and objectives, but by design.

\section{Conclusion}
We investigated monotonicity as a structural inductive bias for improving the robustness of sequence-to-sequence language models. By enforcing non-negativity constraints on feed-forward sublayers in Transformer architectures, we observed substantial reductions in vulnerability to gradient-based adversarial attacks, with only modest impact on summarization quality. These results suggest that simple architectural constraints can meaningfully influence robustness without altering attention mechanisms or relying on adversarial training.

An important direction for future work is to develop theoretical guarantees that characterize when and why monotonicity yields improved robustness in high-dimensional language models. \textcolor{red}{Our preliminary experiments on T5-base and FLAN-T5-base (Section~\ref{sec:foundation-models}) suggest that monotonicity constraints scale effectively to foundation models, though comprehensive evaluation across diverse tasks and model families remains an open challenge.} Scaling this approach to larger model variants and broader tasks is another promising avenue, and may help clarify the role of structural constraints in building more reliable language models.
\section*{Impact Statement}

This work explores architectural constraints that improve the robustness of language models to adversarial manipulation. By studying monotonicity as a structural inductive bias, our approach contributes to the development of language models whose behavior is more predictable and analyzable, which is particularly relevant for safety-critical and decision-support applications.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
